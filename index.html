<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.5.0" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.5.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.5.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.5.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.5.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.5.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">






  <link rel="canonical" href="http://yoursite.com/">



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Hexo</title>
  











  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hexo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home menu-item-active">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/11/07/linux/Linux-Performance-Tuning/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/11/07/linux/Linux-Performance-Tuning/" class="post-title-link" itemprop="http://yoursite.com/index.html">Linux System Tuning</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-11-07 08:55:29" itemprop="dateCreated datePublished" datetime="2018-11-07T08:55:29+08:00">2018-11-07</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2018-12-04 16:54:48" itemprop="dateModified" datetime="2018-12-04T16:54:48+08:00">2018-12-04</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/Linux/" itemprop="url" rel="index"><span itemprop="name">Linux</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><h2 id="Purpose-of-Performance-Tuning"><a href="#Purpose-of-Performance-Tuning" class="headerlink" title="Purpose of Performance Tuning"></a>Purpose of Performance Tuning</h2><ul>
<li>将系统调节成扮演某个角色。譬如：数据库服务器、WEB服务器、文件服务器、邮件服务器等等。</li>
<li>找到并缓解系统瓶颈</li>
<li>调优指标：响应速度，吞吐量<ul>
<li>CPU、内存等硬件在最优情况下等达到最高的性能，你必须清楚。</li>
</ul>
</li>
</ul>
<h2 id="Required-Skills"><a href="#Required-Skills" class="headerlink" title="Required Skills"></a>Required Skills</h2><ul>
<li>Understand both hardware and software</li>
<li>Collecting and analysis of measurable relevant data about a performance problem</li>
<li>Set proper expectations</li>
<li>5 years full time system management experience</li>
</ul>
<h2 id="Tuning-Efficiency（调优效率）"><a href="#Tuning-Efficiency（调优效率）" class="headerlink" title="Tuning Efficiency（调优效率）"></a>Tuning Efficiency（调优效率）</h2><ul>
<li><p>Business Level Tuning</p>
<ul>
<li>Ask right question: “Reduce CPU utilization” or “Business goal”</li>
<li>Adjust workflow（调整业务的流程，减少对系统的不必要请求）</li>
<li>Removing unused services<ul>
<li>PC Smart Card Daemon</li>
<li>Buletooth and hidd</li>
</ul>
</li>
<li>Do i really need the default cron jobs?<ul>
<li>/etc/cron.daily/makewhatis.cron</li>
<li>/etc/cron.daily/mlocate.cron</li>
</ul>
</li>
</ul>
</li>
<li><p>Application Level Tuning</p>
<ul>
<li>Disable or defer expensive operations until analysis?（禁用或延迟对系统而言“很贵的”操作）<ul>
<li>Disable reverse name lookups</li>
<li>Set loglevel to warn for most production daemons</li>
</ul>
</li>
<li>Is syslogd a bottleneck?<ul>
<li>Daemon uses fsync() to flush every file write（系统为保证日志文件不丢失，会立刻调用fsync方法将数据写入磁盘）</li>
<li>Disable by prepending hyphen to name of log file in /etc/rsyslog.conf（在日志文件加“-”号，该日志会滞后写入）</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>应用层调优，调的是应用程序本身。譬如你使用nginx、nfs，它们本身有大量的参数可用来调优。在应用层调优远远优于内核调优。</p>
<ul>
<li>Kernel Level Tuning(RH442)</li>
</ul>
<p>从上往下优化空间越来越小，效果越来越不明显。譬如目前你的WEB Server是apache，调优前首先考虑是否非得使用apache，我们的业务是否是高并发，能不能换成nginx。能够在顶层解决问题，尽量不要希望在底层去解决。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>一个命令敲下去，性能提高10%、20%，这是不切合实际的。红帽操作系统已经调优过了，我们是为了某个特定的角色再进行调优。</li>
<li>不同角色的系统有不同调优的参数，不能指望一个参数搞定所有事情。</li>
<li>物理级别的缺陷，比如硬盘、网卡等由于寿命原因性能大幅下降，则系统层面调优见效甚微。</li>
</ul>
<h1 id="Understand-Hardware"><a href="#Understand-Hardware" class="headerlink" title="Understand Hardware"></a>Understand Hardware</h1><h2 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h2><h3 id="X86架构机器三个特点（RH442）"><a href="#X86架构机器三个特点（RH442）" class="headerlink" title="X86架构机器三个特点（RH442）"></a>X86架构机器三个特点（RH442）</h3><ul>
<li>I/O Address</li>
<li>IRQ（中断请求）</li>
<li>DMA（直接内存存取）</li>
</ul>
<h4 id="查看CPU"><a href="#查看CPU" class="headerlink" title="查看CPU"></a>查看CPU</h4><ul>
<li><p>总览</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"># lscpu </span><br><span class="line">Architecture:          x86_64           #x86架构</span><br><span class="line">CPU op-mode(s):        32-bit, 64-bit</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                40               #逻辑CPU个数</span><br><span class="line">On-line CPU(s) list:   0-39</span><br><span class="line">Thread(s) per core:    2                #每核的线程数</span><br><span class="line">Core(s) per socket:    10               #每颗CPU的核数</span><br><span class="line">Socket(s):             2                #物理CPU个数</span><br><span class="line">NUMA node(s):          2                # NUMA节点数</span><br><span class="line">Vendor ID:             GenuineIntel</span><br><span class="line">CPU family:            6</span><br><span class="line">Model:                 79</span><br><span class="line">Model name:            Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz</span><br><span class="line">Stepping:              1</span><br><span class="line">CPU MHz:               1200.093</span><br><span class="line">BogoMIPS:              4805.86</span><br><span class="line">Virtualization:        VT-x</span><br><span class="line">L1d cache:             32K             #一级数据缓存</span><br><span class="line">L1i cache:             32K             #一级指令缓存</span><br><span class="line">L2 cache:              256K            #二级缓存</span><br><span class="line">L3 cache:              25600K          #三级缓存</span><br><span class="line">NUMA node0 CPU(s):     0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38</span><br><span class="line">NUMA node1 CPU(s):     1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39</span><br><span class="line"></span><br><span class="line"># lscpu </span><br><span class="line">Architecture:          x86_64           #x86架构</span><br><span class="line">CPU op-mode(s):        32-bit, 64-bit</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                8                #逻辑CPU个数</span><br><span class="line">On-line CPU(s) list:   0-7</span><br><span class="line">Thread(s) per core:    1                #每核的线程数</span><br><span class="line">Core(s) per socket:    4                #每颗CPU的核数</span><br><span class="line">Socket(s):             2                #物理CPU个数</span><br><span class="line">NUMA node(s):          2                # NUMA节点数</span><br><span class="line">Vendor ID:             GenuineIntel</span><br><span class="line">CPU family:            6</span><br><span class="line">Model:                 62</span><br><span class="line">Stepping:              4</span><br><span class="line">CPU MHz:               2499.904</span><br><span class="line">BogoMIPS:              4999.28</span><br><span class="line">Virtualization:        VT-x</span><br><span class="line">L1d cache:             32K             #一级数据缓存</span><br><span class="line">L1i cache:             32K             #一级指令缓存</span><br><span class="line">L2 cache:              256K            #二级缓存</span><br><span class="line">L3 cache:              10240K          #三级缓存</span><br><span class="line">NUMA node0 CPU(s):     0,2,4,6</span><br><span class="line">NUMA node1 CPU(s):     1,3,5,7</span><br></pre></td></tr></table></figure>
</li>
<li><p>核数</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># grep CPU /proc/cpuinfo</span><br><span class="line">model name      : Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz</span><br><span class="line">model name      : Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz</span><br><span class="line">......</span><br><span class="line">model name      : Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz</span><br><span class="line">model name      : Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz</span><br><span class="line"># grep CPU /proc/cpuinfo | wc -l</span><br><span class="line">40</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="CPU架构"><a href="#CPU架构" class="headerlink" title="CPU架构"></a>CPU架构</h3><ul>
<li>SMP架构：Symmetric Multi-Processor</li>
</ul>
<p>大多数早期的单处理机系统的设计为让每个 CPU 到每个内存位置都使用同一逻辑路径（一般是平行总线）。这样每次 CPU 访问任意位置的内存时与其他系统中的 CPU 对内存的访问消耗的时间是相同的。此类架构就是我们所说的同步多处理器（SMP）系统。SMP 适合 CPU 数较少的系统，但一旦 CPU 计数超过某一点（8 或者 16），要满足对内存的平等访问所需的平行 trace 数就会使用过多的板载资源，留给外设的空间就太少。</p>
<p><img src="/images/linux/system-tuning/uma.png" alt="uma"></p>
<ul>
<li>MPP架构：Massive Parallel Processing</li>
</ul>
<p>和 NUMA 不同， MPP 提供了另外一种进行系统扩展的方式，它由多个 SMP 服务器通过一定的节点互联网络进行连接，协同工作，完成相同的任务，从用户的角度来看是一个服务器系统。其基本特征是由多个 SMP 服务器 ( 每个 SMP 服务器称节点 ) 通过节点互联网络连接而成，每个节点只访问自己的本地资源 ( 内存、存储等 ) ，是一种完全无共享 (Share Nothing) 结构，因而扩展能力最好，理论上其扩展无限制，目前的技术可实现 512 个节点互联，数千个 CPU 。目前业界对节点互联网络暂无标准，如 NCR 的 Bynet ， IBM 的 SPSwitch ，它们都采用了不同的内部实现机制。但节点互联网仅供 MPP 服务器内部使用，对用户而言是透明的。</p>
<p>在 MPP 系统中，每个 SMP 节点也可以运行自己的操作系统、数据库等。但和 NUMA 不同的是，它不存在异地内存访问的问题。换言之，每个节点内的 CPU 不能访问另一个节点的内存。节点之间的信息交互是通过节点互联网络实现的，这个过程一般称为数据重分配 (Data Redistribution) 。</p>
<p>但是 MPP 服务器需要一种复杂的机制来调度和平衡各个节点的负载和并行处理过程。目前一些基于 MPP 技术的服务器往往通过系统级软件 ( 如数据库 ) 来屏蔽这种复杂性。举例来说， NCR 的 Teradata 就是基于 MPP 技术的一个关系数据库软件，基于此数据库来开发应用时，不管后台服务器由多少个节点组成，开发人员所面对的都是同一个数据库系统，而不需要考虑如何调度其中某几个节点的负载。</p>
<p>MPP (Massively Parallel Processing)，大规模并行处理系统，这样的系统是由许多松耦合的处理单元组成的，要注意的是这里指的是处理单元而不是处理器。每个单元内的CPU都有自己私有的资源，如总线，内存，硬盘等。在每个单元内都有操作系统和管理数据库的实例复本。这种结构最大的特点在于不共享资源。</p>
<ul>
<li>UNMA架构：Non-Uniform Memory Access</li>
</ul>
<p>不是为每个处理器包提供对等的内存访问，而是让每个包/插槽组合有一个或者多个专用内存区以便提供高速访问。每个插槽还有到另一个插槽的互联以便提供对其他插槽内存的低速访问。</p>
<p>下图中 CPU0 访问左边的内存条大约需要三个时钟周期：一个周期是将地址发给内存控制器，一个周期是设置对该内存位置的访问，一个周期是读取或者写入到该位置。但 CPU1 可能需要 6 个时钟周期方可访问内存的同一位置，因为它位于不同的插槽，必须经过两个内存控制器：插槽 1 中的本地内存控制器和插槽 0  中的远程内存控制器。如果在那个位置出现竞争（即如果有一个以上 CPU 同时尝试访问同一位置），内存控制器需要对该内存进行随机且连续的访问，所以内存访问所需时间会较长。添加缓存一致性（保证本地 CPU 缓存包含同一内存位置的相同数据）会让此过程更为复杂。</p>
<p><img src="/images/linux/system-tuning/numa.png" alt="numa"></p>
<p><img src="/images/linux/system-tuning/numa-mac.png" alt="numa"></p>
<ol>
<li>当CPU有多颗时，物理CPU常见通讯方法有三种：</li>
</ol>
<ul>
<li>FSB（传统的前端总线，常用于PC机）</li>
<li>QPI(intel)</li>
<li>HyperTransport(AMD)</li>
</ul>
<h2 id="Numa-Architecture"><a href="#Numa-Architecture" class="headerlink" title="Numa Architecture"></a>Numa Architecture</h2><h3 id="查看Numa-node"><a href="#查看Numa-node" class="headerlink" title="查看Numa node"></a>查看Numa node</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># numactl --hardware</span><br><span class="line">available: 2 nodes (0-1)     #当前机器有2个NUMA node，编号0、1</span><br><span class="line">node 0 cpus: 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30</span><br><span class="line">node 0 size: 32722 MB     #物理内存大小</span><br><span class="line">node 0 free: 2352 MB      #当前free内存大小</span><br><span class="line">node 1 cpus: 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31</span><br><span class="line">node 1 size: 32768 MB</span><br><span class="line">node 1 free: 12314 MB</span><br><span class="line">node distances:           #node距离，可以简单认为Node内部访问及跨Node访问的成本。</span><br><span class="line">node   0   1 </span><br><span class="line">  0:  10  20              #由此可知跨node访问内存的成本是 内部访问的2倍。</span><br><span class="line">  1:  20  10</span><br></pre></td></tr></table></figure>
<h3 id="查看node"><a href="#查看node" class="headerlink" title="查看node"></a>查看node</h3><ul>
<li><p>node0包含的CPU</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@nginx1 ~]# ls -l /sys/devices/system/node/node0/</span><br><span class="line">cpu0/    cpu12/   cpu16/   cpu2/    cpu22/   cpu26/   cpu30/   cpu6/    cpulist </span><br><span class="line">cpu10/   cpu14/   cpu18/   cpu20/   cpu24/   cpu28/   cpu4/    cpu8/    cpumap</span><br></pre></td></tr></table></figure>
</li>
<li><p>/sys/devices/system/cpu</p>
<p>  系统的 CPU 是如何互相连接的信息。</p>
</li>
<li><p>/sys/devices/system/node</p>
<p>  系统中 NUMA 节点以及那些节点间相对距离的信息。</p>
</li>
</ul>
<h3 id="查看cpu-cache"><a href="#查看cpu-cache" class="headerlink" title="查看cpu cache"></a>查看cpu cache</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@kvm-2 ~]# ls -l /sys/devices/system/cpu/cpu0/cache/</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x 2 root root 0 Nov 10  2017 index0   # 1级数据cache</span><br><span class="line">drwxr-xr-x 2 root root 0 Nov  3  2017 index1   # 1级指令cache</span><br><span class="line">drwxr-xr-x 2 root root 0 Nov  3  2017 index2   # 2级cache</span><br><span class="line">drwxr-xr-x 2 root root 0 Nov 10  2017 index3   # 3级cache ,对应cpuinfo里的cache</span><br></pre></td></tr></table></figure>
<p>目录里的文件是cache信息描述，以本机的cpu0/index0为例简单解释一下：</p>
<table><br>    <tr><br>        <td>文件</td><br>        <td>内容</td><br>        <td>说明</td><br>   </tr><br>   <tr><br>        <td>type</td><br>        <td>Data</td><br>        <td>数据cache，如果查看index1就是Instruction</td><br>   </tr><br>   <tr><br>        <td>Level</td><br>        <td>1</td><br>        <td>L1</td><br>    </tr><br>    <tr><br>        <td>Size</td><br>        <td>32K</td><br>        <td>大小为32K</td><br>    </tr><br>    <tr><br>        <td>coherency_line_size</td><br>        <td>64</td><br>        <td rowspan="4">64<em>4</em>128=32K</td><br>    </tr><br>    <tr><br>        <td>physical_line_partition</td><br>        <td>1</td><br>    </tr><br>    <tr><br>        <td>ways_of_associativity</td><br>        <td>4</td><br>    </tr><br>    <tr><br>        <td>number_of_sets</td><br>        <td>128</td><br>    </tr><br>    <tr><br>        <td>shared_cpu_map</td><br>        <td>00000101</td><br>        <td>示这个cache被CPU0和CPU8 share</td><br>    </tr><br></table>

<p>解释一下shared_cpu_map内容的格式：</p>
<p>表面上看是2进制，其实是16进制表示，每个bit表示一个cpu，1个数字可以表示4个cpu<br>截取00000101的后4位，转换为2进制表示。</p>
<table>
<thead>
<tr>
<th>CPU id</th>
<th>15</th>
<th>14</th>
<th>13</th>
<th>12</th>
<th>11</th>
<th>10</th>
<th>9</th>
<th>8</th>
<th>7</th>
<th>6</th>
<th>5</th>
<th>4</th>
<th>3</th>
<th>2</th>
<th>1</th>
<th>0</th>
</tr>
</thead>
<tbody>
<tr>
<td>0×0101的2进制表示</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>0101表示cpu8和cpu0，即cpu0的L1 data cache是和cpu8共享的。</p>
<p>再看一下index3 shared_cpu_map的例子</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cat /sys/devices/system/cpu/cpu0/cache/index3/shared_cpu_map</span><br><span class="line">00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000f0f</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>CPU id</th>
<th>15</th>
<th>14</th>
<th>13</th>
<th>12</th>
<th>11</th>
<th>10</th>
<th>9</th>
<th>8</th>
<th>7</th>
<th>6</th>
<th>5</th>
<th>4</th>
<th>3</th>
<th>2</th>
<th>1</th>
<th>0</th>
</tr>
</thead>
<tbody>
<tr>
<td>0x0f0f的2进制表示</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>cpu0,1,2,3和cpu8,9,10,11共享L3 cache。</p>
<h3 id="查看numa状态"><a href="#查看numa状态" class="headerlink" title="查看numa状态"></a>查看numa状态</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># numastat</span><br><span class="line">                           node0           node1</span><br><span class="line">numa_hit              1011487518       716368222</span><br><span class="line">numa_miss                      0       145365467</span><br><span class="line">numa_foreign           145365467               0</span><br><span class="line">interleave_hit             20673           20631</span><br><span class="line">local_node            1011487341       716343592</span><br><span class="line">other_node                   177       145390097</span><br></pre></td></tr></table></figure>
<p>上述可知node 0的unma_miss过高，可考虑用numactl将进程和CPU绑定。详见CPU调优章节。</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>numa_hit</td>
<td>为这个节点成功的分配尝试数。</td>
</tr>
<tr>
<td>numa_miss</td>
<td>由于在目的节点中内存较低而尝试为这个节点分配到另一个节点的数目。每个 numa_miss 事件都在另一个节点中有对应的 numa_foreign 事件。</td>
</tr>
<tr>
<td>numa_foreign</td>
<td>最初要为这个节点但最后分配个另一个节点的分配数。每个 numa_foreign 事件都在另一个节点中有对应的 numa_miss 事件。</td>
</tr>
<tr>
<td>interleave_hit</td>
<td>成功分配给这个节点的尝试交错策略数。</td>
</tr>
<tr>
<td>local_node</td>
<td>这个节点中的进程成功在这个节点中分配内存的次数。</td>
</tr>
<tr>
<td>other_node</td>
<td>这个节点中的进程成功在另一个节点中分配内存的次数。</td>
</tr>
</tbody>
</table>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@nginx1 ~]# numactl --show</span><br><span class="line">policy: default</span><br><span class="line">preferred node: current</span><br><span class="line">physcpubind: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 </span><br><span class="line">cpubind: 0 1</span><br><span class="line">nodebind: 0 1</span><br><span class="line">membind: 0 1</span><br></pre></td></tr></table></figure>
<h3 id="查看内存numa-node分布"><a href="#查看内存numa-node分布" class="headerlink" title="查看内存numa node分布"></a>查看内存numa node分布</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/&lt;pid&gt;/numa_maps</span><br><span class="line">cat /proc/$(pidof pname|cut -d &quot;&quot; -f1)/numa_maps</span><br></pre></td></tr></table></figure>
<h3 id="查看线程run在哪个processor"><a href="#查看线程run在哪个processor" class="headerlink" title="查看线程run在哪个processor"></a>查看线程run在哪个processor</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">top -p $(pidof pname|sed -e &apos;s/ /,/g&apos;)</span><br><span class="line"></span><br><span class="line">在默认配置下不显示线程信息，需要进入Top后按“shift+H”，打开线程显示。</span><br><span class="line">另外，如果没有P列，还需要按“f”，按“j”，添加，这一列显示的数字就是这个线程上次run的processor id。</span><br></pre></td></tr></table></figure>
<p><a href="https://blog.jcole.us/2010/09/28/mysql-swap-insanity-and-the-numa-architecture/" target="_blank" rel="noopener">https://blog.jcole.us/2010/09/28/mysql-swap-insanity-and-the-numa-architecture/</a><br><a href="https://software.intel.com/en-us/articles/intel-64-architecture-processor-topology-enumeration/" target="_blank" rel="noopener">https://software.intel.com/en-us/articles/intel-64-architecture-processor-topology-enumeration/</a></p>
<h3 id="numad"><a href="#numad" class="headerlink" title="numad"></a>numad</h3><pre><code>numad 是一个自动 NUMA 亲和性管理守护进程，它监控系统中的 NUMA 拓扑以及资源使用以便动态提高 NUMA 资源分配和管理（以及系统性能）。

numad 不会在进程只运行几分钟或者不会消耗很多资源时改进性能。

有连续不可预测内存访问的系统，比如大型内存中的数据库也不大可能从 numad 使用中受益。
</code></pre><h2 id="CPU-Cache"><a href="#CPU-Cache" class="headerlink" title="CPU Cache"></a>CPU Cache</h2><p>静态RAM（SRAM）集成度低（存储相同的数据，静态RAM的体积是动态RAM的6倍），价格高（同容量的静态RAM是动态RAM的四倍）；动态RAM（DRAM）。静态RAM缓存叫一级缓存，而把动态RAM叫二级缓存。</p>
<h3 id="查看CPU-Cache"><a href="#查看CPU-Cache" class="headerlink" title="查看CPU Cache"></a>查看CPU Cache</h3><pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"># lscpu -p</span><br><span class="line"># The following is the parsable format, which can be fed to other</span><br><span class="line"># programs. Each different item in every column has an unique ID</span><br><span class="line"># starting from zero.</span><br><span class="line"># CPU,Core,Socket,Node,,L1d,L1i,L2,L3    # “Node”表示NUMA nodes</span><br><span class="line">0,0,0,0,,0,0,0,0</span><br><span class="line">1,1,1,1,,1,1,1,1</span><br><span class="line">2,2,0,0,,2,2,2,0</span><br><span class="line">3,3,1,1,,3,3,3,1</span><br><span class="line">4,4,0,0,,4,4,4,0</span><br><span class="line">5,5,1,1,,5,5,5,1</span><br><span class="line">6,6,0,0,,6,6,6,0</span><br><span class="line">7,7,1,1,,7,7,7,1</span><br><span class="line"></span><br><span class="line">[root@kvm-2 ~]# dmidecode -t 7</span><br><span class="line"># dmidecode 2.11</span><br><span class="line">SMBIOS 2.7 present.</span><br><span class="line"></span><br><span class="line">Handle 0x0700, DMI type 7, 19 bytes</span><br><span class="line">Cache Information</span><br><span class="line">        Socket Designation: Not Specified</span><br><span class="line">        Configuration: Enabled, Not Socketed, Level 1</span><br><span class="line">        Operational Mode: Write Through</span><br><span class="line">        Location: Internal</span><br><span class="line">        Installed Size: 128 kB         #一级缓存（32k * 4）</span><br><span class="line">        Maximum Size: 128 kB</span><br><span class="line">        Supported SRAM Types:</span><br><span class="line">                Unknown</span><br><span class="line">        Installed SRAM Type: Unknown</span><br><span class="line">        Speed: Unknown</span><br><span class="line">        Error Correction Type: Single-bit ECC</span><br><span class="line">        System Type: Data</span><br><span class="line">        Associativity: 8-way Set-associative</span><br><span class="line"></span><br><span class="line">Handle 0x0701, DMI type 7, 19 bytes</span><br><span class="line">Cache Information</span><br><span class="line">        Socket Designation: Not Specified</span><br><span class="line">        Configuration: Enabled, Not Socketed, Level 2</span><br><span class="line">        Operational Mode: Write Through</span><br><span class="line">        Location: Internal</span><br><span class="line">        Installed Size: 1024 kB       #二级缓存（256k * 4）</span><br><span class="line">        Maximum Size: 1024 kB</span><br><span class="line">        Supported SRAM Types:</span><br><span class="line">                Unknown</span><br><span class="line">        Installed SRAM Type: Unknown</span><br><span class="line">        Speed: Unknown</span><br><span class="line">        Error Correction Type: Single-bit ECC</span><br><span class="line">        System Type: Unified</span><br><span class="line">        Associativity: 8-way Set-associative</span><br><span class="line"></span><br><span class="line">Handle 0x0702, DMI type 7, 19 bytes</span><br><span class="line">Cache Information</span><br><span class="line">        Socket Designation: Not Specified</span><br><span class="line">        Configuration: Enabled, Not Socketed, Level 3</span><br><span class="line">        Operational Mode: Write Back</span><br><span class="line">        Location: Internal</span><br><span class="line">        Installed Size: 10240 kB    #与lscpu数值一样，说明4核共享三级缓存</span><br><span class="line">        Maximum Size: 10240 kB</span><br><span class="line">        Supported SRAM Types:</span><br><span class="line">                Unknown</span><br><span class="line">        Installed SRAM Type: Unknown</span><br><span class="line">        Speed: Unknown</span><br><span class="line">        Error Correction Type: Single-bit ECC</span><br><span class="line">        System Type: Unified</span><br><span class="line">        Associativity: 20-way Set-associative</span><br></pre></td></tr></table></figure>
</code></pre><p>由上输出可得知该服务器的L1，L2是每个核心独享的，L3是共享的。</p>
<h2 id="Memory"><a href="#Memory" class="headerlink" title="Memory"></a>Memory</h2><ul>
<li><p>Memory size,max allowed</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"># free</span><br><span class="line">             total       used       free     shared    buffers     cached</span><br><span class="line">Mem:      90743596   23288228   67455368          0     684452    2938648</span><br><span class="line">-/+ buffers/cache:   19665128   71078468</span><br><span class="line">Swap:     26214392          0   26214392</span><br><span class="line"></span><br><span class="line"># more /proc/meminfo </span><br><span class="line">MemTotal:       263860344 kB      # 已经减去了显卡占用的内存</span><br><span class="line">MemFree:        206924460 kB</span><br><span class="line">MemAvailable:   248609828 kB</span><br><span class="line">Buffers:          145604 kB</span><br><span class="line">Cached:         51754488 kB</span><br><span class="line">SwapCached:            0 kB</span><br><span class="line">Active:         45933660 kB</span><br><span class="line">Inactive:        7115240 kB</span><br><span class="line">Active(anon):    8130140 kB</span><br><span class="line">Inactive(anon):  4851368 kB</span><br><span class="line">Active(file):   37803520 kB</span><br><span class="line">Inactive(file):  2263872 kB</span><br><span class="line"></span><br><span class="line"># dmidecode -t 17</span><br><span class="line"># dmidecode 2.11</span><br><span class="line">SMBIOS 2.7 present.</span><br><span class="line"></span><br><span class="line">Handle 0x1100, DMI type 17, 34 bytes</span><br><span class="line">Memory Device</span><br><span class="line">        Array Handle: 0x1000</span><br><span class="line">        Error Information Handle: Not Provided</span><br><span class="line">        Total Width: 72 bits</span><br><span class="line">        Data Width: 64 bits</span><br><span class="line">        Size: 8192 MB                 # 内存大小</span><br><span class="line">        Form Factor: DIMM</span><br><span class="line">        Set: 1</span><br><span class="line">        Locator: DIMM_A1 </span><br><span class="line">        Bank Locator: Not Specified</span><br><span class="line">        Type: DDR3                   # DDR3代</span><br><span class="line">        Type Detail: Synchronous Registered (Buffered)</span><br><span class="line">        Speed: 1333 MHz              # 时钟频率</span><br><span class="line">        Manufacturer: 00CE04B300CE</span><br><span class="line">        Serial Number: 4400B1EA</span><br><span class="line">        Asset Tag: 01104611</span><br><span class="line">        Part Number: M393B1K70CH0-YH9  </span><br><span class="line">        Rank: 2</span><br><span class="line">        Configured Clock Speed: 1333 MHz</span><br><span class="line">......</span><br></pre></td></tr></table></figure>
</li>
<li><p>bandwidth and letency</p>
<ul>
<li>DDR2(2 bits),DDR3(4 bits),DDR4(8 bits)</li>
<li>Bandwidth = Clock rate(时钟频率) <em> 4(DDR3) </em> 2(Double) * bits / 8（Double就是D，脉冲升频降会各取一次数据。带宽无需手动计算，内存卡会标识PC xxxxMB）</li>
<li>Letency(wait time before read again,in ns)读取内存时，要等待的时间。动态内存需要电门不停的刷，所以读取数据需要时间。</li>
<li>ECC(slower,safer)<ul>
<li>Corrects single-bit errors</li>
<li>Detects multiple-bit errors</li>
</ul>
</li>
</ul>
</li>
<li>Method of memory accessing<ul>
<li>UMA,NUMA</li>
</ul>
</li>
</ul>
<h2 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h2><ul>
<li>Type of storage used<ul>
<li>mechanical magnetic platters（机械磁盘）</li>
<li>SSD devices（固态磁盘）</li>
</ul>
</li>
<li>Hardware RAID Level<ul>
<li>stripe depth（条带深度）</li>
<li>stripe width（条带宽度）</li>
<li>stripe size = stripe depth × stripe width</li>
</ul>
</li>
<li>Direct-attached Storage（直连存储）<ul>
<li>SATA,SAS,IDE</li>
</ul>
</li>
<li>SCSI,Fibre Channel,ISCSI<ul>
<li>Bandwidth,latency,multipath</li>
</ul>
</li>
</ul>
<h3 id="磁盘架构"><a href="#磁盘架构" class="headerlink" title="磁盘架构"></a>磁盘架构</h3><ul>
<li>CLV</li>
<li>CAV</li>
<li><p>Zoned CAV</p>
<p>  Tips</p>
<p>  磁盘外圈速度比里圈速度快得多，所以操作系统默认将swap分区分配在里圈。</p>
</li>
</ul>
<h3 id="磁盘速度"><a href="#磁盘速度" class="headerlink" title="磁盘速度"></a>磁盘速度</h3><p>一般都指Burst speed（顺序读写）速率。</p>
<p><img src="/images/linux/system-tuning/storage-bandwidth.png" alt="storage-bandwidth"></p>
<h3 id="磁盘类型"><a href="#磁盘类型" class="headerlink" title="磁盘类型"></a>磁盘类型</h3><ul>
<li><p>IDE（并口）</p>
<p>  IDE（Integrated Drive Electronics电子集成驱动器）的缩写，它的本意是指把控制器与盘体集成在一起的硬盘驱动器，是一种硬盘的传输接口，它有另一个名称叫做ATA（Advanced Technology Attachment），这两个名词都有厂商在用，指的是相同的东西。</p>
<p>  IDE的规格后来有所进步，而推出了EIDE（Enhanced IDE）的规格名称，而这个规格同时又被称为Fast ATA。所不同的是Fast ATA是专指硬盘接口，而EIDE还制定了连接光盘等非硬盘产品的标准。而这个连接非硬盘类的IDE标准，又称为ATAPI接口。而之后再推出更快的接口，名称都只剩下ATA的字样，像是Ultra ATA、ATA/66、ATA/100等。</p>
</li>
<li><p>SATA（串口）</p>
<p>  SATA（Serial ATA）口的硬盘又叫串口硬盘。2001年，由Intel、APT、Dell、IBM、希捷、迈拓这几大厂商组成的Serial ATA委员会正式确立了Serial ATA 1.0规范。</p>
</li>
<li><p>SCSI（小型计算机系统专用接口）</p>
<p>  SCSI的英文全称为“Small Computer System Interface”（小型计算机系统接口），是同IDE（ATA）完全不同的接口，IDE接口是普通PC的标准接口，而SCSI并不是专门为硬盘设计的接口，是一种广泛应用于小型机上的高速数据传输技术。SCSI接口具有应用范围广、多任务、带宽大、CPU占用率低，以及热插拔等优点，但较高的价格使得它很难如IDE硬盘般普及，因此SCSI硬盘主要应用于中、高端服务器和高档工作站中。</p>
</li>
<li><p>SAS（就是串口的SCSI接口）</p>
<p>  SAS(Serial Attached SCSI)即串行连接SCSI，是新一代的SCSI技术。和现在流行的Serial ATA(SATA)硬盘相同，都是采用串行技术以获得更高的传输速度，并通过缩短连结线改善内部空间等。SAS是并行SCSI接口之后开发出的全新接口。此接口的设计是为了改善存储系统的效能、可用性和扩充性，并且提供与SATA硬盘的兼容性。</p>
</li>
<li><p>FC（光纤通道）</p>
<p>  光纤通道的英文拼写是Fiber Channel，和SCIS接口一样光纤通道最初也不是为硬盘设计开发的接口技术，是专门为网络系统设计的，但随着存储系统对速度的需求，才逐渐应用到硬盘系统中。光纤通道硬盘是为提高多硬盘存储系统的速度和灵活性才开发的，它的出现大大提高了多硬盘系统的通信速度。光纤通道的主要特性有：热插拔性、高速带宽、远程连接、连接设备数量大等。</p>
</li>
<li><p>SSD（固态硬盘）</p>
<p>  固态硬盘（Solid State Disk或Solid State Drive），也称作电子硬盘或者固态电子盘，是由控制单元和固态存储单元（DRAM或FLASH芯片）组成的硬盘。固态硬盘的接口规范和定义、功能及使用方法上与普通硬盘的相同，在产品外形和尺寸上也与普通硬盘一致。由于固态硬盘没有普通硬盘的旋转介质，因而抗震性极佳。其芯片的工作温度范围很宽（-40~85℃）。</p>
<p>  由于固态硬盘技术与传统硬盘技术不同，所以产生了不少新兴的存储器厂商。厂商只需购买NAND存储器，再配合适当的控制芯片，就可以制造固态硬盘了。新一代的固态硬盘普遍采用SATA-2接口。</p>
</li>
</ul>
<h3 id="查看磁盘"><a href="#查看磁盘" class="headerlink" title="查看磁盘"></a>查看磁盘</h3><pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@kvm-2 ~]# fdisk -l /dev/sda </span><br><span class="line"></span><br><span class="line">Disk /dev/sda: 4000.8 GB, 4000787030016 bytes</span><br><span class="line">255 heads, 63 sectors/track, 486401 cylinders</span><br><span class="line">Units = cylinders of 16065 * 512 = 8225280 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br><span class="line">Disk identifier: 0x00000000</span><br><span class="line"></span><br><span class="line">[root@kvm-2 ~]# lspci</span><br><span class="line">......</span><br><span class="line">00:1f.0 ISA bridge: Intel Corporation C600/X79 series chipset LPC Controller (rev 05)</span><br><span class="line">00:1f.2 SATA controller: Intel Corporation C600/X79 series chipset 6-Port SATA AHCI Controller (rev 05)</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line"># hdparm -I /dev/sda</span><br><span class="line"></span><br><span class="line">/dev/sda:</span><br><span class="line"></span><br><span class="line">ATA device, with non-removable media</span><br><span class="line">	Model Number:       ST1000NM0033-9ZM173       # 希捷1T</span><br><span class="line">	Serial Number:      Z1W3VQXC</span><br><span class="line">	Firmware Revision:  GA0A</span><br><span class="line">	Transport:          Serial, SATA Rev 3.0</span><br><span class="line">Standards:</span><br><span class="line">	Supported: 9 8 7 6 5 </span><br><span class="line">	Likely used: 9</span><br><span class="line">......</span><br><span class="line">[root@localhost ~]# dmesg |grep -C5 SATA</span><br></pre></td></tr></table></figure>
</code></pre><h2 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h2><ul>
<li>Electronic disk, no moving mechanical components</li>
<li>No startup time</li>
<li>Very low latency</li>
<li>Potentially longer life time（尽可能延长寿命）<ul>
<li>Wear leveling（磨损平衡）:闪存寿命是以P/E（完全擦写）次数来计算的，而WL就是确保闪存内每个块被写入的次数相等的一种机制。</li>
</ul>
</li>
<li>Parameter<ul>
<li>TBW<ul>
<li>在 SSD 使用寿命结束之前指定工作量可以写入 SSD 的总数据量。</li>
</ul>
</li>
<li>DWPD<ul>
<li>在保固期内（或不同的数年时段内）每天可以写入硬盘用户存储容量的次数。</li>
<li>DWPD = (固态硬盘的 TBW (TB) <em> P/E) / (365 天 </em> 年数 * 固态硬盘用户容量 (GB))</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="SSD-Types"><a href="#SSD-Types" class="headerlink" title="SSD Types"></a>SSD Types</h3><p>固态硬盘就是靠NAND Flash闪存芯片存储数据的，这点类似于我们常见的U盘。NAND Flash根据存储原理分为三种，SLC、MLC、TLC。</p>
<ul>
<li><p>SLC</p>
<p>  Single-Level Cell ，即1bit/cell（1个存储器储存单元可存放1 bit的数据），速度快寿命长，价格超贵（约MLC 3倍以上的价格），约10万次擦写寿命</p>
</li>
<li><p>MLC</p>
<p>  Multi-Level Cell，即2bit/cell，速度一般寿命一般，价格一般，约1000–3000次擦写寿命</p>
</li>
<li><p>TLC</p>
<p>  Trinary-Level Cell，即3bit/cell，也有Flash厂家叫8LC，速度慢寿命短，价格便宜，约1000次擦写寿命。<br>  单位容量的存储器，可以存储更多的数据，所以TLC每百万字节生产成本是最低的。</p>
</li>
</ul>
<p>实战：计算256G的TLC固态硬盘的使用寿命。</p>
<p>假设该硬盘每天读取100G数据，256G*1000/356/100G=7.19（年） </p>
<h3 id="SSD-Garbage-Collection"><a href="#SSD-Garbage-Collection" class="headerlink" title="SSD Garbage Collection"></a>SSD Garbage Collection</h3><p><img src="/images/linux/system-tuning/ssd-garbage-collection.png" alt="ssd-garbage-collection"></p>
<ol>
<li>上图SSD中有两个空的（erased）的Block X和Block Y, 每个Block有12个Pages;</li>
<li>首先在Block X中写入4个Pages(A, B, C, D);</li>
<li>接着再向Block X中写入新的4个pages(E, F, G, H), 同时写入PageA-D的更新数据（A’, B’, C’, D’), 这时PageA-D变为失效数据（invalid）;</li>
<li>为了向PageA-D的位置写入数据，需要将E, F, G, H, A’, B’, C’, D’ 8个pages先搬到Block Y中, 之后再把Block X erase掉，这个过程就为GC。</li>
</ol>
<p>Nand flash 以Page为单位读写数据，而以Block为单位擦除数据。</p>
<p>不过，由于GC的过程增加了数据的读写过程，势必会对SSD的performance的产生一定的影响，所以GC发生的条件与触发点很关键。</p>
<p>GC触发条件大致有3点：</p>
<ol>
<li>Spare Block（）备用块太少</li>
<li>Wear leveling</li>
<li>处理ECC错误Block</li>
</ol>
<h3 id="SSD-Trim"><a href="#SSD-Trim" class="headerlink" title="SSD Trim"></a>SSD Trim</h3><p>操作系统删除数据时，Windows只会做个标记，说明这里已经没东西了，等到真正要写入数据时再来真正删除，并且做标记这个动作会保留在磁盘缓存中，等到磁盘空闲时再执行；Linux只会把inode table回收。</p>
<p>所以对于非空的page，SSD在写入前必须先进行一次Erase，则写入过程为read-erase-modify-write:将整个block的内容读取到cache中，整个block从SSD中Erase,要覆写的page写入到cache的block中，将cache中更新的block写入闪存介质，这个现象称之为写入放大(write amplification)。</p>
<p>为了解决这个问题，SSD开始支持TRIM，TRIM功能使操作系统得以通知SSD哪些页不再包含有效的数据。</p>
<p>当Windows识别到SSD并确认SSD支持Trim后，在删除数据时，会不向硬盘通知删除指令，只使用Volume Bitmap来记住这里的数据已经删除。Volume Bitmap只是一个磁盘快照，其建立速度比直接读写硬盘去标记删除区域要快得多。这一步就已经省下一大笔时间了。然后再是写入数据的时候，由于NAND闪存保存数据是纯粹的数字形式，因此可以直接根据Volume Bitmap的情况，向快照中已删除的区块写入新的数据，而不用花时间去擦除原本的数据。</p>
<ul>
<li><p>Linux启用Trim</p>
<ol>
<li><p>确认 SSD 、操作系统、文件系统都支持 TRIM</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># discard_granularity 非 0 表示支持</span><br><span class="line"># cat /sys/block/sda/queue/discard_granularity</span><br><span class="line">0</span><br><span class="line"># cat /sys/block/nvme0n1/queue/discard_granularity</span><br><span class="line">512</span><br><span class="line"></span><br><span class="line"># DISC-GRAN (discard granularity) 和 DISC-MAX (discard max bytes) 列非 0 表示该 SSD 支持 TRIM 功能。</span><br><span class="line"># lsblk --discard</span><br><span class="line">NAME    DISC-ALN DISC-GRAN DISC-MAX DISC-ZERO</span><br><span class="line">sda            0        0B       0B         0</span><br><span class="line">├─sda1         0        0B       0B         0</span><br><span class="line">├─sda2         0        0B       0B         0</span><br><span class="line">└─sda3         0        0B       0B         0</span><br><span class="line">sr0            0        0B       0B         0</span><br><span class="line">nvme0n1      512      512B       2T         1</span><br><span class="line">nvme1n1      512      512B       2T         1</span><br></pre></td></tr></table></figure>
</li>
<li><p>开启</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">对于 ext4 文件系统，可以在/etc/fstab里添加 discard 参数来启用 TRIM，添加前请确认你的 SSD 支持 TRIM。</span><br><span class="line"># vim /etc/fstab</span><br><span class="line">/dev/sdb1  /data1       ext4   defaults,noatime,discard   0  0</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>Windows启用Trim</p>
</li>
</ul>
<p>注意：如果SSD组RAID0后，将失去Trim功能。</p>
<h2 id="RAID"><a href="#RAID" class="headerlink" title="RAID"></a>RAID</h2><ul>
<li><p>striping（条带化）</p>
<p>  条带（strip）是把连续的数据分割成相同大小的数据块，把每段数据分别写入到阵列中的不同磁盘上的方法。简单的说，条带是一种将多个磁盘驱动器合并为一个卷的方法。 许多情况下，这是通过硬件控制器来完成的。</p>
</li>
<li><p>why striping?</p>
<p>  首先介绍什么是磁盘冲突。当多个进程同时访问一个磁盘时，磁盘的访问次数（每秒的 I/O 操作，IOPS）和数据传输率（每秒传输的数据量，TPS）达到极限后，后面的进程就需要等待，这时就是所谓的磁盘冲突。</p>
<p>  避免磁盘冲突是优化 I/O 性能的一个重要目标，而 I/O 性能的优化与其他资源（如CPU和内存）的优化有着很大的区别,I/O 优化最有效的手段是将 I/O 最大限度的进行平衡。</p>
<p>  条带化技术就是一种自动的将 I/O 的负载均衡到多个物理磁盘上的技术，条带化技术就是将一块连续的数据分成很多小部分并把他们分别存储到不同磁盘上去。这就能使多个进程同时访问数据的多个不同部分而不会造成磁盘冲突，而且在需要对这种数据进行顺序访问的时候可以获得最大程度上的 I/O 并行能力，从而获得非常好的性能。</p>
</li>
<li><p>stripe depth(stripe unit)</p>
<p>  条带深度：指的是条带的大小。这个参数指的是写在每块磁盘上的条带数据块的大小。RAID的数据块大小一般在2KB到512KB之间(或者更大)，其数值是 2 的次方，即2KB,4KB,8KB,16KB这样。</p>
<p>  条带大小对性能的影响比条带宽度难以量化的多：</p>
<ul>
<li>减小条带大小: 由于条带大小减小了，则文件被分成了更多个，更小的数据块。这些数据块会被分散到更多的硬盘上存储，因此提高了传输的性能，但是由于要多次寻找不同的数据块，磁盘定位的性能就下降了。</li>
<li><p>增加条带大小: 与减小条带大小相反，会降低传输性能，提高定位性能。</p>
<p>根据上边的论述，我们会发现根据不同的应用类型，不同的性能需求，不同驱动器的不同特点(如SSD硬盘)，不存在一个普遍适用的”最佳条带大小”。所以这也是存储厂家，文件系统编写者允许我们自己定义条带大小的原因。</p>
</li>
</ul>
</li>
<li><p>stripe width</p>
<p>  条带宽度：是指同时可以并发读或写的条带数量。这个数量等于RAID中的物理硬盘数量。例如一个经过条带化的，具有4块物理硬盘的阵列的条带宽度就是 4。增加条带宽度，可以增加阵列的读写性能。道理很明显，增加更多的硬盘，也就增加了可以同时并发读或写的条带数量。</p>
</li>
<li><p>stripe size</p>
<p>  有时也称block size块大小、chunk size簇大小、stripe length条带长度、granularity粒度，是单块磁盘上的每次I/O的最小单位。</p>
</li>
</ul>
<p>实战：<a href="http://www.mysqlab.net/blog/2011/12/raid10-stripe-size-for-mysql-innodb/" target="_blank" rel="noopener">Raid1+0 stripe size for MySQL InnoDB</a></p>
<h2 id="Networking-Profile"><a href="#Networking-Profile" class="headerlink" title="Networking Profile"></a>Networking Profile</h2><h3 id="查看网卡"><a href="#查看网卡" class="headerlink" title="查看网卡"></a>查看网卡</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@kvm-2 ~]# lspci | grep Ethernet</span><br><span class="line">01:00.0 Ethernet controller: Broadcom Corporation NetXtreme BCM5720 Gigabit Ethernet PCIe</span><br><span class="line">01:00.1 Ethernet controller: Broadcom Corporation NetXtreme BCM5720 Gigabit Ethernet PCIe</span><br><span class="line">02:00.0 Ethernet controller: Broadcom Corporation NetXtreme BCM5720 Gigabit Ethernet PCIe</span><br><span class="line">02:00.1 Ethernet controller: Broadcom Corporation NetXtreme BCM5720 Gigabit Ethernet PCIe</span><br><span class="line"># 示：四块博通千兆网卡</span><br><span class="line"></span><br><span class="line">[root@kvm-2 ~]# ethtool bond0</span><br><span class="line">Settings for bond0:</span><br><span class="line">        Supported ports: [ ]</span><br><span class="line">        Supported link modes:   Not reported</span><br><span class="line">        Supported pause frame use: No</span><br><span class="line">        Supports auto-negotiation: No</span><br><span class="line">        Advertised link modes:  Not reported</span><br><span class="line">        Advertised pause frame use: No</span><br><span class="line">        Advertised auto-negotiation: No</span><br><span class="line">        Speed: 1000Mb/s</span><br><span class="line">        Duplex: Full         # 当前工作在全双工模式</span><br><span class="line">        Port: Other</span><br><span class="line">        PHYAD: 0</span><br><span class="line">        Transceiver: internal</span><br><span class="line">        Auto-negotiation: off</span><br><span class="line">        Link detected: yes</span><br><span class="line"></span><br><span class="line">[root@kvm-2 ~]# ip a s bond0</span><br><span class="line">6: bond0: &lt;BROADCAST,MULTICAST,MASTER,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP </span><br><span class="line">    link/ether f8:bc:12:48:91:64 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet6 fe80::fabc:12ff:fe48:9164/64 scope link </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>
<h2 id="主板"><a href="#主板" class="headerlink" title="主板"></a>主板</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@kvm-2 ~]# dmidecode -t baseboard</span><br></pre></td></tr></table></figure>
<h2 id="PCI设备"><a href="#PCI设备" class="headerlink" title="PCI设备"></a>PCI设备</h2><p>PCI是CPU和外围设备通信的高速传输总线。</p>
<p><img src="/images/linux/system-tuning/pci-bandwidth.png" alt="pci-bandwidth"></p>
<h3 id="查看pci设备"><a href="#查看pci设备" class="headerlink" title="查看pci设备"></a>查看pci设备</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">[root@kvm-2 ~]# lspci            # pciutils软件包</span><br><span class="line">7f:08.0 System peripheral: Intel Corporation Xeon E5 v2/Core i7 QPI Link 0 (rev 04)</span><br><span class="line">......</span><br><span class="line">7f:0c.1 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Unicast Registers (rev 04)</span><br><span class="line">......</span><br><span class="line">7f:16.2 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Broadcast Registers (rev 04)</span><br><span class="line"></span><br><span class="line">[root@kvm-2 ~]# lspci -vt</span><br><span class="line">-+-[0000:7f]-+-08.0  Intel Corporation Xeon E5 v2/Core i7 QPI Link 0</span><br><span class="line"> |           +-09.0  Intel Corporation Xeon E5 v2/Core i7 QPI Link 1</span><br><span class="line">......</span><br><span class="line"> |           +-0f.0  Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 0 Target Address/Thermal Registers</span><br><span class="line">......</span><br><span class="line"> |           +-10.7  Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 1 Channel 0-3 ERROR Registers 3</span><br><span class="line"> |           +-13.0  Intel Corporation Xeon E5 v2/Core i7 R2PCIe</span><br><span class="line"> |           +-13.1  Intel Corporation Xeon E5 v2/Core i7 R2PCIe</span><br><span class="line"> |           +-13.4  Intel Corporation Xeon E5 v2/Core i7 QPI Ring Registers</span><br><span class="line"> |           +-13.5  Intel Corporation Xeon E5 v2/Core i7 QPI Ring Performance Ring Monitoring</span><br><span class="line"> |           +-16.0  Intel Corporation Xeon E5 v2/Core i7 System Address Decoder</span><br><span class="line"> |           +-16.1  Intel Corporation Xeon E5 v2/Core i7 Broadcast Registers</span><br><span class="line"> |           \-16.2  Intel Corporation Xeon E5 v2/Core i7 Broadcast Registers</span><br><span class="line"> \-[0000:00]-+-00.0  Intel Corporation Xeon E5 v2/Core i7 DMI2</span><br><span class="line">             +-01.0-[02]--+-00.0  Broadcom Corporation NetXtreme BCM5720 Gigabit Ethernet PCIe</span><br><span class="line">             |            \-00.1  Broadcom Corporation NetXtreme BCM5720 Gigabit Ethernet PCIe</span><br><span class="line">             +-01.1-[01]--+-00.0  Broadcom Corporation NetXtreme BCM5720 Gigabit Ethernet PCIe</span><br><span class="line">             |            \-00.1  Broadcom Corporation NetXtreme BCM5720 Gigabit Ethernet PCIe</span><br><span class="line">             +-02.0-[04]--</span><br><span class="line">             +-02.2-[03]----00.0  LSI Logic / Symbios Logic MegaRAID SAS 2008 [Falcon]</span><br><span class="line">             +-03.0-[05]--</span><br><span class="line">             +-03.2-[06]--</span><br><span class="line">             +-05.0  Intel Corporation Xeon E5 v2/Core i7 VTd/Memory Map/Misc</span><br><span class="line">             +-05.2  Intel Corporation Xeon E5 v2/Core i7 IIO RAS</span><br><span class="line">             +-11.0-[07]--</span><br><span class="line">             +-16.0  Intel Corporation C600/X79 series chipset MEI Controller #1</span><br><span class="line">             +-16.1  Intel Corporation C600/X79 series chipset MEI Controller #2</span><br><span class="line">             +-1a.0  Intel Corporation C600/X79 series chipset USB2 Enhanced Host Controller #2</span><br><span class="line">             +-1c.0-[08]--</span><br><span class="line">             +-1c.7-[09-0d]----00.0-[0a-0d]--+-00.0-[0b-0c]----00.0-[0c]----00.0  Matrox Electronics Systems Ltd. G200eR2</span><br><span class="line">             |                               \-01.0-[0d]--</span><br><span class="line">             +-1d.0  Intel Corporation C600/X79 series chipset USB2 Enhanced Host Controller #1</span><br><span class="line">             +-1e.0-[0e]--</span><br><span class="line">             +-1f.0  Intel Corporation C600/X79 series chipset LPC Controller</span><br><span class="line">             \-1f.2  Intel Corporation C600/X79 series chipset 6-Port SATA AHCI Controller</span><br><span class="line"></span><br><span class="line">[root@kvm-2 ~]# lspci -xxx -s 7f:13.5        # x越多，列出的信息越详细</span><br><span class="line">7f:13.5 Performance counters: Intel Corporation Xeon E5 v2/Core i7 QPI Ring Performance Ring Monitoring (rev 04)</span><br><span class="line">00: 86 80 36 0e 00 00 00 00 04 00 01 11 10 00 80 00</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">[root@kvm-2 ~]# lspci -vv -s 7f:13.5         # v越多，列出的信息越详细</span><br><span class="line">7f:13.5 Performance counters: Intel Corporation Xeon E5 v2/Core i7 QPI Ring Performance Ring Monitoring (rev 04)</span><br><span class="line">        Subsystem: Dell Device 048c</span><br><span class="line">        Control: I/O- Mem- BusMaster- SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-</span><br><span class="line">        Status: Cap- 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast &gt;TAbort- &lt;TAbort- &lt;MAbort- &gt;SERR- &lt;PERR- INTx-</span><br></pre></td></tr></table></figure>
<h3 id="USB设备"><a href="#USB设备" class="headerlink" title="USB设备"></a>USB设备</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@kvm-2 ~]# lsusb           # usbutils包</span><br><span class="line">Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub</span><br><span class="line">Bus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub</span><br><span class="line">Bus 001 Device 002: ID 8087:0024 Intel Corp. Integrated Rate Matching Hub</span><br><span class="line">Bus 002 Device 002: ID 8087:0024 Intel Corp. Integrated Rate Matching Hub</span><br><span class="line">Bus 001 Device 003: ID 0624:0248 Avocent Corp. Virtual Hub</span><br><span class="line">Bus 001 Device 004: ID 0624:0249 Avocent Corp. Virtual Keyboard/Mouse</span><br><span class="line"></span><br><span class="line">[root@kvm-2 ~]# lsusb -vt</span><br><span class="line">Bus#  2</span><br><span class="line">`-Dev#   1 Vendor 0x1d6b Product 0x0002</span><br><span class="line">  `-Dev#   2 Vendor 0x8087 Product 0x0024</span><br><span class="line">Bus#  1</span><br><span class="line">`-Dev#   1 Vendor 0x1d6b Product 0x0002</span><br><span class="line">  `-Dev#   2 Vendor 0x8087 Product 0x0024</span><br><span class="line">    `-Dev#   3 Vendor 0x0624 Product 0x0248</span><br><span class="line">      `-Dev#   4 Vendor 0x0624 Product 0x0249</span><br></pre></td></tr></table></figure>
<h3 id="查看内核产生的硬件日志"><a href="#查看内核产生的硬件日志" class="headerlink" title="查看内核产生的硬件日志"></a>查看内核产生的硬件日志</h3><ul>
<li>/var/log/dmesg<br> 系统启动，一次性将启动时关于硬件的kernel日志写入该文件。</li>
<li>dmesg命名<br>  实时记录kernel日志，譬如插入USB设备，可使用dmesg命令查看。</li>
</ul>
<h3 id="获取硬件命令汇总"><a href="#获取硬件命令汇总" class="headerlink" title="获取硬件命令汇总"></a>获取硬件命令汇总</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">1. CPU型号</span><br><span class="line">dmidecode -t 4| awk &apos;/Version/ &amp;&amp; /CPU/&apos;|uniq</span><br><span class="line">2. CPU核数</span><br><span class="line">grep -c &apos;processor&apos; /proc/cpuinfo</span><br><span class="line">sysctl kern.smp.cpus|awk &apos;&#123;print $2&#125;&apos;</span><br><span class="line">3.内存大小</span><br><span class="line">free -m|awk &apos;/Mem/&#123;printf(&quot;%d\n&quot;, $2/1024+0.5)&#125;&apos;</span><br><span class="line">sysctl hw.physmem|awk &apos;&#123;printf (&quot;%d\n&quot;, $2/1073741824+0.5)&#125;&apos;</span><br><span class="line">4.硬盘总大小</span><br><span class="line">fdisk -l|awk &apos;/Disk \/dev\//&#123;sum +=$3&#125;END&#123;printf(&quot;%d\n&quot;, sum+0.5)&#125;&apos;</span><br><span class="line">sysctl kern.geom.conftxt|grep -Eo &apos;DISK \w+ \w+&apos;|awk &apos;&#123;sum +=$3&#125;END&#123;printf(&quot;%d\n&quot;, sum/1000000000+0.5)&#125;&apos; </span><br><span class="line">df -Pm | awk &apos;/^\/dev\//&#123;sum +=$2&#125;END&#123;printf(&quot;%d\n&quot;, sum/1024+0.5)&#125;&apos;</span><br><span class="line">5.主板型号</span><br><span class="line">dmidecode -t 1| awk &apos;/Product Name/&apos;</span><br><span class="line">6.取主板Serial</span><br><span class="line">dmidecode -t 1|awk &apos;/Serial/&apos;</span><br><span class="line"></span><br><span class="line">一句话脚本：</span><br><span class="line">dmidecode -t 4| awk &apos;/Version:/&apos;|tail -n1</span><br><span class="line"></span><br><span class="line">if [ `uname` == Linux ];then grep -c &apos;processor&apos; /proc/cpuinfo;else sysctl kern.smp.cpus|awk &apos;&#123;print $2&#125;&apos;;fi</span><br><span class="line"></span><br><span class="line">if [ `uname` == Linux ];then free -m|awk &apos;/Mem/&#123;printf(&quot;%d\n&quot;, $2/1000+0.5)&#125;&apos;;else sysctl hw.physmem|awk &apos;&#123;printf (&quot;%d\n&quot;, $2/1073741824+0.5)&#125;&apos;;fi</span><br><span class="line"></span><br><span class="line">if [ `uname` == Linux ];then fdisk -l|awk &apos;/Disk \/dev\//&#123;sum +=$3&#125;END&#123;printf(&quot;%d\n&quot;, sum+0.5)&#125;&apos;;else sysctl kern.geom.conftxt|egrep -o &apos;DISK \w+ \w+&apos;|awk &apos;&#123;sum +=$3&#125;END&#123;printf(&quot;%d\n&quot;, sum/1000000000+0.5)&#125;&apos;;fi</span><br><span class="line"></span><br><span class="line">dmidecode -t 1| awk &apos;/Product Name/&apos;</span><br></pre></td></tr></table></figure>
<h1 id="Understand-Software"><a href="#Understand-Software" class="headerlink" title="Understand Software"></a>Understand Software</h1><h2 id="Stoarge"><a href="#Stoarge" class="headerlink" title="Stoarge"></a>Stoarge</h2><p>Storage is very slow compare to memory. Linux has two way to compensate the issue.</p>
<ul>
<li>Caching<ul>
<li>Read from memory, write to memory</li>
<li>Read can be cached, write can be deferred</li>
</ul>
</li>
<li>I/O Schedulers<ul>
<li>Kernel attempt to recorder, coalesce I/O requests（临近扇区，合并读请求）</li>
<li>Minimize relocating magnetic head（最小化磁头的动作，重新定位）</li>
</ul>
</li>
</ul>
<h2 id="I-O-Schedulers"><a href="#I-O-Schedulers" class="headerlink" title="I/O Schedulers"></a>I/O Schedulers</h2><p>参考：<a href="http://www.cnblogs.com/cobbliu/p/5389556.html" target="_blank" rel="noopener">http://www.cnblogs.com/cobbliu/p/5389556.html</a></p>
<ul>
<li><p>cfq(Complete Fair Queuing)</p>
<ul>
<li><p>default schduler after kernel 2.6.18</p>
<p>它试图为竞争块设备使用权的所有进程分配一个请求队列和一个时间片，在调度器分配给进程的时间片内，进程可以将其读写请求发送给底层块设备，当进程的时间片消耗完，进程的请求队列将被挂起，等待调度。 </p>
<p>每个进程的时间片和每个进程的队列长度取决于进程的IO优先级，每个进程都会有一个IO优先级，CFQ调度器将会将其作为考虑的因素之一，来确定该进程的请求队列何时可以获取块设备的使用权。IO优先级从高到低可以分为三大类:RT(real time),BE(best try),IDLE(idle),其中RT和BE又可以再划分为8个子优先级。</p>
<p>实际上，我们已经知道CFQ调度器的公平是针对于进程而言的，而只有同步请求(read或syn write)才是针对进程而存在的，他们会放入进程自身的请求队列，而所有同优先级的异步请求，无论来自于哪个进程，都会被放入公共的队列，异步请求的队列总共有8(RT)+8(BE)+1(IDLE)=17个。</p>
</li>
<li><p>IO Priority</p>
<ul>
<li>Class 1(real time): first-access to disk, can starve（饿死） other classes<ul>
<li>0-7: The scheduling class data</li>
</ul>
</li>
<li>Class 2(best-effort): round-robin access, the default<ul>
<li>0-7: The scheduling class data</li>
</ul>
</li>
<li>Class 3(idle): receives disk I/O only if no other requests in queue</li>
</ul>
<p><strong>ionice命令可调节进程的IO优先级</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ionice -n0 -c1 -p pid</span><br><span class="line">ionice -n7 -c2 -p pid</span><br><span class="line">ionice -c3 -p pid        # 我不入地狱，谁入地狱</span><br><span class="line">ionice -c 2 -n 0 bash  # Runs ’bash’ as a best-effort program with highest priority.</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>deadline</p>
<ul>
<li>with predictable service time（可预见的服务时间）</li>
<li><p>for virtualization host</p>
<p>Deadline算法中引入了四个队列，这四个队列可以分为两类，每一类都由读和写两类队列组成，一类队列用来对请求按起始扇区序号进行排序，通过红黑树来组织，称为sort_list；另一类对请求按它们的生成时间进行排序，由链表来组织，称为fifo_list。每当确定了一个传输方向(读或写)，那么将会从相应的sort_list中将一批连续请求dispatch到requst_queue的请求队列里，具体的数目由fifo_batch来确定。只有下面三种情况才会导致一次批量传输的结束：</p>
</li>
</ul>
<ol>
<li>对应的sort_list中已经没有请求了</li>
<li>下一个请求的扇区不满足递增的要求</li>
<li><p>上一个请求已经是批量传输的最后一个请求了</p>
<p>所有的请求在生成时都会被赋上一个期限值(根据jiffies)，并按期限值排序在fifo_list中，读请求的期限时长默认为为500ms，写请求的期限时长默认为5s，可以看出内核对读请求是十分偏心的，其实不仅如此，在deadline调度器中，还定义了一个starved和writes_starved，writes_starved默认为2，可以理解为写请求的饥饿线，内核总是优先处理读请求，starved表明当前处理的读请求批数，只有starved超过了writes_starved后，才会去考虑写请求。因此，假如一个写请求的期限已经超过，该请求也不一定会被立刻响应，因为读请求的batch还没处理完，即使处理完，也必须等到starved超过writes_starved才有机会被响应。为什么内核会偏袒读请求？这是从整体性能上进行考虑的。读请求和应用程序的关系是同步的，因为应用程序要等待读取的内容完毕，才能进行下一步工作，因此读请求会阻塞进程，而写请求则不一样，应用程序发出写请求后，内存的内容何时写入块设备对程序的影响并不大，所以调度器会优先处理读请求。</p>
<p>默认情况下，读请求的超时时间是500ms，写请求的超时时间是5s。</p>
<p><a href="http://www.ibm.com/support/knowledgecenter/api/content/linuxonibm/liaat/liaatbestpractices_pdf.pdf" target="_blank" rel="noopener">这篇文章</a>说在一些多线程应用下，Deadline算法比CFQ算法好。<a href="https://www.percona.com/blog/2009/01/30/linux-schedulers-in-tpcc-like-benchmark/" target="_blank" rel="noopener">这篇文章</a>说在一些数据库应用下，Deadline算法比CFQ算法好。</p>
</li>
</ol>
</li>
<li><p>anticipatory(AS)</p>
<ul>
<li>wait for a while after read request</li>
<li><p>for sequential read workloads（大量顺序读的）</p>
<p>Anticipatory算法从Linux 2.6.33版本后，就被移除了，因为CFQ通过配置也能达到Anticipatory算法的效果。</p>
</li>
</ul>
</li>
<li><p>noop(No Operation)</p>
<ul>
<li>quick to response, low CPU overhead</li>
<li><p>for SSD,virtualization guests（宿主机使用了deadline，则虚拟机使用noop，因为真正写盘操作是主机完成）</p>
<p>Noop调度算法也叫作电梯调度算法，它将IO请求放入到一个FIFO队列中，然后逐个执行这些IO请求，当然对于一些在磁盘上连续的IO请求，Noop算法会适当做一些合并。这个调度算法特别适合那些不希望调度器重新组织IO请求顺序的应用。</p>
<p>这种调度算法在以下场景中优势比较明显：</p>
</li>
</ul>
<ol>
<li><p>在IO调度器下方有更加智能的IO调度设备。如果您的Block Device Drivers是Raid，或者SAN，NAS等存储设备，这些设备会更好地组织IO请求，不用IO调度器去做额外的调度工作；</p>
<ol start="2">
<li><p>上层的应用程序比IO调度器更懂底层设备。或者说上层应用程序到达IO调度器的IO请求已经是它经过精心优化的，那么IO调度器就不需要画蛇添足，只需要按序执行上层传达下来的IO请求即可。</p>
</li>
<li><p>对于一些非旋转磁头氏的存储设备，使用Noop的效果更好。因为对于旋转磁头式的磁盘来说，IO调度器的请求重组要花费一定的CPU时间，但是对于SSD磁盘来说，这些重组IO请求的CPU时间可以节省下来，因为SSD提供了更智能的请求调度算法，不需要内核去画蛇添足。</p>
</li>
</ol>
</li>
</ol>
</li>
</ul>
<h3 id="Queue"><a href="#Queue" class="headerlink" title="Queue"></a>Queue</h3><ul>
<li><p>/sys/block/\&lt;device>/queue/nr_requests</p>
<p>  磁盘请求队列长度（一次性交给磁盘的请求数量）。增大它会牺牲更多内存。 </p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cat /sys/block/sda/queue/nr_requests </span><br><span class="line">128</span><br></pre></td></tr></table></figure>
</li>
<li><p>/sys/block/\&lt;device>/queue/read_ahead_kb</p>
<p>  预先读数据块大小，对于大量的连续读业务，可以增大它。</p>
</li>
</ul>
<h3 id="I-O-Scheduler-Manage"><a href="#I-O-Scheduler-Manage" class="headerlink" title="I/O Scheduler Manage"></a>I/O Scheduler Manage</h3><ul>
<li><p>/sys/block/\&lt;device>/queue/scheduler<br>  切换I/O调度算法。</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cat /sys/block/sda/queue/scheduler </span><br><span class="line">noop anticipatory deadline [cfq]</span><br></pre></td></tr></table></figure>
</li>
<li><p>每种调度算法的可调参数</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 该目录会根据不同Schduler而变化</span><br><span class="line"># cd /sys/block/sda/queue/iosched/</span><br><span class="line">[root@kvm-2 iosched]# ls</span><br><span class="line">back_seek_max      fifo_expire_async  group_idle       low_latency  slice_async     slice_idle</span><br><span class="line">back_seek_penalty  fifo_expire_sync   group_isolation  quantum      slice_async_rq  slice_sync</span><br></pre></td></tr></table></figure>
</li>
<li><p>CFQ</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">/sys/block/&lt;device&gt;/queue/iosched/slice_idle </span><br><span class="line">当一个进程的队列被分配到时间片却没有 IO 请求时，调度器在轮询至下一个队列之前的等待时间，以提升 IO 的局部性，对于 SSD 设备，可以将这个值设为 0。</span><br><span class="line"></span><br><span class="line">/sys/block/&lt;device&gt;/queue/iosched/quantum </span><br><span class="line">一个进程的队列每次被处理 IO 请求的最大数量，默认为 4，RHEL6 为 8，增大这个值可以提升并行处理 IO 的性能，但可能会造成某些 IO 延迟问题。</span><br><span class="line"></span><br><span class="line">/sys/block/&lt;device&gt;/queue/iosched/slice_async_rq </span><br><span class="line">一次处理写请求的最大数</span><br><span class="line"></span><br><span class="line">/sys/block/&lt;device&gt;/queue/iosched/low_latency </span><br><span class="line">如果IO延迟的问题很严重，将这个值设为 1</span><br></pre></td></tr></table></figure>
</li>
<li><p>Deadline</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">/sys/block/&lt;device&gt;/queue/iosched/writes_starved </span><br><span class="line">进行一个写操作之前，允许进行多少次读操作</span><br><span class="line"></span><br><span class="line">/sys/block/&lt;device&gt;/queue/iosched/read_expire </span><br><span class="line">读请求的过期时间，默认为 5ms</span><br><span class="line"></span><br><span class="line">/sys/block/&lt;device&gt;/queue/iosched/write_expire </span><br><span class="line">写请求的过期时间，默认为 500ms</span><br><span class="line"></span><br><span class="line">/sys/block/sda/queue/iosched/front_merges </span><br><span class="line">是否进行前合并</span><br></pre></td></tr></table></figure>
</li>
<li><p>Anticipatory</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">/sys/block/&lt;device&gt;/queue/iosched/antic_expire </span><br><span class="line">预测等待时长，默认为 6ms</span><br><span class="line"></span><br><span class="line">/sys/block/&lt;device&gt;/queue/iosched/&#123;write_expire,read_expire&#125; </span><br><span class="line">读写请求的超时时长</span><br><span class="line"></span><br><span class="line">/sys/block/&lt;device&gt;/queue/iosched/&#123;write_batch_expire,read_batch_expire&#125; </span><br><span class="line">读写的批量处理时长</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Tuning-Theory"><a href="#Tuning-Theory" class="headerlink" title="Tuning Theory"></a>Tuning Theory</h3><ul>
<li>L: Queue length: average number of requests waiting in the system</li>
<li>A: Arrival rate: the rate at which requests enter a system</li>
<li>W: Wait time: average time to satisfy a request<ul>
<li>also known as wall clock,latency,response time, or residence time</li>
</ul>
</li>
</ul>
<p>L = A * W</p>
<h3 id="Queue-Length"><a href="#Queue-Length" class="headerlink" title="Queue Length"></a>Queue Length</h3><ul>
<li>Requests are buffered in memory</li>
<li>L may be read-write tunable or a read-only measurement</li>
</ul>
<h3 id="Wait-Time"><a href="#Wait-Time" class="headerlink" title="Wait Time"></a>Wait Time</h3><ul>
<li>Includes<ul>
<li>Queue time（排队时间）</li>
<li>Service time（服务时间）</li>
</ul>
</li>
<li>Tactics（策略）<ul>
<li>Reduce queue time</li>
<li>Reduce service time</li>
</ul>
</li>
</ul>
<p>L = A <em> W = A </em> (T<sub>q</sub> + T<sub>s</sub>)</p>
<h3 id="Service-Time"><a href="#Service-Time" class="headerlink" title="Service Time"></a>Service Time</h3><ul>
<li>Includes<ul>
<li>Sysem time: time in kernel mode</li>
<li>User time: time in user mode(doing useful work)</li>
</ul>
</li>
<li>Tactics<ul>
<li>Reduce system time(blocks user mode operations)</li>
<li>spend as much time as needed in user mode</li>
</ul>
</li>
</ul>
<p>L = A <em> W = A </em> (Q + S) = A * (T<sub>q</sub> + (T<sub>sys</sub> + T<sub>user</sub>))</p>
<h3 id="Summary-of-Queue-Theory"><a href="#Summary-of-Queue-Theory" class="headerlink" title="Summary of Queue Theory"></a>Summary of Queue Theory</h3><ul>
<li>L: Queue length</li>
<li>A: Arrival rate(requests/second)</li>
<li>W: Wait time(latency, time to satisfy a request)</li>
<li>Q: Queue time</li>
<li>S: Service time(includes system time, user time)</li>
<li><p>C: Complete rate(requests/second)</p>
<ul>
<li>Steady state: A = C</li>
<li>L = A <em> W = A </em> (Q + S) = A * (T<sub>q</sub> + (T<sub>sys</sub> + T<sub>user</sub>))</li>
</ul>
</li>
</ul>
<h3 id="Summary-of-strategies"><a href="#Summary-of-strategies" class="headerlink" title="Summary of strategies"></a>Summary of strategies</h3><ul>
<li>Tune L<ul>
<li>Constrain queue length</li>
<li>Sort the queue to prefer reads</li>
</ul>
</li>
<li>Tune A or C<ul>
<li>Reduce visit count by distributing across multiple resources(SMP,RAID)</li>
<li>Defer resource visits until think time(lazy write)</li>
<li>Improve throughput for resource(more efficient protocol, less overhead)</li>
</ul>
</li>
<li>Tune W<ul>
<li>Use expiration time for requests</li>
<li>use resources with smaller service time(in-memory cache vs disk)</li>
</ul>
</li>
</ul>
<h2 id="Kernel-Module"><a href="#Kernel-Module" class="headerlink" title="Kernel Module"></a>Kernel Module</h2><h3 id="Module-commands"><a href="#Module-commands" class="headerlink" title="Module commands"></a>Module commands</h3><ul>
<li><p>lsmod</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># lsmod </span><br><span class="line">Module                  Size  Used by</span><br><span class="line">tcp_diag                1041  0 </span><br><span class="line">inet_diag               8735  1 tcp_diag    # 表示该模块被tcp_diag依赖，使用次数为1</span><br><span class="line">ip6table_filter         2889  0 </span><br><span class="line">ip6_tables             18732  1 ip6table_filter</span><br><span class="line">ebtable_nat             2009  0 </span><br><span class="line">ebtables               18135  1 ebtable_nat</span><br><span class="line">ipt_MASQUERADE          2466  3 </span><br><span class="line">iptable_nat             6158  1 </span><br><span class="line">nf_nat                 22759  2 ipt_MASQUERADE,iptable_nat</span><br><span class="line">nf_conntrack_ipv4       9506  4 iptable_nat,nf_nat</span><br><span class="line">nf_defrag_ipv4          1483  1 nf_conntrack_ipv4</span><br><span class="line">xt_state                1492  1 </span><br><span class="line">nf_conntrack           79758  5 ipt_MASQUERADE,iptable_nat,nf_nat,nf_conntrack_ipv4,xt_state</span><br><span class="line">ipt_REJECT              2351  2</span><br></pre></td></tr></table></figure>
</li>
<li><p>/lib/modules/\&lt;kernel-release>/kernel/</p>
<p>  内核模块所在目录。</p>
</li>
<li><p>modinfo [ modulename… ]</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># modinfo sx8</span><br><span class="line">filename:       /lib/modules/2.6.32-431.el6.x86_64/kernel/drivers/block/sx8.ko</span><br><span class="line">version:        1.0</span><br><span class="line">description:    Promise SATA SX8 block driver</span><br><span class="line">license:        GPL</span><br><span class="line">author:         Jeff Garzik</span><br><span class="line">srcversion:     4772099AB984FE59198263E</span><br><span class="line">alias:          pci:v0000105Ad00008002sv*sd*bc*sc*i*</span><br><span class="line">alias:          pci:v0000105Ad00008000sv*sd*bc*sc*i*</span><br><span class="line">depends:        </span><br><span class="line">vermagic:       2.6.32-431.el6.x86_64 SMP mod_unload modversions </span><br><span class="line">parm:           max_queue:Maximum number of queued commands. (min==1, max==30, safe==1) (int)</span><br></pre></td></tr></table></figure>
</li>
<li><p>modprobe [ modulename… ]</p>
</li>
<li>rmmod [ modulename… ]</li>
</ul>
<h3 id="Modules-parameters"><a href="#Modules-parameters" class="headerlink" title="Modules parameters"></a>Modules parameters</h3><ul>
<li><p>查看某模块有哪些参数可调整</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># modinfo -p usb_storage</span><br><span class="line">quirks:supplemental list of device IDs and their quirks</span><br><span class="line">delay_use:seconds to delay before using a new device</span><br><span class="line">swi_tru_install:TRU-Install mode (1=Full Logic (def), 2=Force CD-Rom, 3=Force Modem)</span><br><span class="line">option_zero_cd:ZeroCD mode (1=Force Modem (default), 2=Allow CD-Rom</span><br><span class="line"></span><br><span class="line"># modinfo -p sx8</span><br><span class="line">max_queue:Maximum number of queued commands. (min==1, max==30, safe==1)</span><br></pre></td></tr></table></figure>
</li>
<li><p>自定义参数</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># cat /etc/modprobe.d/my.conf </span><br><span class="line">options usb_storage delay_use=3</span><br><span class="line">options st buffer_kbs=128</span><br><span class="line">options sx8 max_queue=10</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用modprobe命令重新加载这些模块，自定义的参数就会生效</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># modprobe usb_storage</span><br><span class="line"># modprobe st</span><br><span class="line"># modprobe sx8</span><br></pre></td></tr></table></figure>
</li>
<li><p>Check runtime module parameters</p>
<ul>
<li>/sys/module/<modulename>/parameters/<pname></pname></modulename></li>
</ul>
</li>
<li><p>Automatically loading modules</p>
<ul>
<li>/etc/sysconfig/modules/my.modules</li>
<li><p>Linux init脚本会执行以上目录下modules结尾的文件。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">modprobe usb_storage|st|sx8</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h2 id="Tuned"><a href="#Tuned" class="headerlink" title="Tuned"></a>Tuned</h2><ul>
<li>Tune a system on the fly as needed</li>
<li>Based on tuning profiles<ul>
<li>max power saving</li>
<li>max disk performance</li>
<li>self made profile allowed（允许自定义tune方案）</li>
<li>profile can even has monitoring program to run（还支持运行监控程序）</li>
</ul>
</li>
<li>SysV service<ul>
<li>tuned</li>
<li>ktune</li>
</ul>
</li>
<li>Can be used with crond to switch between profiles<ul>
<li>0 7 <em> </em> * 1-5 /usr/bin/tuned-adm profile throughput-performance</li>
<li>0 20 <em> </em> * 1-5 /usr/bin/tuned-adm profile server-powersave</li>
</ul>
</li>
</ul>
<h3 id="Use-Tuned"><a href="#Use-Tuned" class="headerlink" title="Use Tuned"></a>Use Tuned</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@cmdb-192-168-21-241 ~]# cat /etc/redhat-release </span><br><span class="line">CentOS Linux release 7.2.1511 (Core) </span><br><span class="line"></span><br><span class="line"># tuned-adm active</span><br><span class="line">Current active profile: virtual-guest</span><br><span class="line"></span><br><span class="line"># tuned-adm list</span><br><span class="line">Available profiles:</span><br><span class="line">- balanced                    - General non-specialized tuned profile</span><br><span class="line">- desktop                     - Optimize for the desktop use-case</span><br><span class="line">- latency-performance         - Optimize for deterministic performance at the cost of increased power consumption</span><br><span class="line">- network-latency             - Optimize for deterministic performance at the cost of increased power consumption, focused on low latency network performance</span><br><span class="line">- network-throughput          - Optimize for streaming network throughput, generally only necessary on older CPUs or 40G+ networks</span><br><span class="line">- powersave                   - Optimize for low power consumption</span><br><span class="line">- throughput-performance      - Broadly applicable tuning that provides excellent performance across a variety of common server workloads</span><br><span class="line">- virtual-guest               - Optimize for running inside a virtual guest</span><br><span class="line">- virtual-host                - Optimize for running KVM guests</span><br><span class="line">Current active profile: virtual-guest</span><br><span class="line"></span><br><span class="line"># tuned-adm profile powersave</span><br></pre></td></tr></table></figure>
<ul>
<li><p>latency-performance</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># grep -vE &apos;^#|^$&apos; /usr/lib/tuned/latency-performance/tuned.conf </span><br><span class="line">[main]</span><br><span class="line">summary=Optimize for deterministic performance at the cost of increased power consumption</span><br><span class="line">[cpu]</span><br><span class="line">force_latency=1</span><br><span class="line">governor=performance</span><br><span class="line">energy_perf_bias=performance</span><br><span class="line">min_perf_pct=100</span><br><span class="line">[sysctl]</span><br><span class="line">kernel.sched_min_granularity_ns=10000000</span><br><span class="line">vm.dirty_ratio=10</span><br><span class="line">vm.dirty_background_ratio=3</span><br><span class="line">vm.swappiness=10</span><br><span class="line">kernel.sched_migration_cost_ns=5000000</span><br></pre></td></tr></table></figure>
</li>
<li><p>throughput-performance</p>
</li>
</ul>
<h3 id="Custom-Tuning-Profiles"><a href="#Custom-Tuning-Profiles" class="headerlink" title="Custom Tuning Profiles"></a>Custom Tuning Profiles</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/lib/tuned/</span><br><span class="line"># mkdir test-performance</span><br><span class="line"></span><br><span class="line"># vim test-performance/tuned.conf</span><br><span class="line">[main]</span><br><span class="line">include=latency-performance</span><br><span class="line">summary=Test profile that uses settings for latency-performance tuning profile</span><br><span class="line"></span><br><span class="line"># tuned-adm list</span><br><span class="line">......</span><br><span class="line">- test-performance            - Test profile that uses settings for latency-performance tuning profile</span><br><span class="line">......</span><br></pre></td></tr></table></figure>
<h2 id="CPU-Tuning"><a href="#CPU-Tuning" class="headerlink" title="CPU Tuning"></a>CPU Tuning</h2><h3 id="Interrupt-and-IRQ"><a href="#Interrupt-and-IRQ" class="headerlink" title="Interrupt and IRQ"></a>Interrupt and IRQ</h3><p>中断请求（IRQ）是用于服务的请求，在硬件层发出。可使用专用硬件线路或者跨硬件总线的信息数据包（消息信号中断，MSI ）发出中断。启用中断后，接收 IRQ 后会提示切换到中断上下文。</p>
<p>CPU绑定后，它仍然要服务于中断。应该将中断绑定至那些非隔离的CPU上，从而避免那些隔离的CPU处理中断程序；</p>
<p>/proc/interrupts文件列出每个I/O 设备中每个 CPU 的中断数，每个 CPU 核处理的中断数，中断类型，以及用逗号分开的注册为接收中断的驱动程序列表。（详情请参考 proc(5) man page：man 5 proc）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# cat /proc/interrupts </span><br><span class="line">           CPU0       CPU1       </span><br><span class="line">  0:      14678          0   IO-APIC-edge      timer</span><br><span class="line">  1:          2          0   IO-APIC-edge      i8042</span><br><span class="line">  4:          2          0   IO-APIC-edge    </span><br><span class="line">  7:          0          0   IO-APIC-edge      parport0</span><br><span class="line">  8:          1          0   IO-APIC-edge      rtc0</span><br><span class="line">  9:          0          0   IO-APIC-fasteoi   acpi</span><br><span class="line"> 12:          4          0   IO-APIC-edge      i8042</span><br><span class="line"> 14:   45394223          0   IO-APIC-edge      ata_piix</span><br><span class="line"> 15:          0          0   IO-APIC-edge      ata_piix</span><br><span class="line"> 16:         56   16232636   IO-APIC-fasteoi   i915, p2p1</span><br><span class="line"> 18:    5333843   11365439   IO-APIC-fasteoi   uhci_hcd:usb4</span><br><span class="line"> 20:    2277759          0   IO-APIC-fasteoi   ata_piix</span><br><span class="line"> 21:          3          0   IO-APIC-fasteoi   ehci_hcd:usb1, uhci_hcd:usb2</span><br><span class="line"> 22:          0          0   IO-APIC-fasteoi   uhci_hcd:usb3</span><br><span class="line"> 23:       3813       6412   IO-APIC-fasteoi   uhci_hcd:usb5, Intel ICH7</span><br><span class="line">......</span><br><span class="line"># APIC表示高级可编程中断控制器（Advanced Programmable Interrupt Controlle）</span><br><span class="line"># APIC是SMP体系的核心，通过APIC可以将中断分发到不同的CPU 来处理。</span><br><span class="line"># i915：Intel i915 集成显卡驱动</span><br></pre></td></tr></table></figure>
<h3 id="Soft-Interrupt-and-Context-Switch"><a href="#Soft-Interrupt-and-Context-Switch" class="headerlink" title="Soft Interrupt and Context Switch"></a>Soft Interrupt and Context Switch</h3><p>上下文切换（也称做进程切换或任务切换）是指 CPU 从一个进程或线程切换到另一个进程或线程。</p>
<p>CPU通过时间片分配算法来循环执行任务，当前任务执行一个时间片后会切换到下一个任务。但是，在切换前会保存上一个任务的状态，以便下次切换回这个任务时，可以再次加载这个任务的状态，从任务保存到再加载的过程就是一次上下文切换。</p>
<h3 id="查看中断"><a href="#查看中断" class="headerlink" title="查看中断"></a>查看中断</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># mpstat</span><br><span class="line"># dstat -c</span><br></pre></td></tr></table></figure>
<h3 id="将IRQ绑定CPU"><a href="#将IRQ绑定CPU" class="headerlink" title="将IRQ绑定CPU"></a>将IRQ绑定CPU</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># echo CPU_MASK &gt; /proc/irq/&lt;irq number&gt;/smp_affinity</span><br><span class="line"></span><br><span class="line"># 案例：将CPU中断绑定到CPU #0,#1上。</span><br><span class="line"># echo 3 &gt; /proc/irq/&lt;irq number&gt;/smp_affinity</span><br></pre></td></tr></table></figure>
<p>将IRQ绑定到某个CPU，那么最好在系统启动时，将那个CPU隔离起来，不被scheduler通常的调度。<br>可以通过在Linux kernel中加入启动参数：isolcpus=cpu-list将CPU隔离起来。</p>
<h3 id="IRQ-Irqbalance"><a href="#IRQ-Irqbalance" class="headerlink" title="IRQ Irqbalance"></a>IRQ Irqbalance</h3><p>irqbalance用于优化中断分配，它会自动收集系统数据以分析使用模式，并依据系统负载状况将工作状态置于 Performance mode 或 Power-save mode。处于Performance mode 时，irqbalance 会将中断尽可能均匀地分发给各个 CPU core，以充分利用 CPU 多核，提升性能。</p>
<p>处于Power-save mode 时，irqbalance 会将中断集中分配给第一个 CPU，以保证其它空闲 CPU 的睡眠时间，降低能耗</p>
<p>但是在实时系统中会导致中断自动漂移，对性能造成不稳定因素，在高性能的场合建议关闭。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># /etc/init.d/irqbalance stop</span><br></pre></td></tr></table></figure>
<h3 id="查看上下文切换"><a href="#查看上下文切换" class="headerlink" title="查看上下文切换"></a>查看上下文切换</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># sar -w           # 查看上下文切换的平均次数，以及进程创建的平均值</span><br><span class="line"># vmstat 1 3       # 每秒上下文切换次数</span><br></pre></td></tr></table></figure>
<h3 id="如何减少上下文切换"><a href="#如何减少上下文切换" class="headerlink" title="如何减少上下文切换"></a>如何减少上下文切换</h3><pre><code>既然上下文切换会导致额外的开销，因此减少上下文切换次数便可以提高多线程程序的运行效率。减少上下文切换的方法有无锁并发编程、CAS算法、使用最少线程和使用协程。

- 无锁并发编程。多线程竞争时，会引起上下文切换，所以多线程处理数据时，可以用一些办法来避免使用锁，如将数据的ID按照Hash取模分段，不同的线程处理不同段的数据
- CAS算法。Java的Atomic包使用CAS算法来更新数据，而不需要加锁
- 使用最少线程。避免创建不需要的线程，比如任务很少，但是创建了很多线程来处理，这样会造成大量线程都处于等待状态
- 协程。在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换
</code></pre><h2 id="CPU-Iffinity（姻亲关系）"><a href="#CPU-Iffinity（姻亲关系）" class="headerlink" title="CPU Iffinity（姻亲关系）"></a>CPU Iffinity（姻亲关系）</h2><p>当软中断和上下文切换过大时。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">taskset</span><br><span class="line">mask       二进程  #CPU</span><br><span class="line">0x0000 0001  0001：node 0</span><br><span class="line">0x0000 0003  0011：node 0和1</span><br><span class="line">0x0000 0005  0101：node 0和2</span><br><span class="line">0x0000 0007  0111：node 0-2</span><br><span class="line"></span><br><span class="line"># taskset -p mask pid</span><br><span class="line">101, 3# CPU</span><br><span class="line"># taskset -p 0x00000005 101</span><br><span class="line"></span><br><span class="line">绑定进程101，CPU 0-2#、7#</span><br><span class="line">taskset -p -c 0-2,7 101</span><br><span class="line"></span><br><span class="line"># 指定CPU启动进程</span><br><span class="line">taskset mask -- program</span><br><span class="line">taskset -c 0,5,7-9 – myprogram</span><br></pre></td></tr></table></figure>
<h3 id="CPU-子系统"><a href="#CPU-子系统" class="headerlink" title="CPU 子系统"></a>CPU 子系统</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># mkdir /cpusets</span><br><span class="line"># vim /etc/fstab</span><br><span class="line">cpuset                  /cpusets                cpuset  defaults        0 0</span><br><span class="line"># mount -a</span><br><span class="line"># ls /cpusets/</span><br><span class="line">cgroup.clone_children  cgroup.sane_behavior  cpuset.mem_exclusive   cpuset.memory_pressure          cpuset.memory_spread_slab  cpuset.sched_relax_domain_level  tasks</span><br><span class="line">cgroup.event_control   cpuset.cpu_exclusive  cpuset.mem_hardwall    cpuset.memory_pressure_enabled  cpuset.mems                notify_on_release</span><br><span class="line">cgroup.procs           cpuset.cpus           cpuset.memory_migrate  cpuset.memory_spread_page       cpuset.sched_load_balance  release_agent</span><br><span class="line">#</span><br><span class="line"># cat /cpusets/cpuset.cpus </span><br><span class="line">0-3</span><br><span class="line"></span><br><span class="line"># 创建子域</span><br><span class="line"># mkdir /cpusets/domain1</span><br><span class="line"># ls /cpusets/domain1/</span><br><span class="line">......</span><br><span class="line"># echo 0-1 &gt;/cpusets/domain1/cpuset.cpus   #将CPU #0,#1绑定进来</span><br><span class="line"># echo 0 &gt;/cpusets/domain1/cpuset.mems     #将内存绑定进来</span><br><span class="line"># echo #pid /cpusets/domain1/tasks         #将某个进程绑定进来，该进程只能在CPU 0-1上运行</span><br><span class="line"></span><br><span class="line"># ps -e -o psr,pid,cmd</span><br></pre></td></tr></table></figure>
<h2 id="Memory-Tuning"><a href="#Memory-Tuning" class="headerlink" title="Memory Tuning"></a>Memory Tuning</h2><h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><ul>
<li>MMU：memory manager unit内存管理单元</li>
<li>TLB：缓存MMU转换的结果，使用大内存页提高性能</li>
</ul>
<h3 id="swap"><a href="#swap" class="headerlink" title="swap"></a>swap</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">vm.swappiness=&#123;0..100&#125;：使用交换分区的倾向性, 默认60</span><br><span class="line">	overcommit_memory=2： 过量使用（0：启发式过量；1：总是过量；2：允许下述超出百分比）</span><br><span class="line">	overcommit_ratio=50：</span><br><span class="line">		可用内存：swap+RAM*ratio</span><br><span class="line">			swap: 2G</span><br><span class="line">			RAM: 8G</span><br><span class="line">		        可用内存：memory=2G+8G*50%=6G</span><br><span class="line"></span><br><span class="line">	充分使用物理内存：</span><br><span class="line">		1、swap跟RAM一样大；</span><br><span class="line">		2、overcommit_memory=2, overcommit_ratio=100：swappiness=0；</span><br><span class="line">			memory: swap+ram</span><br><span class="line">	参考设置：</span><br><span class="line">		1、Batch compute（批处理计算）：&lt;= 4 * RAM</span><br><span class="line">		2、Database server：&lt;= 1G</span><br><span class="line">		3、Application server：&gt;= 0.5 * RAM</span><br><span class="line"></span><br><span class="line">默认值：</span><br><span class="line">[root@nginx1 ~]# cat /proc/sys/vm/overcommit_memory </span><br><span class="line">0</span><br><span class="line">[root@nginx1 ~]# cat /proc/sys/vm/swappiness </span><br><span class="line">60</span><br><span class="line">[root@nginx1 ~]# cat /proc/sys/vm/overcommit_ratio </span><br><span class="line">50</span><br></pre></td></tr></table></figure>
<h3 id="swap-tuning"><a href="#swap-tuning" class="headerlink" title="swap tuning"></a>swap tuning</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 在每个磁盘上建立swap分区，并给与相同优先级</span><br><span class="line">/dev/sda1   swap swap pri=5 0 0</span><br><span class="line">/dev/sdb1   swap swap pri=5 0 0</span><br><span class="line">/dev/sdc1   swap swap pri=1 0 0    # 最慢的磁盘，swap最小的优先级</span><br></pre></td></tr></table></figure>
<h3 id="min-free-kbytes"><a href="#min-free-kbytes" class="headerlink" title="min_free_kbytes"></a>min_free_kbytes</h3><ul>
<li>/proc/sys/vm/min_free_kbytes<br>强制Linux VM最低保留多少空闲内存（Kbytes）</li>
</ul>
<p>内存管理从三个层次管理内存，分别是node, zone ,page。64位的x86物理机内存从高地址到低地址分为: Normal DMA32 DMA</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@kvm-2 ~]# grep Node /proc/zoneinfo </span><br><span class="line">Node 0, zone      DMA</span><br><span class="line">Node 0, zone    DMA32</span><br><span class="line">Node 0, zone   Normal</span><br><span class="line">Node 1, zone   Normal</span><br></pre></td></tr></table></figure>
<p>每个zone都有自己的min low high,如下，但是单位是page。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[root@kvm-2 ~]# grep &quot;Node 0, zone&quot; -A10 /proc/zoneinfo </span><br><span class="line">Node 0, zone      DMA</span><br><span class="line">  pages free     3934</span><br><span class="line">        min      3</span><br><span class="line">        low      3</span><br><span class="line">        high     4</span><br><span class="line">        scanned  0</span><br><span class="line">        spanned  4095</span><br><span class="line">        present  3835</span><br><span class="line">    nr_free_pages 3934</span><br><span class="line">    nr_inactive_anon 0</span><br><span class="line">    nr_active_anon 0</span><br><span class="line">--</span><br><span class="line">Node 0, zone    DMA32</span><br><span class="line">  pages free     571977</span><br><span class="line">        min      749</span><br><span class="line">        low      936</span><br><span class="line">        high     1123</span><br><span class="line">        scanned  0</span><br><span class="line">        spanned  1044480</span><br><span class="line">        present  756520</span><br><span class="line">    nr_free_pages 571977</span><br><span class="line">    nr_inactive_anon 0</span><br><span class="line">    nr_active_anon 0</span><br><span class="line">--</span><br><span class="line">Node 0, zone   Normal</span><br><span class="line">  pages free     4737209</span><br><span class="line">        min      9478</span><br><span class="line">        low      11847</span><br><span class="line">        high     14217</span><br><span class="line">        scanned  0</span><br><span class="line">        spanned  9699328</span><br><span class="line">        present  9566720</span><br><span class="line">    nr_free_pages 4737209</span><br><span class="line">    nr_inactive_anon 166</span><br><span class="line">    nr_active_anon 3973945</span><br></pre></td></tr></table></figure>
<p>上面可知：low = 5/4 <em> min、high = 3/2 </em> min。</p>
<p>min 和 low的区别：</p>
<ol>
<li>min下的内存是保留给内核使用的；当到达min，会触发内存的direct reclaim</li>
<li>low水位比min高一些，当内存可用量小于low的时候，会触发 kswapd reclaim。当kswapd慢慢的将内存 回收到high水位，就开始继续睡眠</li>
</ol>
<ul>
<li>/proc/sys/vm/extra_free_kbytes</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@kvm-2 ~]# sysctl  -a | grep free</span><br><span class="line">vm.min_free_kbytes = 90112</span><br><span class="line">vm.extra_free_kbytes = 0</span><br></pre></td></tr></table></figure>
<p>意义：low = min_free_kbytes*5/4 + extra_free_kbytes</p>
<ul>
<li>总结<br>调整该内存的内核参数的时候！调大的风险远大于调小的风险，会导致频繁的触发内存回收！如果有人想将vm.min_free_kbytes 调大，千万要注意当前的Free，一旦超过Free内存，会立刻触发direct reclaim。</li>
</ul>
<h3 id="Dirty-Page"><a href="#Dirty-Page" class="headerlink" title="Dirty Page"></a>Dirty Page</h3><p>因为硬盘的读写速度远赶不上内存的速度，系统就把读写比较频繁的数据事先放到内存中，以提高读写速度，这就叫高速缓存，linux是以页作为高速缓存的单位，当进程修改了高速缓存里的数据时，该页就被内核标记为脏页，内核将会在合适的时间把脏页的数据写到磁盘中去，以保持高速缓存中的数据和磁盘中的数据是一致的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@kvm-2 ~]# sysctl -a | grep dirty</span><br><span class="line">vm.dirty_background_ratio = 10</span><br><span class="line">vm.dirty_background_bytes = 0</span><br><span class="line">vm.dirty_ratio = 20</span><br><span class="line">vm.dirty_bytes = 0</span><br><span class="line">vm.dirty_writeback_centisecs = 500</span><br><span class="line">vm.dirty_expire_centisecs = 3000</span><br><span class="line"></span><br><span class="line">[root@kvm-2 ~]# sysctl -a | grep pdflush</span><br><span class="line">vm.nr_pdflush_threads = 0</span><br></pre></td></tr></table></figure>
<ul>
<li><p>vm.dirty_background_ratio</p>
<p>  这个参数指定了当文件系统缓存脏页数量达到系统内存百分之多少时（如5%）就会触发pdflush/flush/kdmflush等后台回写进程运行，将一定缓存的脏页<strong>异步</strong>地刷入外存；</p>
</li>
<li><p>vm.dirty_ratio</p>
<p>  这个参数则指定了当文件系统缓存脏页数量达到系统内存百分之多少时（如10%），系统不得不开始处理缓存脏页（因为此时脏页数量已经比较多，为了避免数据丢失需要将一定脏页刷入外存）；在此过程中很多应用进程可能会因为系统转而处理文件IO而阻塞。</p>
</li>
</ul>
<blockquote>
<p>之前一直错误的以为dirty_ratio的触发条件不可能达到，因为每次肯定会先达到vm.dirty_background_ratio的条件，后来才知道自己理解错了。确实是先达到vm.dirty_background_ratio的条件然后触发flush进程进行异步的回写操作，但是这一过程中应用进程仍然可以进行写操作，如果多个应用进程写入的量大于flush进程刷出的量那自然会达到vm.dirty_ratio这个参数所设定的坎，此时操作系统会转入同步地处理脏页的过程，阻塞应用进程。</p>
</blockquote>
<ul>
<li><p>vm.nr_pdflush_threads<br>  pdflush是linux系统后台运行的一个线程，这个进程负责把page cahce中的dirty状态的数据定期的输入磁盘。</p>
<p>  /proc/sys/vm/nr_pdflush_threads查看当前系统运行pdflush数量。当一段时间（一般是1s）没有任何的pdflush处于工作状态，系统会remove一个pdflush线程。pdflush最大和最小的数量是有配置的，但这些配置一般很少修改。</p>
</li>
<li><p>vm.dirty_writeback_centisecs</p>
<p>  默认一般是500（单位是1/100秒）。这个参数表示5s的时间pdflush就会被唤起去刷新脏数据。建议用户使用默认值。</p>
</li>
<li><p>vm.dirty_expire_centisecs</p>
<p>  默认是3000（单位是1/100秒）。这个值表示page cache中的数据多久之后被标记为脏数据。只有标记为脏的数据在下一个周期到来时pdflush才会刷入到磁盘，这样就意味着用户写的数据在30秒之后才有可能被刷入磁盘，在这期间断电都是会丢数据的。</p>
</li>
<li><p>drop_caches（干净页的回收，缓存清理）</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@nginx1 ~]# cat /proc/sys/vm/drop_caches</span><br><span class="line">0</span><br><span class="line">[root@nginx1 ~]# sync  #先将缓存写入磁盘</span><br><span class="line">[root@nginx1 ~]# echo 1 /proc/sys/vm/drop_caches  #释放所有页缓冲内存</span><br><span class="line">[root@nginx1 ~]# echo 2 /proc/sys/vm/drop_caches  #释放未使用的slab缓冲内存</span><br><span class="line">[root@nginx1 ~]# echo 3 /proc/sys/vm/drop_caches  #释放所有页缓冲和slab缓冲内存</span><br><span class="line">备注：</span><br><span class="line">slab缓存详解（一）http://blog.chinaunix.net/uid-27102327-id-3268687.html</span><br><span class="line">slab缓存详解（二）http://blog.chinaunix.net/uid-27102327-id-3268711.html</span><br><span class="line">http://blog.csdn.net/hs794502825/article/details/7981524</span><br></pre></td></tr></table></figure>
<h3 id="OOM-Kill"><a href="#OOM-Kill" class="headerlink" title="OOM Kill"></a>OOM Kill</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@nginx1 ~]# cat /proc/sys/vm/panic_on_oom     # 0:on, 1:off</span><br><span class="line">0</span><br><span class="line">[root@nginx1 ~]# echo 1 /proc/sys/vm/panic_on_oom  # 关闭oom kill，不推荐</span><br><span class="line">注意：由调整的进程衍生的进程将继承该进程的 oom_score。例如：如果 sshd 进程不受 oom _killer 功能影响，所有由 SSH 会话产生的进程都将不受其影响。</span><br><span class="line"></span><br><span class="line">当内存耗尽时，系统使用oom kill杀死大oom_score（-16~15，2的平方）的进程。oom_score得分由oom_adj得来。</span><br><span class="line">减小oom-adj值，避免被系统杀死：</span><br><span class="line"># echo -17 &gt; /proc/$(pidof sshd)/oom_adj</span><br><span class="line"></span><br><span class="line">-17：避免oom_killer杀死自己</span><br><span class="line">-16~15：帮助计算oom_score</span><br><span class="line">16：预留的最低级别，一般对于缓存的进程才有可能设置成这个级别</span><br></pre></td></tr></table></figure>
<p>有时free查看还有充足的内存，但还是会触发OOM，是因为该进程可能占用了特殊的内存地址空间。</p>
<h3 id="Huge-Page"><a href="#Huge-Page" class="headerlink" title="Huge Page"></a>Huge Page</h3><p>操作系统默认的内存是以4KB分页的，而虚拟地址和内存地址需要转换， 而这个转换要查表，CPU为了加速这个查表过程会内建TLB(Translation Lookaside Buffer)。 显然，如果虚拟页越小，表里的条目数也就越多，而TLB大小是有限的，条目数越多TLB的Cache Miss也就会越高， 所以如果我们能启用大内存页就能间接降低TLB Cache Miss。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@jump204_76 ~]# sysctl -w vm.nr_hugepages=128</span><br><span class="line">vm.nr_hugepages = 128</span><br><span class="line">[root@jump204_76 ~]# cat /proc/meminfo | grep Huge</span><br><span class="line">AnonHugePages:     77824 kB</span><br><span class="line">HugePages_Total:     128</span><br><span class="line">HugePages_Free:      128</span><br><span class="line">HugePages_Rsvd:        0</span><br><span class="line">HugePages_Surp:        0</span><br><span class="line">Hugepagesize:       2048 kB</span><br></pre></td></tr></table></figure>
<h3 id="Transparent-HugePages（透明大页）"><a href="#Transparent-HugePages（透明大页）" class="headerlink" title="Transparent HugePages（透明大页）"></a>Transparent HugePages（透明大页）</h3><ul>
<li><p>什么是Transparent HugePages（透明大页）?</p>
<p>  简单的讲，对于内存占用较大的程序，可以通过开启HugePage来提升系统性能。但这里会有个要求，就是在编写程序时，代码里需要显示的对HugePage进行支持。</p>
<p>  而红帽企业版Linux为了减少程序开发的复杂性，并对HugePage进行支持，部署了Transparent HugePages。Transparent HugePages是一个使管理Huge Pages自动化的抽象层，实现方案为操作系统后台有一个叫做khugepaged的进程，它会一直扫描所有进程占用的内存，在可能的情况下会把4kPage交换为Huge Pages。</p>
<p>  为什么Transparent HugePages（透明大页）对系统的性能会产生影响？<br>  在khugepaged进行扫描进程占用内存，并将4kPage交换为Huge Pages的这个过程中，对于操作的内存的各种分配活动都需要各种内存锁，直接影响程序的内存访问性能。并且，这个过程对于应用是透明的，在应用层面不可控制,对于专门为4k page优化的程序来说，可能会造成随机的性能下降现象。</p>
</li>
<li><p>怎么设置Transparent HugePages（透明大页）?</p>
<ol>
<li><p>查看是否启用透明大页</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@venus153 ~]# cat /sys/kernel/mm/transparent_hugepage/enabled </span><br><span class="line">[always] madvise never</span><br><span class="line">使用命令查看时，如果输出结果为[always]表示透明大页启用了，[never]表示透明大页禁用。</span><br></pre></td></tr></table></figure>
</li>
<li><p>关闭透明大页</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled </span><br><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br></pre></td></tr></table></figure>
<ol start="3">
<li><p>启用透明大页</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo always &gt;  /sys/kernel/mm/transparent_hugepage/enabled </span><br><span class="line">echo always &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>设置开机关闭</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># vim /etc/rc.local</span><br><span class="line">if test -f /sys/kernel/mm/redhat_transparent_hugepage/enabled; then     </span><br><span class="line">     echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled     </span><br><span class="line">    echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag </span><br><span class="line">fi</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
</ul>
<h3 id="进程间通信"><a href="#进程间通信" class="headerlink" title="进程间通信"></a>进程间通信</h3><p>管理命令<br>ipcs<br>ipcrm</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">shm:</span><br><span class="line">	shmmni: 系统级别，所允许使用的共享内存段上限；</span><br><span class="line">	shmall: 系统级别，能够为共享内存分配使用的最大页面数；</span><br><span class="line">	shmmax: 单个共享内存段的上限；</span><br><span class="line">messages:</span><br><span class="line">	msgmnb: 单个消息队列的上限，单位为字节；</span><br><span class="line">	msgmni: 系统级别，消息队列个数上限；</span><br><span class="line">	msgmax: 单个消息大小的上限，单位为字节；</span><br><span class="line"></span><br><span class="line">[root@localhost ~]# sysctl -a | grep shm</span><br><span class="line">kernel.shmmax = 68719476736</span><br><span class="line">kernel.shmall = 4294967296        #单位page，等于4294967296*4096 byte</span><br><span class="line">kernel.shmmni = 4096</span><br><span class="line">kernel.shm_rmid_forced = 0</span><br><span class="line">vm.hugetlb_shm_group = 0</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@nginx1 ~]# ipcs -l</span><br><span class="line"></span><br><span class="line">------ Shared Memory Limits --------</span><br><span class="line">max number of segments = 4096                #段最大数量</span><br><span class="line">max seg size (kbytes) = 67108864               #段大小</span><br><span class="line">max total shared memory (kbytes) = 17179869184</span><br><span class="line">min seg size (bytes) = 1</span><br><span class="line"></span><br><span class="line">------ Semaphore Limits --------</span><br><span class="line">max number of arrays = 128</span><br><span class="line">max semaphores per array = 250</span><br><span class="line">max semaphores system wide = 32000</span><br><span class="line">max ops per semop call = 32</span><br><span class="line">semaphore max value = 32767</span><br><span class="line"></span><br><span class="line">------ Messages: Limits --------</span><br><span class="line">max queues system wide = 32768</span><br><span class="line">max size of message (bytes) = 65536</span><br><span class="line">default max size of queue (bytes) = 65536</span><br></pre></td></tr></table></figure>
<h2 id="I-O-Tuning"><a href="#I-O-Tuning" class="headerlink" title="I/O Tuning"></a>I/O Tuning</h2><h3 id="File-System"><a href="#File-System" class="headerlink" title="File System"></a>File System</h3><ul>
<li><p>Ext4</p>
<ul>
<li><p>内节点表初始化</p>
<p>对于超大文件系统，mkfs.ext4  进程要花很长时间初始化文件系统中到所有内节点表。可使用 -Elazy_itable_init=1 选项延迟这个进程。如果使用这个选项，内核进程将在挂载文件系统后继续初始化<br>该文件它。可使用 mount 命令的 -o init_itable=n 选项控制发生初始化到比例，其中执行这个后台初始化的时间约为 1/n。n 的默认值为 10。</p>
</li>
<li><p>Auto-fsync 行为</p>
<p>因为在重命名、截取或者重新写入某个现有文件后，有些应用程序不总是可以正确执行 fsync()，在重命名和截取操作后，ext4  默认自动同步文件。这个行为与原有到 ext3 文件系统行为大致相同。但 fsync()<br>操作可能会很耗时，因此如果不需要这个自动行为，请在 m ount 命令后使用 -o noauto_da_alloc 选项禁用它。这意味着该程序必须明确使用 fsync() 以保证数据一致。</p>
</li>
<li><p>日志  I/O  优先权</p>
<p>默认情况下，日志注释 I/O 比普通 I/O 的优先权稍高。这个优先权可使用 mount 命令的journal_ioprio=n 选项控制。默认值为 3。有效值范围为 0 -7，其中 0  时最高优先权 I /O。</p>
</li>
</ul>
</li>
<li><p>XFS</p>
</li>
</ul>
<h2 id="Network-Tuning"><a href="#Network-Tuning" class="headerlink" title="Network Tuning"></a>Network Tuning</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tcp_max_tw_buckets: 只允许调大</span><br><span class="line">	tw：保存timewait的连接个数</span><br><span class="line">		established --&gt; tw</span><br></pre></td></tr></table></figure>
<h1 id="结尾"><a href="#结尾" class="headerlink" title="结尾"></a>结尾</h1><ul>
<li>性能观测工具</li>
</ul>
<p><img src="/images/linux/system-tuning/linux_observability_tools.png" alt="linux_observability_tools"></p>
<ul>
<li>性能测评工具</li>
</ul>
<p><img src="/images/linux/system-tuning/linux_benchmarking_tools.png" alt="linux_benchmarking_tools"></p>
<ul>
<li>性能调优工具</li>
</ul>
<p><img src="/images/linux/system-tuning/linux_tuning_tools.png" alt="linux_tuning_tools"></p>
<ul>
<li>sar命令</li>
</ul>
<p><img src="/images/linux/system-tuning/linux_observability_sar.png" alt="linux_observability_sar"></p>
<ul>
<li><a href="http://www.brendangregg.com/linuxperf.html" target="_blank" rel="noopener">Linux Performance大全</a></li>
</ul>
<p><img src="/images/linux/system-tuning/linux_perf_tools_full.png" alt="linux_perf_tools_full"></p>

          
        
      
    </div>

    

    
    
    

    

    
       
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/11/07/linux/Overview-of-performance-tuning/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/11/07/linux/Overview-of-performance-tuning/" class="post-title-link" itemprop="http://yoursite.com/index.html">Linux System Tuning</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-11-07 08:55:29" itemprop="dateCreated datePublished" datetime="2018-11-07T08:55:29+08:00">2018-11-07</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2018-12-04 16:54:48" itemprop="dateModified" datetime="2018-12-04T16:54:48+08:00">2018-12-04</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/Linux/" itemprop="url" rel="index"><span itemprop="name">Linux</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><h2 id="Purpose-of-Performance-Tuning"><a href="#Purpose-of-Performance-Tuning" class="headerlink" title="Purpose of Performance Tuning"></a>Purpose of Performance Tuning</h2><ul>
<li>将系统调节成扮演某个角色。譬如：数据库服务器、WEB服务器、文件服务器、邮件服务器等等。</li>
<li>找到并缓解系统瓶颈</li>
<li>调优指标：响应速度，吞吐量<ul>
<li>CPU、内存等硬件在最优情况下等达到最高的性能，你必须清楚。</li>
</ul>
</li>
</ul>
<h2 id="Required-Skills"><a href="#Required-Skills" class="headerlink" title="Required Skills"></a>Required Skills</h2><ul>
<li>Understand both hardware and software</li>
<li>Collecting and analysis of measurable relevant data about a performance problem</li>
<li>Set proper expectations</li>
<li>5 years full time system management experience</li>
</ul>
<h2 id="Tuning-Efficiency（调优效率）"><a href="#Tuning-Efficiency（调优效率）" class="headerlink" title="Tuning Efficiency（调优效率）"></a>Tuning Efficiency（调优效率）</h2><ul>
<li><p>Business Level Tuning</p>
<ul>
<li>Ask right question: “Reduce CPU utilization” or “Business goal”</li>
<li>Adjust workflow（调整业务的流程，减少对系统的不必要请求）</li>
<li>Removing unused services<ul>
<li>PC Smart Card Daemon</li>
<li>Buletooth and hidd</li>
</ul>
</li>
<li>Do i really need the default cron jobs?<ul>
<li>/etc/cron.daily/makewhatis.cron</li>
<li>/etc/cron.daily/mlocate.cron</li>
</ul>
</li>
</ul>
</li>
<li><p>Application Level Tuning</p>
<ul>
<li>Disable or defer expensive operations until analysis?（禁用或延迟对系统而言“很贵的”操作）<ul>
<li>Disable reverse name lookups</li>
<li>Set loglevel to warn for most production daemons</li>
</ul>
</li>
<li>Is syslogd a bottleneck?<ul>
<li>Daemon uses fsync() to flush every file write（系统为保证日志文件不丢失，会立刻调用fsync方法将数据写入磁盘）</li>
<li>Disable by prepending hyphen to name of log file in /etc/rsyslog.conf（在日志文件加“-”号，该日志会滞后写入）</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>应用层调优，调的是应用程序本身。譬如你使用nginx、nfs，它们本身有大量的参数可用来调优。在应用层调优远远优于内核调优。</p>
<ul>
<li>Kernel Level Tuning(RH442)</li>
</ul>
<p>从上往下优化空间越来越小，效果越来越不明显。譬如目前你的WEB Server是apache，调优前首先考虑是否非得使用apache，我们的业务是否是高并发，能不能换成nginx。能够在顶层解决问题，尽量不要希望在底层去解决。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>一个命令敲下去，性能提高10%、20%，这是不切合实际的。红帽操作系统已经调优过了，我们是为了某个特定的角色再进行调优。</li>
<li>不同角色的系统有不同调优的参数，不能指望一个参数搞定所有事情。</li>
<li>物理级别的缺陷，比如硬盘、网卡等由于寿命原因性能大幅下降，则系统层面调优见效甚微。</li>
</ul>
<h1 id="Understand-Hardware"><a href="#Understand-Hardware" class="headerlink" title="Understand Hardware"></a>Understand Hardware</h1><h2 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h2><h3 id="X86架构机器三个特点（RH442）"><a href="#X86架构机器三个特点（RH442）" class="headerlink" title="X86架构机器三个特点（RH442）"></a>X86架构机器三个特点（RH442）</h3><ul>
<li>I/O Address</li>
<li>IRQ（中断请求）</li>
<li>DMA（直接内存存取）</li>
</ul>
<h4 id="查看CPU"><a href="#查看CPU" class="headerlink" title="查看CPU"></a>查看CPU</h4><ul>
<li><p>总览</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"># lscpu </span><br><span class="line">Architecture:          x86_64           #x86架构</span><br><span class="line">CPU op-mode(s):        32-bit, 64-bit</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                40               #逻辑CPU个数</span><br><span class="line">On-line CPU(s) list:   0-39</span><br><span class="line">Thread(s) per core:    2                #每核的线程数</span><br><span class="line">Core(s) per socket:    10               #每颗CPU的核数</span><br><span class="line">Socket(s):             2                #物理CPU个数</span><br><span class="line">NUMA node(s):          2                # NUMA节点数</span><br><span class="line">Vendor ID:             GenuineIntel</span><br><span class="line">CPU family:            6</span><br><span class="line">Model:                 79</span><br><span class="line">Model name:            Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz</span><br><span class="line">Stepping:              1</span><br><span class="line">CPU MHz:               1200.093</span><br><span class="line">BogoMIPS:              4805.86</span><br><span class="line">Virtualization:        VT-x</span><br><span class="line">L1d cache:             32K             #一级数据缓存</span><br><span class="line">L1i cache:             32K             #一级指令缓存</span><br><span class="line">L2 cache:              256K            #二级缓存</span><br><span class="line">L3 cache:              25600K          #三级缓存</span><br><span class="line">NUMA node0 CPU(s):     0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38</span><br><span class="line">NUMA node1 CPU(s):     1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39</span><br><span class="line"></span><br><span class="line"># lscpu </span><br><span class="line">Architecture:          x86_64           #x86架构</span><br><span class="line">CPU op-mode(s):        32-bit, 64-bit</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                8                #逻辑CPU个数</span><br><span class="line">On-line CPU(s) list:   0-7</span><br><span class="line">Thread(s) per core:    1                #每核的线程数</span><br><span class="line">Core(s) per socket:    4                #每颗CPU的核数</span><br><span class="line">Socket(s):             2                #物理CPU个数</span><br><span class="line">NUMA node(s):          2                # NUMA节点数</span><br><span class="line">Vendor ID:             GenuineIntel</span><br><span class="line">CPU family:            6</span><br><span class="line">Model:                 62</span><br><span class="line">Stepping:              4</span><br><span class="line">CPU MHz:               2499.904</span><br><span class="line">BogoMIPS:              4999.28</span><br><span class="line">Virtualization:        VT-x</span><br><span class="line">L1d cache:             32K             #一级数据缓存</span><br><span class="line">L1i cache:             32K             #一级指令缓存</span><br><span class="line">L2 cache:              256K            #二级缓存</span><br><span class="line">L3 cache:              10240K          #三级缓存</span><br><span class="line">NUMA node0 CPU(s):     0,2,4,6</span><br><span class="line">NUMA node1 CPU(s):     1,3,5,7</span><br></pre></td></tr></table></figure>
</li>
<li><p>核数</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># grep CPU /proc/cpuinfo</span><br><span class="line">model name      : Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz</span><br><span class="line">model name      : Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz</span><br><span class="line">......</span><br><span class="line">model name      : Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz</span><br><span class="line">model name      : Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz</span><br><span class="line"># grep CPU /proc/cpuinfo | wc -l</span><br><span class="line">40</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="CPU架构"><a href="#CPU架构" class="headerlink" title="CPU架构"></a>CPU架构</h3><ul>
<li>SMP架构：Symmetric Multi-Processor</li>
</ul>
<p>大多数早期的单处理机系统的设计为让每个 CPU 到每个内存位置都使用同一逻辑路径（一般是平行总线）。这样每次 CPU 访问任意位置的内存时与其他系统中的 CPU 对内存的访问消耗的时间是相同的。此类架构就是我们所说的同步多处理器（SMP）系统。SMP 适合 CPU 数较少的系统，但一旦 CPU 计数超过某一点（8 或者 16），要满足对内存的平等访问所需的平行 trace 数就会使用过多的板载资源，留给外设的空间就太少。</p>
<p><img src="/images/linux/system-tuning/uma.png" alt="uma"></p>
<ul>
<li>MPP架构：Massive Parallel Processing</li>
</ul>
<p>和 NUMA 不同， MPP 提供了另外一种进行系统扩展的方式，它由多个 SMP 服务器通过一定的节点互联网络进行连接，协同工作，完成相同的任务，从用户的角度来看是一个服务器系统。其基本特征是由多个 SMP 服务器 ( 每个 SMP 服务器称节点 ) 通过节点互联网络连接而成，每个节点只访问自己的本地资源 ( 内存、存储等 ) ，是一种完全无共享 (Share Nothing) 结构，因而扩展能力最好，理论上其扩展无限制，目前的技术可实现 512 个节点互联，数千个 CPU 。目前业界对节点互联网络暂无标准，如 NCR 的 Bynet ， IBM 的 SPSwitch ，它们都采用了不同的内部实现机制。但节点互联网仅供 MPP 服务器内部使用，对用户而言是透明的。</p>
<p>在 MPP 系统中，每个 SMP 节点也可以运行自己的操作系统、数据库等。但和 NUMA 不同的是，它不存在异地内存访问的问题。换言之，每个节点内的 CPU 不能访问另一个节点的内存。节点之间的信息交互是通过节点互联网络实现的，这个过程一般称为数据重分配 (Data Redistribution) 。</p>
<p>但是 MPP 服务器需要一种复杂的机制来调度和平衡各个节点的负载和并行处理过程。目前一些基于 MPP 技术的服务器往往通过系统级软件 ( 如数据库 ) 来屏蔽这种复杂性。举例来说， NCR 的 Teradata 就是基于 MPP 技术的一个关系数据库软件，基于此数据库来开发应用时，不管后台服务器由多少个节点组成，开发人员所面对的都是同一个数据库系统，而不需要考虑如何调度其中某几个节点的负载。</p>
<p>MPP (Massively Parallel Processing)，大规模并行处理系统，这样的系统是由许多松耦合的处理单元组成的，要注意的是这里指的是处理单元而不是处理器。每个单元内的CPU都有自己私有的资源，如总线，内存，硬盘等。在每个单元内都有操作系统和管理数据库的实例复本。这种结构最大的特点在于不共享资源。</p>
<ul>
<li>UNMA架构：Non-Uniform Memory Access</li>
</ul>
<p>不是为每个处理器包提供对等的内存访问，而是让每个包/插槽组合有一个或者多个专用内存区以便提供高速访问。每个插槽还有到另一个插槽的互联以便提供对其他插槽内存的低速访问。</p>
<p>下图中 CPU0 访问左边的内存条大约需要三个时钟周期：一个周期是将地址发给内存控制器，一个周期是设置对该内存位置的访问，一个周期是读取或者写入到该位置。但 CPU1 可能需要 6 个时钟周期方可访问内存的同一位置，因为它位于不同的插槽，必须经过两个内存控制器：插槽 1 中的本地内存控制器和插槽 0  中的远程内存控制器。如果在那个位置出现竞争（即如果有一个以上 CPU 同时尝试访问同一位置），内存控制器需要对该内存进行随机且连续的访问，所以内存访问所需时间会较长。添加缓存一致性（保证本地 CPU 缓存包含同一内存位置的相同数据）会让此过程更为复杂。</p>
<p><img src="/images/linux/system-tuning/numa.png" alt="numa"></p>
<p><img src="/images/linux/system-tuning/numa-mac.png" alt="numa"></p>
<ol>
<li>当CPU有多颗时，物理CPU常见通讯方法有三种：</li>
</ol>
<ul>
<li>FSB（传统的前端总线，常用于PC机）</li>
<li>QPI(intel)</li>
<li>HyperTransport(AMD)</li>
</ul>
<h2 id="Numa-Architecture"><a href="#Numa-Architecture" class="headerlink" title="Numa Architecture"></a>Numa Architecture</h2><h3 id="查看Numa-node"><a href="#查看Numa-node" class="headerlink" title="查看Numa node"></a>查看Numa node</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># numactl --hardware</span><br><span class="line">available: 2 nodes (0-1)     #当前机器有2个NUMA node，编号0、1</span><br><span class="line">node 0 cpus: 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30</span><br><span class="line">node 0 size: 32722 MB     #物理内存大小</span><br><span class="line">node 0 free: 2352 MB      #当前free内存大小</span><br><span class="line">node 1 cpus: 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31</span><br><span class="line">node 1 size: 32768 MB</span><br><span class="line">node 1 free: 12314 MB</span><br><span class="line">node distances:           #node距离，可以简单认为Node内部访问及跨Node访问的成本。</span><br><span class="line">node   0   1 </span><br><span class="line">  0:  10  20              #由此可知跨node访问内存的成本是 内部访问的2倍。</span><br><span class="line">  1:  20  10</span><br></pre></td></tr></table></figure>
<h3 id="查看node"><a href="#查看node" class="headerlink" title="查看node"></a>查看node</h3><ul>
<li><p>node0包含的CPU</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@nginx1 ~]# ls -l /sys/devices/system/node/node0/</span><br><span class="line">cpu0/    cpu12/   cpu16/   cpu2/    cpu22/   cpu26/   cpu30/   cpu6/    cpulist </span><br><span class="line">cpu10/   cpu14/   cpu18/   cpu20/   cpu24/   cpu28/   cpu4/    cpu8/    cpumap</span><br></pre></td></tr></table></figure>
</li>
<li><p>/sys/devices/system/cpu</p>
<p>  系统的 CPU 是如何互相连接的信息。</p>
</li>
<li><p>/sys/devices/system/node</p>
<p>  系统中 NUMA 节点以及那些节点间相对距离的信息。</p>
</li>
</ul>
<h3 id="查看cpu-cache"><a href="#查看cpu-cache" class="headerlink" title="查看cpu cache"></a>查看cpu cache</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@kvm-2 ~]# ls -l /sys/devices/system/cpu/cpu0/cache/</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x 2 root root 0 Nov 10  2017 index0   # 1级数据cache</span><br><span class="line">drwxr-xr-x 2 root root 0 Nov  3  2017 index1   # 1级指令cache</span><br><span class="line">drwxr-xr-x 2 root root 0 Nov  3  2017 index2   # 2级cache</span><br><span class="line">drwxr-xr-x 2 root root 0 Nov 10  2017 index3   # 3级cache ,对应cpuinfo里的cache</span><br></pre></td></tr></table></figure>
<p>目录里的文件是cache信息描述，以本机的cpu0/index0为例简单解释一下：</p>
<table><br>    <tr><br>        <td>文件</td><br>        <td>内容</td><br>        <td>说明</td><br>   </tr><br>   <tr><br>        <td>type</td><br>        <td>Data</td><br>        <td>数据cache，如果查看index1就是Instruction</td><br>   </tr><br>   <tr><br>        <td>Level</td><br>        <td>1</td><br>        <td>L1</td><br>    </tr><br>    <tr><br>        <td>Size</td><br>        <td>32K</td><br>        <td>大小为32K</td><br>    </tr><br>    <tr><br>        <td>coherency_line_size</td><br>        <td>64</td><br>        <td rowspan="4">64<em>4</em>128=32K</td><br>    </tr><br>    <tr><br>        <td>physical_line_partition</td><br>        <td>1</td><br>    </tr><br>    <tr><br>        <td>ways_of_associativity</td><br>        <td>4</td><br>    </tr><br>    <tr><br>        <td>number_of_sets</td><br>        <td>128</td><br>    </tr><br>    <tr><br>        <td>shared_cpu_map</td><br>        <td>00000101</td><br>        <td>示这个cache被CPU0和CPU8 share</td><br>    </tr><br></table>

<p>解释一下shared_cpu_map内容的格式：</p>
<p>表面上看是2进制，其实是16进制表示，每个bit表示一个cpu，1个数字可以表示4个cpu<br>截取00000101的后4位，转换为2进制表示。</p>
<table>
<thead>
<tr>
<th>CPU id</th>
<th>15</th>
<th>14</th>
<th>13</th>
<th>12</th>
<th>11</th>
<th>10</th>
<th>9</th>
<th>8</th>
<th>7</th>
<th>6</th>
<th>5</th>
<th>4</th>
<th>3</th>
<th>2</th>
<th>1</th>
<th>0</th>
</tr>
</thead>
<tbody>
<tr>
<td>0×0101的2进制表示</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>0101表示cpu8和cpu0，即cpu0的L1 data cache是和cpu8共享的。</p>
<p>再看一下index3 shared_cpu_map的例子</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cat /sys/devices/system/cpu/cpu0/cache/index3/shared_cpu_map</span><br><span class="line">00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000f0f</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>CPU id</th>
<th>15</th>
<th>14</th>
<th>13</th>
<th>12</th>
<th>11</th>
<th>10</th>
<th>9</th>
<th>8</th>
<th>7</th>
<th>6</th>
<th>5</th>
<th>4</th>
<th>3</th>
<th>2</th>
<th>1</th>
<th>0</th>
</tr>
</thead>
<tbody>
<tr>
<td>0x0f0f的2进制表示</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>cpu0,1,2,3和cpu8,9,10,11共享L3 cache。</p>
<h3 id="查看numa状态"><a href="#查看numa状态" class="headerlink" title="查看numa状态"></a>查看numa状态</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># numastat</span><br><span class="line">                           node0           node1</span><br><span class="line">numa_hit              1011487518       716368222</span><br><span class="line">numa_miss                      0       145365467</span><br><span class="line">numa_foreign           145365467               0</span><br><span class="line">interleave_hit             20673           20631</span><br><span class="line">local_node            1011487341       716343592</span><br><span class="line">other_node                   177       145390097</span><br></pre></td></tr></table></figure>
<p>上述可知node 0的unma_miss过高，可考虑用numactl将进程和CPU绑定。详见CPU调优章节。</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>numa_hit</td>
<td>为这个节点成功的分配尝试数。</td>
</tr>
<tr>
<td>numa_miss</td>
<td>由于在目的节点中内存较低而尝试为这个节点分配到另一个节点的数目。每个 numa_miss 事件都在另一个节点中有对应的 numa_foreign 事件。</td>
</tr>
<tr>
<td>numa_foreign</td>
<td>最初要为这个节点但最后分配个另一个节点的分配数。每个 numa_foreign 事件都在另一个节点中有对应的 numa_miss 事件。</td>
</tr>
<tr>
<td>interleave_hit</td>
<td>成功分配给这个节点的尝试交错策略数。</td>
</tr>
<tr>
<td>local_node</td>
<td>这个节点中的进程成功在这个节点中分配内存的次数。</td>
</tr>
<tr>
<td>other_node</td>
<td>这个节点中的进程成功在另一个节点中分配内存的次数。</td>
</tr>
</tbody>
</table>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@nginx1 ~]# numactl --show</span><br><span class="line">policy: default</span><br><span class="line">preferred node: current</span><br><span class="line">physcpubind: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 </span><br><span class="line">cpubind: 0 1</span><br><span class="line">nodebind: 0 1</span><br><span class="line">membind: 0 1</span><br></pre></td></tr></table></figure>
<h3 id="查看内存numa-node分布"><a href="#查看内存numa-node分布" class="headerlink" title="查看内存numa node分布"></a>查看内存numa node分布</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/&lt;pid&gt;/numa_maps</span><br><span class="line">cat /proc/$(pidof pname|cut -d &quot;&quot; -f1)/numa_maps</span><br></pre></td></tr></table></figure>
<h3 id="查看线程run在哪个processor"><a href="#查看线程run在哪个processor" class="headerlink" title="查看线程run在哪个processor"></a>查看线程run在哪个processor</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">top -p $(pidof pname|sed -e &apos;s/ /,/g&apos;)</span><br><span class="line"></span><br><span class="line">在默认配置下不显示线程信息，需要进入Top后按“shift+H”，打开线程显示。</span><br><span class="line">另外，如果没有P列，还需要按“f”，按“j”，添加，这一列显示的数字就是这个线程上次run的processor id。</span><br></pre></td></tr></table></figure>
<p><a href="https://blog.jcole.us/2010/09/28/mysql-swap-insanity-and-the-numa-architecture/" target="_blank" rel="noopener">https://blog.jcole.us/2010/09/28/mysql-swap-insanity-and-the-numa-architecture/</a><br><a href="https://software.intel.com/en-us/articles/intel-64-architecture-processor-topology-enumeration/" target="_blank" rel="noopener">https://software.intel.com/en-us/articles/intel-64-architecture-processor-topology-enumeration/</a></p>
<h3 id="numad"><a href="#numad" class="headerlink" title="numad"></a>numad</h3><pre><code>numad 是一个自动 NUMA 亲和性管理守护进程，它监控系统中的 NUMA 拓扑以及资源使用以便动态提高 NUMA 资源分配和管理（以及系统性能）。

numad 不会在进程只运行几分钟或者不会消耗很多资源时改进性能。

有连续不可预测内存访问的系统，比如大型内存中的数据库也不大可能从 numad 使用中受益。
</code></pre><h2 id="CPU-Cache"><a href="#CPU-Cache" class="headerlink" title="CPU Cache"></a>CPU Cache</h2><p>静态RAM（SRAM）集成度低（存储相同的数据，静态RAM的体积是动态RAM的6倍），价格高（同容量的静态RAM是动态RAM的四倍）；动态RAM（DRAM）。静态RAM缓存叫一级缓存，而把动态RAM叫二级缓存。</p>
<h3 id="查看CPU-Cache"><a href="#查看CPU-Cache" class="headerlink" title="查看CPU Cache"></a>查看CPU Cache</h3><pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"># lscpu -p</span><br><span class="line"># The following is the parsable format, which can be fed to other</span><br><span class="line"># programs. Each different item in every column has an unique ID</span><br><span class="line"># starting from zero.</span><br><span class="line"># CPU,Core,Socket,Node,,L1d,L1i,L2,L3    # “Node”表示NUMA nodes</span><br><span class="line">0,0,0,0,,0,0,0,0</span><br><span class="line">1,1,1,1,,1,1,1,1</span><br><span class="line">2,2,0,0,,2,2,2,0</span><br><span class="line">3,3,1,1,,3,3,3,1</span><br><span class="line">4,4,0,0,,4,4,4,0</span><br><span class="line">5,5,1,1,,5,5,5,1</span><br><span class="line">6,6,0,0,,6,6,6,0</span><br><span class="line">7,7,1,1,,7,7,7,1</span><br><span class="line"></span><br><span class="line">[root@kvm-2 ~]# dmidecode -t 7</span><br><span class="line"># dmidecode 2.11</span><br><span class="line">SMBIOS 2.7 present.</span><br><span class="line"></span><br><span class="line">Handle 0x0700, DMI type 7, 19 bytes</span><br><span class="line">Cache Information</span><br><span class="line">        Socket Designation: Not Specified</span><br><span class="line">        Configuration: Enabled, Not Socketed, Level 1</span><br><span class="line">        Operational Mode: Write Through</span><br><span class="line">        Location: Internal</span><br><span class="line">        Installed Size: 128 kB         #一级缓存（32k * 4）</span><br><span class="line">        Maximum Size: 128 kB</span><br><span class="line">        Supported SRAM Types:</span><br><span class="line">                Unknown</span><br><span class="line">        Installed SRAM Type: Unknown</span><br><span class="line">        Speed: Unknown</span><br><span class="line">        Error Correction Type: Single-bit ECC</span><br><span class="line">        System Type: Data</span><br><span class="line">        Associativity: 8-way Set-associative</span><br><span class="line"></span><br><span class="line">Handle 0x0701, DMI type 7, 19 bytes</span><br><span class="line">Cache Information</span><br><span class="line">        Socket Designation: Not Specified</span><br><span class="line">        Configuration: Enabled, Not Socketed, Level 2</span><br><span class="line">        Operational Mode: Write Through</span><br><span class="line">        Location: Internal</span><br><span class="line">        Installed Size: 1024 kB       #二级缓存（256k * 4）</span><br><span class="line">        Maximum Size: 1024 kB</span><br><span class="line">        Supported SRAM Types:</span><br><span class="line">                Unknown</span><br><span class="line">        Installed SRAM Type: Unknown</span><br><span class="line">        Speed: Unknown</span><br><span class="line">        Error Correction Type: Single-bit ECC</span><br><span class="line">        System Type: Unified</span><br><span class="line">        Associativity: 8-way Set-associative</span><br><span class="line"></span><br><span class="line">Handle 0x0702, DMI type 7, 19 bytes</span><br><span class="line">Cache Information</span><br><span class="line">        Socket Designation: Not Specified</span><br><span class="line">        Configuration: Enabled, Not Socketed, Level 3</span><br><span class="line">        Operational Mode: Write Back</span><br><span class="line">        Location: Internal</span><br><span class="line">        Installed Size: 10240 kB    #与lscpu数值一样，说明4核共享三级缓存</span><br><span class="line">        Maximum Size: 10240 kB</span><br><span class="line">        Supported SRAM Types:</span><br><span class="line">                Unknown</span><br><span class="line">        Installed SRAM Type: Unknown</span><br><span class="line">        Speed: Unknown</span><br><span class="line">        Error Correction Type: Single-bit ECC</span><br><span class="line">        System Type: Unified</span><br><span class="line">        Associativity: 20-way Set-associative</span><br></pre></td></tr></table></figure>
</code></pre><p>由上输出可得知该服务器的L1，L2是每个核心独享的，L3是共享的。</p>
<h2 id="Memory"><a href="#Memory" class="headerlink" title="Memory"></a>Memory</h2><ul>
<li><p>Memory size,max allowed</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"># free</span><br><span class="line">             total       used       free     shared    buffers     cached</span><br><span class="line">Mem:      90743596   23288228   67455368          0     684452    2938648</span><br><span class="line">-/+ buffers/cache:   19665128   71078468</span><br><span class="line">Swap:     26214392          0   26214392</span><br><span class="line"></span><br><span class="line"># more /proc/meminfo </span><br><span class="line">MemTotal:       263860344 kB      # 已经减去了显卡占用的内存</span><br><span class="line">MemFree:        206924460 kB</span><br><span class="line">MemAvailable:   248609828 kB</span><br><span class="line">Buffers:          145604 kB</span><br><span class="line">Cached:         51754488 kB</span><br><span class="line">SwapCached:            0 kB</span><br><span class="line">Active:         45933660 kB</span><br><span class="line">Inactive:        7115240 kB</span><br><span class="line">Active(anon):    8130140 kB</span><br><span class="line">Inactive(anon):  4851368 kB</span><br><span class="line">Active(file):   37803520 kB</span><br><span class="line">Inactive(file):  2263872 kB</span><br><span class="line"></span><br><span class="line"># dmidecode -t 17</span><br><span class="line"># dmidecode 2.11</span><br><span class="line">SMBIOS 2.7 present.</span><br><span class="line"></span><br><span class="line">Handle 0x1100, DMI type 17, 34 bytes</span><br><span class="line">Memory Device</span><br><span class="line">        Array Handle: 0x1000</span><br><span class="line">        Error Information Handle: Not Provided</span><br><span class="line">        Total Width: 72 bits</span><br><span class="line">        Data Width: 64 bits</span><br><span class="line">        Size: 8192 MB                 # 内存大小</span><br><span class="line">        Form Factor: DIMM</span><br><span class="line">        Set: 1</span><br><span class="line">        Locator: DIMM_A1 </span><br><span class="line">        Bank Locator: Not Specified</span><br><span class="line">        Type: DDR3                   # DDR3代</span><br><span class="line">        Type Detail: Synchronous Registered (Buffered)</span><br><span class="line">        Speed: 1333 MHz              # 时钟频率</span><br><span class="line">        Manufacturer: 00CE04B300CE</span><br><span class="line">        Serial Number: 4400B1EA</span><br><span class="line">        Asset Tag: 01104611</span><br><span class="line">        Part Number: M393B1K70CH0-YH9  </span><br><span class="line">        Rank: 2</span><br><span class="line">        Configured Clock Speed: 1333 MHz</span><br><span class="line">......</span><br></pre></td></tr></table></figure>
</li>
<li><p>bandwidth and letency</p>
<ul>
<li>DDR2(2 bits),DDR3(4 bits),DDR4(8 bits)</li>
<li>Bandwidth = Clock rate(时钟频率) <em> 4(DDR3) </em> 2(Double) * bits / 8（Double就是D，脉冲升频降会各取一次数据。带宽无需手动计算，内存卡会标识PC xxxxMB）</li>
<li>Letency(wait time before read again,in ns)读取内存时，要等待的时间。动态内存需要电门不停的刷，所以读取数据需要时间。</li>
<li>ECC(slower,safer)<ul>
<li>Corrects single-bit errors</li>
<li>Detects multiple-bit errors</li>
</ul>
</li>
</ul>
</li>
<li>Method of memory accessing<ul>
<li>UMA,NUMA</li>
</ul>
</li>
</ul>
<h2 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h2><ul>
<li>Type of storage used<ul>
<li>mechanical magnetic platters（机械磁盘）</li>
<li>SSD devices（固态磁盘）</li>
</ul>
</li>
<li>Hardware RAID Level<ul>
<li>stripe depth（条带深度）</li>
<li>stripe width（条带宽度）</li>
<li>stripe size = stripe depth × stripe width</li>
</ul>
</li>
<li>Direct-attached Storage（直连存储）<ul>
<li>SATA,SAS,IDE</li>
</ul>
</li>
<li>SCSI,Fibre Channel,ISCSI<ul>
<li>Bandwidth,latency,multipath</li>
</ul>
</li>
</ul>
<h3 id="磁盘架构"><a href="#磁盘架构" class="headerlink" title="磁盘架构"></a>磁盘架构</h3><ul>
<li>CLV</li>
<li>CAV</li>
<li><p>Zoned CAV</p>
<p>  Tips</p>
<p>  磁盘外圈速度比里圈速度快得多，所以操作系统默认将swap分区分配在里圈。</p>
</li>
</ul>
<h3 id="磁盘速度"><a href="#磁盘速度" class="headerlink" title="磁盘速度"></a>磁盘速度</h3><p>一般都指Burst speed（顺序读写）速率。</p>
<p><img src="/images/linux/system-tuning/storage-bandwidth.png" alt="storage-bandwidth"></p>
<h3 id="磁盘类型"><a href="#磁盘类型" class="headerlink" title="磁盘类型"></a>磁盘类型</h3><ul>
<li><p>IDE（并口）</p>
<p>  IDE（Integrated Drive Electronics电子集成驱动器）的缩写，它的本意是指把控制器与盘体集成在一起的硬盘驱动器，是一种硬盘的传输接口，它有另一个名称叫做ATA（Advanced Technology Attachment），这两个名词都有厂商在用，指的是相同的东西。</p>
<p>  IDE的规格后来有所进步，而推出了EIDE（Enhanced IDE）的规格名称，而这个规格同时又被称为Fast ATA。所不同的是Fast ATA是专指硬盘接口，而EIDE还制定了连接光盘等非硬盘产品的标准。而这个连接非硬盘类的IDE标准，又称为ATAPI接口。而之后再推出更快的接口，名称都只剩下ATA的字样，像是Ultra ATA、ATA/66、ATA/100等。</p>
</li>
<li><p>SATA（串口）</p>
<p>  SATA（Serial ATA）口的硬盘又叫串口硬盘。2001年，由Intel、APT、Dell、IBM、希捷、迈拓这几大厂商组成的Serial ATA委员会正式确立了Serial ATA 1.0规范。</p>
</li>
<li><p>SCSI（小型计算机系统专用接口）</p>
<p>  SCSI的英文全称为“Small Computer System Interface”（小型计算机系统接口），是同IDE（ATA）完全不同的接口，IDE接口是普通PC的标准接口，而SCSI并不是专门为硬盘设计的接口，是一种广泛应用于小型机上的高速数据传输技术。SCSI接口具有应用范围广、多任务、带宽大、CPU占用率低，以及热插拔等优点，但较高的价格使得它很难如IDE硬盘般普及，因此SCSI硬盘主要应用于中、高端服务器和高档工作站中。</p>
</li>
<li><p>SAS（就是串口的SCSI接口）</p>
<p>  SAS(Serial Attached SCSI)即串行连接SCSI，是新一代的SCSI技术。和现在流行的Serial ATA(SATA)硬盘相同，都是采用串行技术以获得更高的传输速度，并通过缩短连结线改善内部空间等。SAS是并行SCSI接口之后开发出的全新接口。此接口的设计是为了改善存储系统的效能、可用性和扩充性，并且提供与SATA硬盘的兼容性。</p>
</li>
<li><p>FC（光纤通道）</p>
<p>  光纤通道的英文拼写是Fiber Channel，和SCIS接口一样光纤通道最初也不是为硬盘设计开发的接口技术，是专门为网络系统设计的，但随着存储系统对速度的需求，才逐渐应用到硬盘系统中。光纤通道硬盘是为提高多硬盘存储系统的速度和灵活性才开发的，它的出现大大提高了多硬盘系统的通信速度。光纤通道的主要特性有：热插拔性、高速带宽、远程连接、连接设备数量大等。</p>
</li>
<li><p>SSD（固态硬盘）</p>
<p>  固态硬盘（Solid State Disk或Solid State Drive），也称作电子硬盘或者固态电子盘，是由控制单元和固态存储单元（DRAM或FLASH芯片）组成的硬盘。固态硬盘的接口规范和定义、功能及使用方法上与普通硬盘的相同，在产品外形和尺寸上也与普通硬盘一致。由于固态硬盘没有普通硬盘的旋转介质，因而抗震性极佳。其芯片的工作温度范围很宽（-40~85℃）。</p>
<p>  由于固态硬盘技术与传统硬盘技术不同，所以产生了不少新兴的存储器厂商。厂商只需购买NAND存储器，再配合适当的控制芯片，就可以制造固态硬盘了。新一代的固态硬盘普遍采用SATA-2接口。</p>
</li>
</ul>
<h3 id="查看磁盘"><a href="#查看磁盘" class="headerlink" title="查看磁盘"></a>查看磁盘</h3><pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@kvm-2 ~]# fdisk -l /dev/sda </span><br><span class="line"></span><br><span class="line">Disk /dev/sda: 4000.8 GB, 4000787030016 bytes</span><br><span class="line">255 heads, 63 sectors/track, 486401 cylinders</span><br><span class="line">Units = cylinders of 16065 * 512 = 8225280 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br><span class="line">Disk identifier: 0x00000000</span><br><span class="line"></span><br><span class="line">[root@kvm-2 ~]# lspci</span><br><span class="line">......</span><br><span class="line">00:1f.0 ISA bridge: Intel Corporation C600/X79 series chipset LPC Controller (rev 05)</span><br><span class="line">00:1f.2 SATA controller: Intel Corporation C600/X79 series chipset 6-Port SATA AHCI Controller (rev 05)</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line"># hdparm -I /dev/sda</span><br><span class="line"></span><br><span class="line">/dev/sda:</span><br><span class="line"></span><br><span class="line">ATA device, with non-removable media</span><br><span class="line">	Model Number:       ST1000NM0033-9ZM173       # 希捷1T</span><br><span class="line">	Serial Number:      Z1W3VQXC</span><br><span class="line">	Firmware Revision:  GA0A</span><br><span class="line">	Transport:          Serial, SATA Rev 3.0</span><br><span class="line">Standards:</span><br><span class="line">	Supported: 9 8 7 6 5 </span><br><span class="line">	Likely used: 9</span><br><span class="line">......</span><br><span class="line">[root@localhost ~]# dmesg |grep -C5 SATA</span><br></pre></td></tr></table></figure>
</code></pre><h2 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h2><ul>
<li>Electronic disk, no moving mechanical components</li>
<li>No startup time</li>
<li>Very low latency</li>
<li>Potentially longer life time（尽可能延长寿命）<ul>
<li>Wear leveling（磨损平衡）:闪存寿命是以P/E（完全擦写）次数来计算的，而WL就是确保闪存内每个块被写入的次数相等的一种机制。</li>
</ul>
</li>
<li>Parameter<ul>
<li>TBW<ul>
<li>在 SSD 使用寿命结束之前指定工作量可以写入 SSD 的总数据量。</li>
</ul>
</li>
<li>DWPD<ul>
<li>在保固期内（或不同的数年时段内）每天可以写入硬盘用户存储容量的次数。</li>
<li>DWPD = (固态硬盘的 TBW (TB) <em> P/E) / (365 天 </em> 年数 * 固态硬盘用户容量 (GB))</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="SSD-Types"><a href="#SSD-Types" class="headerlink" title="SSD Types"></a>SSD Types</h3><p>固态硬盘就是靠NAND Flash闪存芯片存储数据的，这点类似于我们常见的U盘。NAND Flash根据存储原理分为三种，SLC、MLC、TLC。</p>
<ul>
<li><p>SLC</p>
<p>  Single-Level Cell ，即1bit/cell（1个存储器储存单元可存放1 bit的数据），速度快寿命长，价格超贵（约MLC 3倍以上的价格），约10万次擦写寿命</p>
</li>
<li><p>MLC</p>
<p>  Multi-Level Cell，即2bit/cell，速度一般寿命一般，价格一般，约1000–3000次擦写寿命</p>
</li>
<li><p>TLC</p>
<p>  Trinary-Level Cell，即3bit/cell，也有Flash厂家叫8LC，速度慢寿命短，价格便宜，约1000次擦写寿命。<br>  单位容量的存储器，可以存储更多的数据，所以TLC每百万字节生产成本是最低的。</p>
</li>
</ul>
<p>实战：计算256G的TLC固态硬盘的使用寿命。</p>
<p>假设该硬盘每天读取100G数据，256G*1000/356/100G=7.19（年） </p>
<h3 id="SSD-Garbage-Collection"><a href="#SSD-Garbage-Collection" class="headerlink" title="SSD Garbage Collection"></a>SSD Garbage Collection</h3><p><img src="/images/linux/system-tuning/ssd-garbage-collection.png" alt="ssd-garbage-collection"></p>
<ol>
<li>上图SSD中有两个空的（erased）的Block X和Block Y, 每个Block有12个Pages;</li>
<li>首先在Block X中写入4个Pages(A, B, C, D);</li>
<li>接着再向Block X中写入新的4个pages(E, F, G, H), 同时写入PageA-D的更新数据（A’, B’, C’, D’), 这时PageA-D变为失效数据（invalid）;</li>
<li>为了向PageA-D的位置写入数据，需要将E, F, G, H, A’, B’, C’, D’ 8个pages先搬到Block Y中, 之后再把Block X erase掉，这个过程就为GC。</li>
</ol>
<p>Nand flash 以Page为单位读写数据，而以Block为单位擦除数据。</p>
<p>不过，由于GC的过程增加了数据的读写过程，势必会对SSD的performance的产生一定的影响，所以GC发生的条件与触发点很关键。</p>
<p>GC触发条件大致有3点：</p>
<ol>
<li>Spare Block（）备用块太少</li>
<li>Wear leveling</li>
<li>处理ECC错误Block</li>
</ol>
<h3 id="SSD-Trim"><a href="#SSD-Trim" class="headerlink" title="SSD Trim"></a>SSD Trim</h3><p>操作系统删除数据时，Windows只会做个标记，说明这里已经没东西了，等到真正要写入数据时再来真正删除，并且做标记这个动作会保留在磁盘缓存中，等到磁盘空闲时再执行；Linux只会把inode table回收。</p>
<p>所以对于非空的page，SSD在写入前必须先进行一次Erase，则写入过程为read-erase-modify-write:将整个block的内容读取到cache中，整个block从SSD中Erase,要覆写的page写入到cache的block中，将cache中更新的block写入闪存介质，这个现象称之为写入放大(write amplification)。</p>
<p>为了解决这个问题，SSD开始支持TRIM，TRIM功能使操作系统得以通知SSD哪些页不再包含有效的数据。</p>
<p>当Windows识别到SSD并确认SSD支持Trim后，在删除数据时，会不向硬盘通知删除指令，只使用Volume Bitmap来记住这里的数据已经删除。Volume Bitmap只是一个磁盘快照，其建立速度比直接读写硬盘去标记删除区域要快得多。这一步就已经省下一大笔时间了。然后再是写入数据的时候，由于NAND闪存保存数据是纯粹的数字形式，因此可以直接根据Volume Bitmap的情况，向快照中已删除的区块写入新的数据，而不用花时间去擦除原本的数据。</p>
<ul>
<li><p>Linux启用Trim</p>
<ol>
<li><p>确认 SSD 、操作系统、文件系统都支持 TRIM</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># discard_granularity 非 0 表示支持</span><br><span class="line"># cat /sys/block/sda/queue/discard_granularity</span><br><span class="line">0</span><br><span class="line"># cat /sys/block/nvme0n1/queue/discard_granularity</span><br><span class="line">512</span><br><span class="line"></span><br><span class="line"># DISC-GRAN (discard granularity) 和 DISC-MAX (discard max bytes) 列非 0 表示该 SSD 支持 TRIM 功能。</span><br><span class="line"># lsblk --discard</span><br><span class="line">NAME    DISC-ALN DISC-GRAN DISC-MAX DISC-ZERO</span><br><span class="line">sda            0        0B       0B         0</span><br><span class="line">├─sda1         0        0B       0B         0</span><br><span class="line">├─sda2         0        0B       0B         0</span><br><span class="line">└─sda3         0        0B       0B         0</span><br><span class="line">sr0            0        0B       0B         0</span><br><span class="line">nvme0n1      512      512B       2T         1</span><br><span class="line">nvme1n1      512      512B       2T         1</span><br></pre></td></tr></table></figure>
</li>
<li><p>开启</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">对于 ext4 文件系统，可以在/etc/fstab里添加 discard 参数来启用 TRIM，添加前请确认你的 SSD 支持 TRIM。</span><br><span class="line"># vim /etc/fstab</span><br><span class="line">/dev/sdb1  /data1       ext4   defaults,noatime,discard   0  0</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>Windows启用Trim</p>
</li>
</ul>
<p>注意：如果SSD组RAID0后，将失去Trim功能。</p>
<h2 id="RAID"><a href="#RAID" class="headerlink" title="RAID"></a>RAID</h2><ul>
<li><p>striping（条带化）</p>
<p>  条带（strip）是把连续的数据分割成相同大小的数据块，把每段数据分别写入到阵列中的不同磁盘上的方法。简单的说，条带是一种将多个磁盘驱动器合并为一个卷的方法。 许多情况下，这是通过硬件控制器来完成的。</p>
</li>
<li><p>why striping?</p>
<p>  首先介绍什么是磁盘冲突。当多个进程同时访问一个磁盘时，磁盘的访问次数（每秒的 I/O 操作，IOPS）和数据传输率（每秒传输的数据量，TPS）达到极限后，后面的进程就需要等待，这时就是所谓的磁盘冲突。</p>
<p>  避免磁盘冲突是优化 I/O 性能的一个重要目标，而 I/O 性能的优化与其他资源（如CPU和内存）的优化有着很大的区别,I/O 优化最有效的手段是将 I/O 最大限度的进行平衡。</p>
<p>  条带化技术就是一种自动的将 I/O 的负载均衡到多个物理磁盘上的技术，条带化技术就是将一块连续的数据分成很多小部分并把他们分别存储到不同磁盘上去。这就能使多个进程同时访问数据的多个不同部分而不会造成磁盘冲突，而且在需要对这种数据进行顺序访问的时候可以获得最大程度上的 I/O 并行能力，从而获得非常好的性能。</p>
</li>
<li><p>stripe depth(stripe unit)</p>
<p>  条带深度：指的是条带的大小。这个参数指的是写在每块磁盘上的条带数据块的大小。RAID的数据块大小一般在2KB到512KB之间(或者更大)，其数值是 2 的次方，即2KB,4KB,8KB,16KB这样。</p>
<p>  条带大小对性能的影响比条带宽度难以量化的多：</p>
<ul>
<li>减小条带大小: 由于条带大小减小了，则文件被分成了更多个，更小的数据块。这些数据块会被分散到更多的硬盘上存储，因此提高了传输的性能，但是由于要多次寻找不同的数据块，磁盘定位的性能就下降了。</li>
<li><p>增加条带大小: 与减小条带大小相反，会降低传输性能，提高定位性能。</p>
<p>根据上边的论述，我们会发现根据不同的应用类型，不同的性能需求，不同驱动器的不同特点(如SSD硬盘)，不存在一个普遍适用的”最佳条带大小”。所以这也是存储厂家，文件系统编写者允许我们自己定义条带大小的原因。</p>
</li>
</ul>
</li>
<li><p>stripe width</p>
<p>  条带宽度：是指同时可以并发读或写的条带数量。这个数量等于RAID中的物理硬盘数量。例如一个经过条带化的，具有4块物理硬盘的阵列的条带宽度就是 4。增加条带宽度，可以增加阵列的读写性能。道理很明显，增加更多的硬盘，也就增加了可以同时并发读或写的条带数量。</p>
</li>
<li><p>stripe size</p>
<p>  有时也称block size块大小、chunk size簇大小、stripe length条带长度、granularity粒度，是单块磁盘上的每次I/O的最小单位。</p>
</li>
</ul>
<p>实战：<a href="http://www.mysqlab.net/blog/2011/12/raid10-stripe-size-for-mysql-innodb/" target="_blank" rel="noopener">Raid1+0 stripe size for MySQL InnoDB</a></p>
<h2 id="Networking-Profile"><a href="#Networking-Profile" class="headerlink" title="Networking Profile"></a>Networking Profile</h2><h3 id="查看网卡"><a href="#查看网卡" class="headerlink" title="查看网卡"></a>查看网卡</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@kvm-2 ~]# lspci | grep Ethernet</span><br><span class="line">01:00.0 Ethernet controller: Broadcom Corporation NetXtreme BCM5720 Gigabit Ethernet PCIe</span><br><span class="line">01:00.1 Ethernet controller: Broadcom Corporation NetXtreme BCM5720 Gigabit Ethernet PCIe</span><br><span class="line">02:00.0 Ethernet controller: Broadcom Corporation NetXtreme BCM5720 Gigabit Ethernet PCIe</span><br><span class="line">02:00.1 Ethernet controller: Broadcom Corporation NetXtreme BCM5720 Gigabit Ethernet PCIe</span><br><span class="line"># 示：四块博通千兆网卡</span><br><span class="line"></span><br><span class="line">[root@kvm-2 ~]# ethtool bond0</span><br><span class="line">Settings for bond0:</span><br><span class="line">        Supported ports: [ ]</span><br><span class="line">        Supported link modes:   Not reported</span><br><span class="line">        Supported pause frame use: No</span><br><span class="line">        Supports auto-negotiation: No</span><br><span class="line">        Advertised link modes:  Not reported</span><br><span class="line">        Advertised pause frame use: No</span><br><span class="line">        Advertised auto-negotiation: No</span><br><span class="line">        Speed: 1000Mb/s</span><br><span class="line">        Duplex: Full         # 当前工作在全双工模式</span><br><span class="line">        Port: Other</span><br><span class="line">        PHYAD: 0</span><br><span class="line">        Transceiver: internal</span><br><span class="line">        Auto-negotiation: off</span><br><span class="line">        Link detected: yes</span><br><span class="line"></span><br><span class="line">[root@kvm-2 ~]# ip a s bond0</span><br><span class="line">6: bond0: &lt;BROADCAST,MULTICAST,MASTER,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP </span><br><span class="line">    link/ether f8:bc:12:48:91:64 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet6 fe80::fabc:12ff:fe48:9164/64 scope link </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>
<h2 id="主板"><a href="#主板" class="headerlink" title="主板"></a>主板</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@kvm-2 ~]# dmidecode -t baseboard</span><br></pre></td></tr></table></figure>
<h2 id="PCI设备"><a href="#PCI设备" class="headerlink" title="PCI设备"></a>PCI设备</h2><p>PCI是CPU和外围设备通信的高速传输总线。</p>
<p><img src="/images/linux/system-tuning/pci-bandwidth.png" alt="pci-bandwidth"></p>
<h3 id="查看pci设备"><a href="#查看pci设备" class="headerlink" title="查看pci设备"></a>查看pci设备</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">[root@kvm-2 ~]# lspci            # pciutils软件包</span><br><span class="line">7f:08.0 System peripheral: Intel Corporation Xeon E5 v2/Core i7 QPI Link 0 (rev 04)</span><br><span class="line">......</span><br><span class="line">7f:0c.1 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Unicast Registers (rev 04)</span><br><span class="line">......</span><br><span class="line">7f:16.2 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Broadcast Registers (rev 04)</span><br><span class="line"></span><br><span class="line">[root@kvm-2 ~]# lspci -vt</span><br><span class="line">-+-[0000:7f]-+-08.0  Intel Corporation Xeon E5 v2/Core i7 QPI Link 0</span><br><span class="line"> |           +-09.0  Intel Corporation Xeon E5 v2/Core i7 QPI Link 1</span><br><span class="line">......</span><br><span class="line"> |           +-0f.0  Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 0 Target Address/Thermal Registers</span><br><span class="line">......</span><br><span class="line"> |           +-10.7  Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 1 Channel 0-3 ERROR Registers 3</span><br><span class="line"> |           +-13.0  Intel Corporation Xeon E5 v2/Core i7 R2PCIe</span><br><span class="line"> |           +-13.1  Intel Corporation Xeon E5 v2/Core i7 R2PCIe</span><br><span class="line"> |           +-13.4  Intel Corporation Xeon E5 v2/Core i7 QPI Ring Registers</span><br><span class="line"> |           +-13.5  Intel Corporation Xeon E5 v2/Core i7 QPI Ring Performance Ring Monitoring</span><br><span class="line"> |           +-16.0  Intel Corporation Xeon E5 v2/Core i7 System Address Decoder</span><br><span class="line"> |           +-16.1  Intel Corporation Xeon E5 v2/Core i7 Broadcast Registers</span><br><span class="line"> |           \-16.2  Intel Corporation Xeon E5 v2/Core i7 Broadcast Registers</span><br><span class="line"> \-[0000:00]-+-00.0  Intel Corporation Xeon E5 v2/Core i7 DMI2</span><br><span class="line">             +-01.0-[02]--+-00.0  Broadcom Corporation NetXtreme BCM5720 Gigabit Ethernet PCIe</span><br><span class="line">             |            \-00.1  Broadcom Corporation NetXtreme BCM5720 Gigabit Ethernet PCIe</span><br><span class="line">             +-01.1-[01]--+-00.0  Broadcom Corporation NetXtreme BCM5720 Gigabit Ethernet PCIe</span><br><span class="line">             |            \-00.1  Broadcom Corporation NetXtreme BCM5720 Gigabit Ethernet PCIe</span><br><span class="line">             +-02.0-[04]--</span><br><span class="line">             +-02.2-[03]----00.0  LSI Logic / Symbios Logic MegaRAID SAS 2008 [Falcon]</span><br><span class="line">             +-03.0-[05]--</span><br><span class="line">             +-03.2-[06]--</span><br><span class="line">             +-05.0  Intel Corporation Xeon E5 v2/Core i7 VTd/Memory Map/Misc</span><br><span class="line">             +-05.2  Intel Corporation Xeon E5 v2/Core i7 IIO RAS</span><br><span class="line">             +-11.0-[07]--</span><br><span class="line">             +-16.0  Intel Corporation C600/X79 series chipset MEI Controller #1</span><br><span class="line">             +-16.1  Intel Corporation C600/X79 series chipset MEI Controller #2</span><br><span class="line">             +-1a.0  Intel Corporation C600/X79 series chipset USB2 Enhanced Host Controller #2</span><br><span class="line">             +-1c.0-[08]--</span><br><span class="line">             +-1c.7-[09-0d]----00.0-[0a-0d]--+-00.0-[0b-0c]----00.0-[0c]----00.0  Matrox Electronics Systems Ltd. G200eR2</span><br><span class="line">             |                               \-01.0-[0d]--</span><br><span class="line">             +-1d.0  Intel Corporation C600/X79 series chipset USB2 Enhanced Host Controller #1</span><br><span class="line">             +-1e.0-[0e]--</span><br><span class="line">             +-1f.0  Intel Corporation C600/X79 series chipset LPC Controller</span><br><span class="line">             \-1f.2  Intel Corporation C600/X79 series chipset 6-Port SATA AHCI Controller</span><br><span class="line"></span><br><span class="line">[root@kvm-2 ~]# lspci -xxx -s 7f:13.5        # x越多，列出的信息越详细</span><br><span class="line">7f:13.5 Performance counters: Intel Corporation Xeon E5 v2/Core i7 QPI Ring Performance Ring Monitoring (rev 04)</span><br><span class="line">00: 86 80 36 0e 00 00 00 00 04 00 01 11 10 00 80 00</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">[root@kvm-2 ~]# lspci -vv -s 7f:13.5         # v越多，列出的信息越详细</span><br><span class="line">7f:13.5 Performance counters: Intel Corporation Xeon E5 v2/Core i7 QPI Ring Performance Ring Monitoring (rev 04)</span><br><span class="line">        Subsystem: Dell Device 048c</span><br><span class="line">        Control: I/O- Mem- BusMaster- SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-</span><br><span class="line">        Status: Cap- 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast &gt;TAbort- &lt;TAbort- &lt;MAbort- &gt;SERR- &lt;PERR- INTx-</span><br></pre></td></tr></table></figure>
<h3 id="USB设备"><a href="#USB设备" class="headerlink" title="USB设备"></a>USB设备</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@kvm-2 ~]# lsusb           # usbutils包</span><br><span class="line">Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub</span><br><span class="line">Bus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub</span><br><span class="line">Bus 001 Device 002: ID 8087:0024 Intel Corp. Integrated Rate Matching Hub</span><br><span class="line">Bus 002 Device 002: ID 8087:0024 Intel Corp. Integrated Rate Matching Hub</span><br><span class="line">Bus 001 Device 003: ID 0624:0248 Avocent Corp. Virtual Hub</span><br><span class="line">Bus 001 Device 004: ID 0624:0249 Avocent Corp. Virtual Keyboard/Mouse</span><br><span class="line"></span><br><span class="line">[root@kvm-2 ~]# lsusb -vt</span><br><span class="line">Bus#  2</span><br><span class="line">`-Dev#   1 Vendor 0x1d6b Product 0x0002</span><br><span class="line">  `-Dev#   2 Vendor 0x8087 Product 0x0024</span><br><span class="line">Bus#  1</span><br><span class="line">`-Dev#   1 Vendor 0x1d6b Product 0x0002</span><br><span class="line">  `-Dev#   2 Vendor 0x8087 Product 0x0024</span><br><span class="line">    `-Dev#   3 Vendor 0x0624 Product 0x0248</span><br><span class="line">      `-Dev#   4 Vendor 0x0624 Product 0x0249</span><br></pre></td></tr></table></figure>
<h3 id="查看内核产生的硬件日志"><a href="#查看内核产生的硬件日志" class="headerlink" title="查看内核产生的硬件日志"></a>查看内核产生的硬件日志</h3><ul>
<li>/var/log/dmesg<br> 系统启动，一次性将启动时关于硬件的kernel日志写入该文件。</li>
<li>dmesg命名<br>  实时记录kernel日志，譬如插入USB设备，可使用dmesg命令查看。</li>
</ul>
<h3 id="获取硬件命令汇总"><a href="#获取硬件命令汇总" class="headerlink" title="获取硬件命令汇总"></a>获取硬件命令汇总</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">1. CPU型号</span><br><span class="line">dmidecode -t 4| awk &apos;/Version/ &amp;&amp; /CPU/&apos;|uniq</span><br><span class="line">2. CPU核数</span><br><span class="line">grep -c &apos;processor&apos; /proc/cpuinfo</span><br><span class="line">sysctl kern.smp.cpus|awk &apos;&#123;print $2&#125;&apos;</span><br><span class="line">3.内存大小</span><br><span class="line">free -m|awk &apos;/Mem/&#123;printf(&quot;%d\n&quot;, $2/1024+0.5)&#125;&apos;</span><br><span class="line">sysctl hw.physmem|awk &apos;&#123;printf (&quot;%d\n&quot;, $2/1073741824+0.5)&#125;&apos;</span><br><span class="line">4.硬盘总大小</span><br><span class="line">fdisk -l|awk &apos;/Disk \/dev\//&#123;sum +=$3&#125;END&#123;printf(&quot;%d\n&quot;, sum+0.5)&#125;&apos;</span><br><span class="line">sysctl kern.geom.conftxt|grep -Eo &apos;DISK \w+ \w+&apos;|awk &apos;&#123;sum +=$3&#125;END&#123;printf(&quot;%d\n&quot;, sum/1000000000+0.5)&#125;&apos; </span><br><span class="line">df -Pm | awk &apos;/^\/dev\//&#123;sum +=$2&#125;END&#123;printf(&quot;%d\n&quot;, sum/1024+0.5)&#125;&apos;</span><br><span class="line">5.主板型号</span><br><span class="line">dmidecode -t 1| awk &apos;/Product Name/&apos;</span><br><span class="line">6.取主板Serial</span><br><span class="line">dmidecode -t 1|awk &apos;/Serial/&apos;</span><br><span class="line"></span><br><span class="line">一句话脚本：</span><br><span class="line">dmidecode -t 4| awk &apos;/Version:/&apos;|tail -n1</span><br><span class="line"></span><br><span class="line">if [ `uname` == Linux ];then grep -c &apos;processor&apos; /proc/cpuinfo;else sysctl kern.smp.cpus|awk &apos;&#123;print $2&#125;&apos;;fi</span><br><span class="line"></span><br><span class="line">if [ `uname` == Linux ];then free -m|awk &apos;/Mem/&#123;printf(&quot;%d\n&quot;, $2/1000+0.5)&#125;&apos;;else sysctl hw.physmem|awk &apos;&#123;printf (&quot;%d\n&quot;, $2/1073741824+0.5)&#125;&apos;;fi</span><br><span class="line"></span><br><span class="line">if [ `uname` == Linux ];then fdisk -l|awk &apos;/Disk \/dev\//&#123;sum +=$3&#125;END&#123;printf(&quot;%d\n&quot;, sum+0.5)&#125;&apos;;else sysctl kern.geom.conftxt|egrep -o &apos;DISK \w+ \w+&apos;|awk &apos;&#123;sum +=$3&#125;END&#123;printf(&quot;%d\n&quot;, sum/1000000000+0.5)&#125;&apos;;fi</span><br><span class="line"></span><br><span class="line">dmidecode -t 1| awk &apos;/Product Name/&apos;</span><br></pre></td></tr></table></figure>
<h1 id="Understand-Software"><a href="#Understand-Software" class="headerlink" title="Understand Software"></a>Understand Software</h1><h2 id="Stoarge"><a href="#Stoarge" class="headerlink" title="Stoarge"></a>Stoarge</h2><p>Storage is very slow compare to memory. Linux has two way to compensate the issue.</p>
<ul>
<li>Caching<ul>
<li>Read from memory, write to memory</li>
<li>Read can be cached, write can be deferred</li>
</ul>
</li>
<li>I/O Schedulers<ul>
<li>Kernel attempt to recorder, coalesce I/O requests（临近扇区，合并读请求）</li>
<li>Minimize relocating magnetic head（最小化磁头的动作，重新定位）</li>
</ul>
</li>
</ul>
<h2 id="I-O-Schedulers"><a href="#I-O-Schedulers" class="headerlink" title="I/O Schedulers"></a>I/O Schedulers</h2><p>参考：<a href="http://www.cnblogs.com/cobbliu/p/5389556.html" target="_blank" rel="noopener">http://www.cnblogs.com/cobbliu/p/5389556.html</a></p>
<ul>
<li><p>cfq(Complete Fair Queuing)</p>
<ul>
<li><p>default schduler after kernel 2.6.18</p>
<p>它试图为竞争块设备使用权的所有进程分配一个请求队列和一个时间片，在调度器分配给进程的时间片内，进程可以将其读写请求发送给底层块设备，当进程的时间片消耗完，进程的请求队列将被挂起，等待调度。 </p>
<p>每个进程的时间片和每个进程的队列长度取决于进程的IO优先级，每个进程都会有一个IO优先级，CFQ调度器将会将其作为考虑的因素之一，来确定该进程的请求队列何时可以获取块设备的使用权。IO优先级从高到低可以分为三大类:RT(real time),BE(best try),IDLE(idle),其中RT和BE又可以再划分为8个子优先级。</p>
<p>实际上，我们已经知道CFQ调度器的公平是针对于进程而言的，而只有同步请求(read或syn write)才是针对进程而存在的，他们会放入进程自身的请求队列，而所有同优先级的异步请求，无论来自于哪个进程，都会被放入公共的队列，异步请求的队列总共有8(RT)+8(BE)+1(IDLE)=17个。</p>
</li>
<li><p>IO Priority</p>
<ul>
<li>Class 1(real time): first-access to disk, can starve（饿死） other classes<ul>
<li>0-7: The scheduling class data</li>
</ul>
</li>
<li>Class 2(best-effort): round-robin access, the default<ul>
<li>0-7: The scheduling class data</li>
</ul>
</li>
<li>Class 3(idle): receives disk I/O only if no other requests in queue</li>
</ul>
<p><strong>ionice命令可调节进程的IO优先级</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ionice -n0 -c1 -p pid</span><br><span class="line">ionice -n7 -c2 -p pid</span><br><span class="line">ionice -c3 -p pid        # 我不入地狱，谁入地狱</span><br><span class="line">ionice -c 2 -n 0 bash  # Runs ’bash’ as a best-effort program with highest priority.</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>deadline</p>
<ul>
<li>with predictable service time（可预见的服务时间）</li>
<li><p>for virtualization host</p>
<p>Deadline算法中引入了四个队列，这四个队列可以分为两类，每一类都由读和写两类队列组成，一类队列用来对请求按起始扇区序号进行排序，通过红黑树来组织，称为sort_list；另一类对请求按它们的生成时间进行排序，由链表来组织，称为fifo_list。每当确定了一个传输方向(读或写)，那么将会从相应的sort_list中将一批连续请求dispatch到requst_queue的请求队列里，具体的数目由fifo_batch来确定。只有下面三种情况才会导致一次批量传输的结束：</p>
</li>
</ul>
<ol>
<li>对应的sort_list中已经没有请求了</li>
<li>下一个请求的扇区不满足递增的要求</li>
<li><p>上一个请求已经是批量传输的最后一个请求了</p>
<p>所有的请求在生成时都会被赋上一个期限值(根据jiffies)，并按期限值排序在fifo_list中，读请求的期限时长默认为为500ms，写请求的期限时长默认为5s，可以看出内核对读请求是十分偏心的，其实不仅如此，在deadline调度器中，还定义了一个starved和writes_starved，writes_starved默认为2，可以理解为写请求的饥饿线，内核总是优先处理读请求，starved表明当前处理的读请求批数，只有starved超过了writes_starved后，才会去考虑写请求。因此，假如一个写请求的期限已经超过，该请求也不一定会被立刻响应，因为读请求的batch还没处理完，即使处理完，也必须等到starved超过writes_starved才有机会被响应。为什么内核会偏袒读请求？这是从整体性能上进行考虑的。读请求和应用程序的关系是同步的，因为应用程序要等待读取的内容完毕，才能进行下一步工作，因此读请求会阻塞进程，而写请求则不一样，应用程序发出写请求后，内存的内容何时写入块设备对程序的影响并不大，所以调度器会优先处理读请求。</p>
<p>默认情况下，读请求的超时时间是500ms，写请求的超时时间是5s。</p>
<p><a href="http://www.ibm.com/support/knowledgecenter/api/content/linuxonibm/liaat/liaatbestpractices_pdf.pdf" target="_blank" rel="noopener">这篇文章</a>说在一些多线程应用下，Deadline算法比CFQ算法好。<a href="https://www.percona.com/blog/2009/01/30/linux-schedulers-in-tpcc-like-benchmark/" target="_blank" rel="noopener">这篇文章</a>说在一些数据库应用下，Deadline算法比CFQ算法好。</p>
</li>
</ol>
</li>
<li><p>anticipatory(AS)</p>
<ul>
<li>wait for a while after read request</li>
<li><p>for sequential read workloads（大量顺序读的）</p>
<p>Anticipatory算法从Linux 2.6.33版本后，就被移除了，因为CFQ通过配置也能达到Anticipatory算法的效果。</p>
</li>
</ul>
</li>
<li><p>noop(No Operation)</p>
<ul>
<li>quick to response, low CPU overhead</li>
<li><p>for SSD,virtualization guests（宿主机使用了deadline，则虚拟机使用noop，因为真正写盘操作是主机完成）</p>
<p>Noop调度算法也叫作电梯调度算法，它将IO请求放入到一个FIFO队列中，然后逐个执行这些IO请求，当然对于一些在磁盘上连续的IO请求，Noop算法会适当做一些合并。这个调度算法特别适合那些不希望调度器重新组织IO请求顺序的应用。</p>
<p>这种调度算法在以下场景中优势比较明显：</p>
</li>
</ul>
<ol>
<li><p>在IO调度器下方有更加智能的IO调度设备。如果您的Block Device Drivers是Raid，或者SAN，NAS等存储设备，这些设备会更好地组织IO请求，不用IO调度器去做额外的调度工作；</p>
<ol start="2">
<li><p>上层的应用程序比IO调度器更懂底层设备。或者说上层应用程序到达IO调度器的IO请求已经是它经过精心优化的，那么IO调度器就不需要画蛇添足，只需要按序执行上层传达下来的IO请求即可。</p>
</li>
<li><p>对于一些非旋转磁头氏的存储设备，使用Noop的效果更好。因为对于旋转磁头式的磁盘来说，IO调度器的请求重组要花费一定的CPU时间，但是对于SSD磁盘来说，这些重组IO请求的CPU时间可以节省下来，因为SSD提供了更智能的请求调度算法，不需要内核去画蛇添足。</p>
</li>
</ol>
</li>
</ol>
</li>
</ul>
<h3 id="Queue"><a href="#Queue" class="headerlink" title="Queue"></a>Queue</h3><ul>
<li><p>/sys/block/\&lt;device>/queue/nr_requests</p>
<p>  磁盘请求队列长度（一次性交给磁盘的请求数量）。增大它会牺牲更多内存。 </p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cat /sys/block/sda/queue/nr_requests </span><br><span class="line">128</span><br></pre></td></tr></table></figure>
</li>
<li><p>/sys/block/\&lt;device>/queue/read_ahead_kb</p>
<p>  预先读数据块大小，对于大量的连续读业务，可以增大它。</p>
</li>
</ul>
<h3 id="I-O-Scheduler-Manage"><a href="#I-O-Scheduler-Manage" class="headerlink" title="I/O Scheduler Manage"></a>I/O Scheduler Manage</h3><ul>
<li><p>/sys/block/\&lt;device>/queue/scheduler<br>  切换I/O调度算法。</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cat /sys/block/sda/queue/scheduler </span><br><span class="line">noop anticipatory deadline [cfq]</span><br></pre></td></tr></table></figure>
</li>
<li><p>每种调度算法的可调参数</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 该目录会根据不同Schduler而变化</span><br><span class="line"># cd /sys/block/sda/queue/iosched/</span><br><span class="line">[root@kvm-2 iosched]# ls</span><br><span class="line">back_seek_max      fifo_expire_async  group_idle       low_latency  slice_async     slice_idle</span><br><span class="line">back_seek_penalty  fifo_expire_sync   group_isolation  quantum      slice_async_rq  slice_sync</span><br></pre></td></tr></table></figure>
</li>
<li><p>CFQ</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">/sys/block/&lt;device&gt;/queue/iosched/slice_idle </span><br><span class="line">当一个进程的队列被分配到时间片却没有 IO 请求时，调度器在轮询至下一个队列之前的等待时间，以提升 IO 的局部性，对于 SSD 设备，可以将这个值设为 0。</span><br><span class="line"></span><br><span class="line">/sys/block/&lt;device&gt;/queue/iosched/quantum </span><br><span class="line">一个进程的队列每次被处理 IO 请求的最大数量，默认为 4，RHEL6 为 8，增大这个值可以提升并行处理 IO 的性能，但可能会造成某些 IO 延迟问题。</span><br><span class="line"></span><br><span class="line">/sys/block/&lt;device&gt;/queue/iosched/slice_async_rq </span><br><span class="line">一次处理写请求的最大数</span><br><span class="line"></span><br><span class="line">/sys/block/&lt;device&gt;/queue/iosched/low_latency </span><br><span class="line">如果IO延迟的问题很严重，将这个值设为 1</span><br></pre></td></tr></table></figure>
</li>
<li><p>Deadline</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">/sys/block/&lt;device&gt;/queue/iosched/writes_starved </span><br><span class="line">进行一个写操作之前，允许进行多少次读操作</span><br><span class="line"></span><br><span class="line">/sys/block/&lt;device&gt;/queue/iosched/read_expire </span><br><span class="line">读请求的过期时间，默认为 5ms</span><br><span class="line"></span><br><span class="line">/sys/block/&lt;device&gt;/queue/iosched/write_expire </span><br><span class="line">写请求的过期时间，默认为 500ms</span><br><span class="line"></span><br><span class="line">/sys/block/sda/queue/iosched/front_merges </span><br><span class="line">是否进行前合并</span><br></pre></td></tr></table></figure>
</li>
<li><p>Anticipatory</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">/sys/block/&lt;device&gt;/queue/iosched/antic_expire </span><br><span class="line">预测等待时长，默认为 6ms</span><br><span class="line"></span><br><span class="line">/sys/block/&lt;device&gt;/queue/iosched/&#123;write_expire,read_expire&#125; </span><br><span class="line">读写请求的超时时长</span><br><span class="line"></span><br><span class="line">/sys/block/&lt;device&gt;/queue/iosched/&#123;write_batch_expire,read_batch_expire&#125; </span><br><span class="line">读写的批量处理时长</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Tuning-Theory"><a href="#Tuning-Theory" class="headerlink" title="Tuning Theory"></a>Tuning Theory</h3><ul>
<li>L: Queue length: average number of requests waiting in the system</li>
<li>A: Arrival rate: the rate at which requests enter a system</li>
<li>W: Wait time: average time to satisfy a request<ul>
<li>also known as wall clock,latency,response time, or residence time</li>
</ul>
</li>
</ul>
<p>L = A * W</p>
<h3 id="Queue-Length"><a href="#Queue-Length" class="headerlink" title="Queue Length"></a>Queue Length</h3><ul>
<li>Requests are buffered in memory</li>
<li>L may be read-write tunable or a read-only measurement</li>
</ul>
<h3 id="Wait-Time"><a href="#Wait-Time" class="headerlink" title="Wait Time"></a>Wait Time</h3><ul>
<li>Includes<ul>
<li>Queue time（排队时间）</li>
<li>Service time（服务时间）</li>
</ul>
</li>
<li>Tactics（策略）<ul>
<li>Reduce queue time</li>
<li>Reduce service time</li>
</ul>
</li>
</ul>
<p>L = A <em> W = A </em> (T<sub>q</sub> + T<sub>s</sub>)</p>
<h3 id="Service-Time"><a href="#Service-Time" class="headerlink" title="Service Time"></a>Service Time</h3><ul>
<li>Includes<ul>
<li>Sysem time: time in kernel mode</li>
<li>User time: time in user mode(doing useful work)</li>
</ul>
</li>
<li>Tactics<ul>
<li>Reduce system time(blocks user mode operations)</li>
<li>spend as much time as needed in user mode</li>
</ul>
</li>
</ul>
<p>L = A <em> W = A </em> (Q + S) = A * (T<sub>q</sub> + (T<sub>sys</sub> + T<sub>user</sub>))</p>
<h3 id="Summary-of-Queue-Theory"><a href="#Summary-of-Queue-Theory" class="headerlink" title="Summary of Queue Theory"></a>Summary of Queue Theory</h3><ul>
<li>L: Queue length</li>
<li>A: Arrival rate(requests/second)</li>
<li>W: Wait time(latency, time to satisfy a request)</li>
<li>Q: Queue time</li>
<li>S: Service time(includes system time, user time)</li>
<li><p>C: Complete rate(requests/second)</p>
<ul>
<li>Steady state: A = C</li>
<li>L = A <em> W = A </em> (Q + S) = A * (T<sub>q</sub> + (T<sub>sys</sub> + T<sub>user</sub>))</li>
</ul>
</li>
</ul>
<h3 id="Summary-of-strategies"><a href="#Summary-of-strategies" class="headerlink" title="Summary of strategies"></a>Summary of strategies</h3><ul>
<li>Tune L<ul>
<li>Constrain queue length</li>
<li>Sort the queue to prefer reads</li>
</ul>
</li>
<li>Tune A or C<ul>
<li>Reduce visit count by distributing across multiple resources(SMP,RAID)</li>
<li>Defer resource visits until think time(lazy write)</li>
<li>Improve throughput for resource(more efficient protocol, less overhead)</li>
</ul>
</li>
<li>Tune W<ul>
<li>Use expiration time for requests</li>
<li>use resources with smaller service time(in-memory cache vs disk)</li>
</ul>
</li>
</ul>
<h2 id="Kernel-Module"><a href="#Kernel-Module" class="headerlink" title="Kernel Module"></a>Kernel Module</h2><h3 id="Module-commands"><a href="#Module-commands" class="headerlink" title="Module commands"></a>Module commands</h3><ul>
<li><p>lsmod</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># lsmod </span><br><span class="line">Module                  Size  Used by</span><br><span class="line">tcp_diag                1041  0 </span><br><span class="line">inet_diag               8735  1 tcp_diag    # 表示该模块被tcp_diag依赖，使用次数为1</span><br><span class="line">ip6table_filter         2889  0 </span><br><span class="line">ip6_tables             18732  1 ip6table_filter</span><br><span class="line">ebtable_nat             2009  0 </span><br><span class="line">ebtables               18135  1 ebtable_nat</span><br><span class="line">ipt_MASQUERADE          2466  3 </span><br><span class="line">iptable_nat             6158  1 </span><br><span class="line">nf_nat                 22759  2 ipt_MASQUERADE,iptable_nat</span><br><span class="line">nf_conntrack_ipv4       9506  4 iptable_nat,nf_nat</span><br><span class="line">nf_defrag_ipv4          1483  1 nf_conntrack_ipv4</span><br><span class="line">xt_state                1492  1 </span><br><span class="line">nf_conntrack           79758  5 ipt_MASQUERADE,iptable_nat,nf_nat,nf_conntrack_ipv4,xt_state</span><br><span class="line">ipt_REJECT              2351  2</span><br></pre></td></tr></table></figure>
</li>
<li><p>/lib/modules/\&lt;kernel-release>/kernel/</p>
<p>  内核模块所在目录。</p>
</li>
<li><p>modinfo [ modulename… ]</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># modinfo sx8</span><br><span class="line">filename:       /lib/modules/2.6.32-431.el6.x86_64/kernel/drivers/block/sx8.ko</span><br><span class="line">version:        1.0</span><br><span class="line">description:    Promise SATA SX8 block driver</span><br><span class="line">license:        GPL</span><br><span class="line">author:         Jeff Garzik</span><br><span class="line">srcversion:     4772099AB984FE59198263E</span><br><span class="line">alias:          pci:v0000105Ad00008002sv*sd*bc*sc*i*</span><br><span class="line">alias:          pci:v0000105Ad00008000sv*sd*bc*sc*i*</span><br><span class="line">depends:        </span><br><span class="line">vermagic:       2.6.32-431.el6.x86_64 SMP mod_unload modversions </span><br><span class="line">parm:           max_queue:Maximum number of queued commands. (min==1, max==30, safe==1) (int)</span><br></pre></td></tr></table></figure>
</li>
<li><p>modprobe [ modulename… ]</p>
</li>
<li>rmmod [ modulename… ]</li>
</ul>
<h3 id="Modules-parameters"><a href="#Modules-parameters" class="headerlink" title="Modules parameters"></a>Modules parameters</h3><ul>
<li><p>查看某模块有哪些参数可调整</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># modinfo -p usb_storage</span><br><span class="line">quirks:supplemental list of device IDs and their quirks</span><br><span class="line">delay_use:seconds to delay before using a new device</span><br><span class="line">swi_tru_install:TRU-Install mode (1=Full Logic (def), 2=Force CD-Rom, 3=Force Modem)</span><br><span class="line">option_zero_cd:ZeroCD mode (1=Force Modem (default), 2=Allow CD-Rom</span><br><span class="line"></span><br><span class="line"># modinfo -p sx8</span><br><span class="line">max_queue:Maximum number of queued commands. (min==1, max==30, safe==1)</span><br></pre></td></tr></table></figure>
</li>
<li><p>自定义参数</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># cat /etc/modprobe.d/my.conf </span><br><span class="line">options usb_storage delay_use=3</span><br><span class="line">options st buffer_kbs=128</span><br><span class="line">options sx8 max_queue=10</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用modprobe命令重新加载这些模块，自定义的参数就会生效</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># modprobe usb_storage</span><br><span class="line"># modprobe st</span><br><span class="line"># modprobe sx8</span><br></pre></td></tr></table></figure>
</li>
<li><p>Check runtime module parameters</p>
<ul>
<li>/sys/module/<modulename>/parameters/<pname></pname></modulename></li>
</ul>
</li>
<li><p>Automatically loading modules</p>
<ul>
<li>/etc/sysconfig/modules/my.modules</li>
<li><p>Linux init脚本会执行以上目录下modules结尾的文件。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">modprobe usb_storage|st|sx8</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h2 id="Tuned"><a href="#Tuned" class="headerlink" title="Tuned"></a>Tuned</h2><ul>
<li>Tune a system on the fly as needed</li>
<li>Based on tuning profiles<ul>
<li>max power saving</li>
<li>max disk performance</li>
<li>self made profile allowed（允许自定义tune方案）</li>
<li>profile can even has monitoring program to run（还支持运行监控程序）</li>
</ul>
</li>
<li>SysV service<ul>
<li>tuned</li>
<li>ktune</li>
</ul>
</li>
<li>Can be used with crond to switch between profiles<ul>
<li>0 7 <em> </em> * 1-5 /usr/bin/tuned-adm profile throughput-performance</li>
<li>0 20 <em> </em> * 1-5 /usr/bin/tuned-adm profile server-powersave</li>
</ul>
</li>
</ul>
<h3 id="Use-Tuned"><a href="#Use-Tuned" class="headerlink" title="Use Tuned"></a>Use Tuned</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@cmdb-192-168-21-241 ~]# cat /etc/redhat-release </span><br><span class="line">CentOS Linux release 7.2.1511 (Core) </span><br><span class="line"></span><br><span class="line"># tuned-adm active</span><br><span class="line">Current active profile: virtual-guest</span><br><span class="line"></span><br><span class="line"># tuned-adm list</span><br><span class="line">Available profiles:</span><br><span class="line">- balanced                    - General non-specialized tuned profile</span><br><span class="line">- desktop                     - Optimize for the desktop use-case</span><br><span class="line">- latency-performance         - Optimize for deterministic performance at the cost of increased power consumption</span><br><span class="line">- network-latency             - Optimize for deterministic performance at the cost of increased power consumption, focused on low latency network performance</span><br><span class="line">- network-throughput          - Optimize for streaming network throughput, generally only necessary on older CPUs or 40G+ networks</span><br><span class="line">- powersave                   - Optimize for low power consumption</span><br><span class="line">- throughput-performance      - Broadly applicable tuning that provides excellent performance across a variety of common server workloads</span><br><span class="line">- virtual-guest               - Optimize for running inside a virtual guest</span><br><span class="line">- virtual-host                - Optimize for running KVM guests</span><br><span class="line">Current active profile: virtual-guest</span><br><span class="line"></span><br><span class="line"># tuned-adm profile powersave</span><br></pre></td></tr></table></figure>
<ul>
<li><p>latency-performance</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># grep -vE &apos;^#|^$&apos; /usr/lib/tuned/latency-performance/tuned.conf </span><br><span class="line">[main]</span><br><span class="line">summary=Optimize for deterministic performance at the cost of increased power consumption</span><br><span class="line">[cpu]</span><br><span class="line">force_latency=1</span><br><span class="line">governor=performance</span><br><span class="line">energy_perf_bias=performance</span><br><span class="line">min_perf_pct=100</span><br><span class="line">[sysctl]</span><br><span class="line">kernel.sched_min_granularity_ns=10000000</span><br><span class="line">vm.dirty_ratio=10</span><br><span class="line">vm.dirty_background_ratio=3</span><br><span class="line">vm.swappiness=10</span><br><span class="line">kernel.sched_migration_cost_ns=5000000</span><br></pre></td></tr></table></figure>
</li>
<li><p>throughput-performance</p>
</li>
</ul>
<h3 id="Custom-Tuning-Profiles"><a href="#Custom-Tuning-Profiles" class="headerlink" title="Custom Tuning Profiles"></a>Custom Tuning Profiles</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/lib/tuned/</span><br><span class="line"># mkdir test-performance</span><br><span class="line"></span><br><span class="line"># vim test-performance/tuned.conf</span><br><span class="line">[main]</span><br><span class="line">include=latency-performance</span><br><span class="line">summary=Test profile that uses settings for latency-performance tuning profile</span><br><span class="line"></span><br><span class="line"># tuned-adm list</span><br><span class="line">......</span><br><span class="line">- test-performance            - Test profile that uses settings for latency-performance tuning profile</span><br><span class="line">......</span><br></pre></td></tr></table></figure>
<h2 id="CPU-Tuning"><a href="#CPU-Tuning" class="headerlink" title="CPU Tuning"></a>CPU Tuning</h2><h3 id="Interrupt-and-IRQ"><a href="#Interrupt-and-IRQ" class="headerlink" title="Interrupt and IRQ"></a>Interrupt and IRQ</h3><p>中断请求（IRQ）是用于服务的请求，在硬件层发出。可使用专用硬件线路或者跨硬件总线的信息数据包（消息信号中断，MSI ）发出中断。启用中断后，接收 IRQ 后会提示切换到中断上下文。</p>
<p>CPU绑定后，它仍然要服务于中断。应该将中断绑定至那些非隔离的CPU上，从而避免那些隔离的CPU处理中断程序；</p>
<p>/proc/interrupts文件列出每个I/O 设备中每个 CPU 的中断数，每个 CPU 核处理的中断数，中断类型，以及用逗号分开的注册为接收中断的驱动程序列表。（详情请参考 proc(5) man page：man 5 proc）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# cat /proc/interrupts </span><br><span class="line">           CPU0       CPU1       </span><br><span class="line">  0:      14678          0   IO-APIC-edge      timer</span><br><span class="line">  1:          2          0   IO-APIC-edge      i8042</span><br><span class="line">  4:          2          0   IO-APIC-edge    </span><br><span class="line">  7:          0          0   IO-APIC-edge      parport0</span><br><span class="line">  8:          1          0   IO-APIC-edge      rtc0</span><br><span class="line">  9:          0          0   IO-APIC-fasteoi   acpi</span><br><span class="line"> 12:          4          0   IO-APIC-edge      i8042</span><br><span class="line"> 14:   45394223          0   IO-APIC-edge      ata_piix</span><br><span class="line"> 15:          0          0   IO-APIC-edge      ata_piix</span><br><span class="line"> 16:         56   16232636   IO-APIC-fasteoi   i915, p2p1</span><br><span class="line"> 18:    5333843   11365439   IO-APIC-fasteoi   uhci_hcd:usb4</span><br><span class="line"> 20:    2277759          0   IO-APIC-fasteoi   ata_piix</span><br><span class="line"> 21:          3          0   IO-APIC-fasteoi   ehci_hcd:usb1, uhci_hcd:usb2</span><br><span class="line"> 22:          0          0   IO-APIC-fasteoi   uhci_hcd:usb3</span><br><span class="line"> 23:       3813       6412   IO-APIC-fasteoi   uhci_hcd:usb5, Intel ICH7</span><br><span class="line">......</span><br><span class="line"># APIC表示高级可编程中断控制器（Advanced Programmable Interrupt Controlle）</span><br><span class="line"># APIC是SMP体系的核心，通过APIC可以将中断分发到不同的CPU 来处理。</span><br><span class="line"># i915：Intel i915 集成显卡驱动</span><br></pre></td></tr></table></figure>
<h3 id="Soft-Interrupt-and-Context-Switch"><a href="#Soft-Interrupt-and-Context-Switch" class="headerlink" title="Soft Interrupt and Context Switch"></a>Soft Interrupt and Context Switch</h3><p>上下文切换（也称做进程切换或任务切换）是指 CPU 从一个进程或线程切换到另一个进程或线程。</p>
<p>CPU通过时间片分配算法来循环执行任务，当前任务执行一个时间片后会切换到下一个任务。但是，在切换前会保存上一个任务的状态，以便下次切换回这个任务时，可以再次加载这个任务的状态，从任务保存到再加载的过程就是一次上下文切换。</p>
<h3 id="查看中断"><a href="#查看中断" class="headerlink" title="查看中断"></a>查看中断</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># mpstat</span><br><span class="line"># dstat -c</span><br></pre></td></tr></table></figure>
<h3 id="将IRQ绑定CPU"><a href="#将IRQ绑定CPU" class="headerlink" title="将IRQ绑定CPU"></a>将IRQ绑定CPU</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># echo CPU_MASK &gt; /proc/irq/&lt;irq number&gt;/smp_affinity</span><br><span class="line"></span><br><span class="line"># 案例：将CPU中断绑定到CPU #0,#1上。</span><br><span class="line"># echo 3 &gt; /proc/irq/&lt;irq number&gt;/smp_affinity</span><br></pre></td></tr></table></figure>
<p>将IRQ绑定到某个CPU，那么最好在系统启动时，将那个CPU隔离起来，不被scheduler通常的调度。<br>可以通过在Linux kernel中加入启动参数：isolcpus=cpu-list将CPU隔离起来。</p>
<h3 id="IRQ-Irqbalance"><a href="#IRQ-Irqbalance" class="headerlink" title="IRQ Irqbalance"></a>IRQ Irqbalance</h3><p>irqbalance用于优化中断分配，它会自动收集系统数据以分析使用模式，并依据系统负载状况将工作状态置于 Performance mode 或 Power-save mode。处于Performance mode 时，irqbalance 会将中断尽可能均匀地分发给各个 CPU core，以充分利用 CPU 多核，提升性能。</p>
<p>处于Power-save mode 时，irqbalance 会将中断集中分配给第一个 CPU，以保证其它空闲 CPU 的睡眠时间，降低能耗</p>
<p>但是在实时系统中会导致中断自动漂移，对性能造成不稳定因素，在高性能的场合建议关闭。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># /etc/init.d/irqbalance stop</span><br></pre></td></tr></table></figure>
<h3 id="查看上下文切换"><a href="#查看上下文切换" class="headerlink" title="查看上下文切换"></a>查看上下文切换</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># sar -w           # 查看上下文切换的平均次数，以及进程创建的平均值</span><br><span class="line"># vmstat 1 3       # 每秒上下文切换次数</span><br></pre></td></tr></table></figure>
<h3 id="如何减少上下文切换"><a href="#如何减少上下文切换" class="headerlink" title="如何减少上下文切换"></a>如何减少上下文切换</h3><pre><code>既然上下文切换会导致额外的开销，因此减少上下文切换次数便可以提高多线程程序的运行效率。减少上下文切换的方法有无锁并发编程、CAS算法、使用最少线程和使用协程。

- 无锁并发编程。多线程竞争时，会引起上下文切换，所以多线程处理数据时，可以用一些办法来避免使用锁，如将数据的ID按照Hash取模分段，不同的线程处理不同段的数据
- CAS算法。Java的Atomic包使用CAS算法来更新数据，而不需要加锁
- 使用最少线程。避免创建不需要的线程，比如任务很少，但是创建了很多线程来处理，这样会造成大量线程都处于等待状态
- 协程。在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换
</code></pre><h2 id="CPU-Iffinity（姻亲关系）"><a href="#CPU-Iffinity（姻亲关系）" class="headerlink" title="CPU Iffinity（姻亲关系）"></a>CPU Iffinity（姻亲关系）</h2><p>当软中断和上下文切换过大时。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">taskset</span><br><span class="line">mask       二进程  #CPU</span><br><span class="line">0x0000 0001  0001：node 0</span><br><span class="line">0x0000 0003  0011：node 0和1</span><br><span class="line">0x0000 0005  0101：node 0和2</span><br><span class="line">0x0000 0007  0111：node 0-2</span><br><span class="line"></span><br><span class="line"># taskset -p mask pid</span><br><span class="line">101, 3# CPU</span><br><span class="line"># taskset -p 0x00000005 101</span><br><span class="line"></span><br><span class="line">绑定进程101，CPU 0-2#、7#</span><br><span class="line">taskset -p -c 0-2,7 101</span><br><span class="line"></span><br><span class="line"># 指定CPU启动进程</span><br><span class="line">taskset mask -- program</span><br><span class="line">taskset -c 0,5,7-9 – myprogram</span><br></pre></td></tr></table></figure>
<h3 id="CPU-子系统"><a href="#CPU-子系统" class="headerlink" title="CPU 子系统"></a>CPU 子系统</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># mkdir /cpusets</span><br><span class="line"># vim /etc/fstab</span><br><span class="line">cpuset                  /cpusets                cpuset  defaults        0 0</span><br><span class="line"># mount -a</span><br><span class="line"># ls /cpusets/</span><br><span class="line">cgroup.clone_children  cgroup.sane_behavior  cpuset.mem_exclusive   cpuset.memory_pressure          cpuset.memory_spread_slab  cpuset.sched_relax_domain_level  tasks</span><br><span class="line">cgroup.event_control   cpuset.cpu_exclusive  cpuset.mem_hardwall    cpuset.memory_pressure_enabled  cpuset.mems                notify_on_release</span><br><span class="line">cgroup.procs           cpuset.cpus           cpuset.memory_migrate  cpuset.memory_spread_page       cpuset.sched_load_balance  release_agent</span><br><span class="line">#</span><br><span class="line"># cat /cpusets/cpuset.cpus </span><br><span class="line">0-3</span><br><span class="line"></span><br><span class="line"># 创建子域</span><br><span class="line"># mkdir /cpusets/domain1</span><br><span class="line"># ls /cpusets/domain1/</span><br><span class="line">......</span><br><span class="line"># echo 0-1 &gt;/cpusets/domain1/cpuset.cpus   #将CPU #0,#1绑定进来</span><br><span class="line"># echo 0 &gt;/cpusets/domain1/cpuset.mems     #将内存绑定进来</span><br><span class="line"># echo #pid /cpusets/domain1/tasks         #将某个进程绑定进来，该进程只能在CPU 0-1上运行</span><br><span class="line"></span><br><span class="line"># ps -e -o psr,pid,cmd</span><br></pre></td></tr></table></figure>
<h2 id="Memory-Tuning"><a href="#Memory-Tuning" class="headerlink" title="Memory Tuning"></a>Memory Tuning</h2><h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><ul>
<li>MMU：memory manager unit内存管理单元</li>
<li>TLB：缓存MMU转换的结果，使用大内存页提高性能</li>
</ul>
<h3 id="swap"><a href="#swap" class="headerlink" title="swap"></a>swap</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">vm.swappiness=&#123;0..100&#125;：使用交换分区的倾向性, 默认60</span><br><span class="line">	overcommit_memory=2： 过量使用（0：启发式过量；1：总是过量；2：允许下述超出百分比）</span><br><span class="line">	overcommit_ratio=50：</span><br><span class="line">		可用内存：swap+RAM*ratio</span><br><span class="line">			swap: 2G</span><br><span class="line">			RAM: 8G</span><br><span class="line">		        可用内存：memory=2G+8G*50%=6G</span><br><span class="line"></span><br><span class="line">	充分使用物理内存：</span><br><span class="line">		1、swap跟RAM一样大；</span><br><span class="line">		2、overcommit_memory=2, overcommit_ratio=100：swappiness=0；</span><br><span class="line">			memory: swap+ram</span><br><span class="line">	参考设置：</span><br><span class="line">		1、Batch compute（批处理计算）：&lt;= 4 * RAM</span><br><span class="line">		2、Database server：&lt;= 1G</span><br><span class="line">		3、Application server：&gt;= 0.5 * RAM</span><br><span class="line"></span><br><span class="line">默认值：</span><br><span class="line">[root@nginx1 ~]# cat /proc/sys/vm/overcommit_memory </span><br><span class="line">0</span><br><span class="line">[root@nginx1 ~]# cat /proc/sys/vm/swappiness </span><br><span class="line">60</span><br><span class="line">[root@nginx1 ~]# cat /proc/sys/vm/overcommit_ratio </span><br><span class="line">50</span><br></pre></td></tr></table></figure>
<h3 id="swap-tuning"><a href="#swap-tuning" class="headerlink" title="swap tuning"></a>swap tuning</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 在每个磁盘上建立swap分区，并给与相同优先级</span><br><span class="line">/dev/sda1   swap swap pri=5 0 0</span><br><span class="line">/dev/sdb1   swap swap pri=5 0 0</span><br><span class="line">/dev/sdc1   swap swap pri=1 0 0    # 最慢的磁盘，swap最小的优先级</span><br></pre></td></tr></table></figure>
<h3 id="min-free-kbytes"><a href="#min-free-kbytes" class="headerlink" title="min_free_kbytes"></a>min_free_kbytes</h3><ul>
<li>/proc/sys/vm/min_free_kbytes<br>强制Linux VM最低保留多少空闲内存（Kbytes）</li>
</ul>
<p>内存管理从三个层次管理内存，分别是node, zone ,page。64位的x86物理机内存从高地址到低地址分为: Normal DMA32 DMA</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@kvm-2 ~]# grep Node /proc/zoneinfo </span><br><span class="line">Node 0, zone      DMA</span><br><span class="line">Node 0, zone    DMA32</span><br><span class="line">Node 0, zone   Normal</span><br><span class="line">Node 1, zone   Normal</span><br></pre></td></tr></table></figure>
<p>每个zone都有自己的min low high,如下，但是单位是page。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[root@kvm-2 ~]# grep &quot;Node 0, zone&quot; -A10 /proc/zoneinfo </span><br><span class="line">Node 0, zone      DMA</span><br><span class="line">  pages free     3934</span><br><span class="line">        min      3</span><br><span class="line">        low      3</span><br><span class="line">        high     4</span><br><span class="line">        scanned  0</span><br><span class="line">        spanned  4095</span><br><span class="line">        present  3835</span><br><span class="line">    nr_free_pages 3934</span><br><span class="line">    nr_inactive_anon 0</span><br><span class="line">    nr_active_anon 0</span><br><span class="line">--</span><br><span class="line">Node 0, zone    DMA32</span><br><span class="line">  pages free     571977</span><br><span class="line">        min      749</span><br><span class="line">        low      936</span><br><span class="line">        high     1123</span><br><span class="line">        scanned  0</span><br><span class="line">        spanned  1044480</span><br><span class="line">        present  756520</span><br><span class="line">    nr_free_pages 571977</span><br><span class="line">    nr_inactive_anon 0</span><br><span class="line">    nr_active_anon 0</span><br><span class="line">--</span><br><span class="line">Node 0, zone   Normal</span><br><span class="line">  pages free     4737209</span><br><span class="line">        min      9478</span><br><span class="line">        low      11847</span><br><span class="line">        high     14217</span><br><span class="line">        scanned  0</span><br><span class="line">        spanned  9699328</span><br><span class="line">        present  9566720</span><br><span class="line">    nr_free_pages 4737209</span><br><span class="line">    nr_inactive_anon 166</span><br><span class="line">    nr_active_anon 3973945</span><br></pre></td></tr></table></figure>
<p>上面可知：low = 5/4 <em> min、high = 3/2 </em> min。</p>
<p>min 和 low的区别：</p>
<ol>
<li>min下的内存是保留给内核使用的；当到达min，会触发内存的direct reclaim</li>
<li>low水位比min高一些，当内存可用量小于low的时候，会触发 kswapd reclaim。当kswapd慢慢的将内存 回收到high水位，就开始继续睡眠</li>
</ol>
<ul>
<li>/proc/sys/vm/extra_free_kbytes</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@kvm-2 ~]# sysctl  -a | grep free</span><br><span class="line">vm.min_free_kbytes = 90112</span><br><span class="line">vm.extra_free_kbytes = 0</span><br></pre></td></tr></table></figure>
<p>意义：low = min_free_kbytes*5/4 + extra_free_kbytes</p>
<ul>
<li>总结<br>调整该内存的内核参数的时候！调大的风险远大于调小的风险，会导致频繁的触发内存回收！如果有人想将vm.min_free_kbytes 调大，千万要注意当前的Free，一旦超过Free内存，会立刻触发direct reclaim。</li>
</ul>
<h3 id="Dirty-Page"><a href="#Dirty-Page" class="headerlink" title="Dirty Page"></a>Dirty Page</h3><p>因为硬盘的读写速度远赶不上内存的速度，系统就把读写比较频繁的数据事先放到内存中，以提高读写速度，这就叫高速缓存，linux是以页作为高速缓存的单位，当进程修改了高速缓存里的数据时，该页就被内核标记为脏页，内核将会在合适的时间把脏页的数据写到磁盘中去，以保持高速缓存中的数据和磁盘中的数据是一致的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@kvm-2 ~]# sysctl -a | grep dirty</span><br><span class="line">vm.dirty_background_ratio = 10</span><br><span class="line">vm.dirty_background_bytes = 0</span><br><span class="line">vm.dirty_ratio = 20</span><br><span class="line">vm.dirty_bytes = 0</span><br><span class="line">vm.dirty_writeback_centisecs = 500</span><br><span class="line">vm.dirty_expire_centisecs = 3000</span><br><span class="line"></span><br><span class="line">[root@kvm-2 ~]# sysctl -a | grep pdflush</span><br><span class="line">vm.nr_pdflush_threads = 0</span><br></pre></td></tr></table></figure>
<ul>
<li><p>vm.dirty_background_ratio</p>
<p>  这个参数指定了当文件系统缓存脏页数量达到系统内存百分之多少时（如5%）就会触发pdflush/flush/kdmflush等后台回写进程运行，将一定缓存的脏页<strong>异步</strong>地刷入外存；</p>
</li>
<li><p>vm.dirty_ratio</p>
<p>  这个参数则指定了当文件系统缓存脏页数量达到系统内存百分之多少时（如10%），系统不得不开始处理缓存脏页（因为此时脏页数量已经比较多，为了避免数据丢失需要将一定脏页刷入外存）；在此过程中很多应用进程可能会因为系统转而处理文件IO而阻塞。</p>
</li>
</ul>
<blockquote>
<p>之前一直错误的以为dirty_ratio的触发条件不可能达到，因为每次肯定会先达到vm.dirty_background_ratio的条件，后来才知道自己理解错了。确实是先达到vm.dirty_background_ratio的条件然后触发flush进程进行异步的回写操作，但是这一过程中应用进程仍然可以进行写操作，如果多个应用进程写入的量大于flush进程刷出的量那自然会达到vm.dirty_ratio这个参数所设定的坎，此时操作系统会转入同步地处理脏页的过程，阻塞应用进程。</p>
</blockquote>
<ul>
<li><p>vm.nr_pdflush_threads<br>  pdflush是linux系统后台运行的一个线程，这个进程负责把page cahce中的dirty状态的数据定期的输入磁盘。</p>
<p>  /proc/sys/vm/nr_pdflush_threads查看当前系统运行pdflush数量。当一段时间（一般是1s）没有任何的pdflush处于工作状态，系统会remove一个pdflush线程。pdflush最大和最小的数量是有配置的，但这些配置一般很少修改。</p>
</li>
<li><p>vm.dirty_writeback_centisecs</p>
<p>  默认一般是500（单位是1/100秒）。这个参数表示5s的时间pdflush就会被唤起去刷新脏数据。建议用户使用默认值。</p>
</li>
<li><p>vm.dirty_expire_centisecs</p>
<p>  默认是3000（单位是1/100秒）。这个值表示page cache中的数据多久之后被标记为脏数据。只有标记为脏的数据在下一个周期到来时pdflush才会刷入到磁盘，这样就意味着用户写的数据在30秒之后才有可能被刷入磁盘，在这期间断电都是会丢数据的。</p>
</li>
<li><p>drop_caches（干净页的回收，缓存清理）</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@nginx1 ~]# cat /proc/sys/vm/drop_caches</span><br><span class="line">0</span><br><span class="line">[root@nginx1 ~]# sync  #先将缓存写入磁盘</span><br><span class="line">[root@nginx1 ~]# echo 1 /proc/sys/vm/drop_caches  #释放所有页缓冲内存</span><br><span class="line">[root@nginx1 ~]# echo 2 /proc/sys/vm/drop_caches  #释放未使用的slab缓冲内存</span><br><span class="line">[root@nginx1 ~]# echo 3 /proc/sys/vm/drop_caches  #释放所有页缓冲和slab缓冲内存</span><br><span class="line">备注：</span><br><span class="line">slab缓存详解（一）http://blog.chinaunix.net/uid-27102327-id-3268687.html</span><br><span class="line">slab缓存详解（二）http://blog.chinaunix.net/uid-27102327-id-3268711.html</span><br><span class="line">http://blog.csdn.net/hs794502825/article/details/7981524</span><br></pre></td></tr></table></figure>
<h3 id="OOM-Kill"><a href="#OOM-Kill" class="headerlink" title="OOM Kill"></a>OOM Kill</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@nginx1 ~]# cat /proc/sys/vm/panic_on_oom     # 0:on, 1:off</span><br><span class="line">0</span><br><span class="line">[root@nginx1 ~]# echo 1 /proc/sys/vm/panic_on_oom  # 关闭oom kill，不推荐</span><br><span class="line">注意：由调整的进程衍生的进程将继承该进程的 oom_score。例如：如果 sshd 进程不受 oom _killer 功能影响，所有由 SSH 会话产生的进程都将不受其影响。</span><br><span class="line"></span><br><span class="line">当内存耗尽时，系统使用oom kill杀死大oom_score（-16~15，2的平方）的进程。oom_score得分由oom_adj得来。</span><br><span class="line">减小oom-adj值，避免被系统杀死：</span><br><span class="line"># echo -17 &gt; /proc/$(pidof sshd)/oom_adj</span><br><span class="line"></span><br><span class="line">-17：避免oom_killer杀死自己</span><br><span class="line">-16~15：帮助计算oom_score</span><br><span class="line">16：预留的最低级别，一般对于缓存的进程才有可能设置成这个级别</span><br></pre></td></tr></table></figure>
<p>有时free查看还有充足的内存，但还是会触发OOM，是因为该进程可能占用了特殊的内存地址空间。</p>
<h3 id="Huge-Page"><a href="#Huge-Page" class="headerlink" title="Huge Page"></a>Huge Page</h3><p>操作系统默认的内存是以4KB分页的，而虚拟地址和内存地址需要转换， 而这个转换要查表，CPU为了加速这个查表过程会内建TLB(Translation Lookaside Buffer)。 显然，如果虚拟页越小，表里的条目数也就越多，而TLB大小是有限的，条目数越多TLB的Cache Miss也就会越高， 所以如果我们能启用大内存页就能间接降低TLB Cache Miss。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@jump204_76 ~]# sysctl -w vm.nr_hugepages=128</span><br><span class="line">vm.nr_hugepages = 128</span><br><span class="line">[root@jump204_76 ~]# cat /proc/meminfo | grep Huge</span><br><span class="line">AnonHugePages:     77824 kB</span><br><span class="line">HugePages_Total:     128</span><br><span class="line">HugePages_Free:      128</span><br><span class="line">HugePages_Rsvd:        0</span><br><span class="line">HugePages_Surp:        0</span><br><span class="line">Hugepagesize:       2048 kB</span><br></pre></td></tr></table></figure>
<h3 id="Transparent-HugePages（透明大页）"><a href="#Transparent-HugePages（透明大页）" class="headerlink" title="Transparent HugePages（透明大页）"></a>Transparent HugePages（透明大页）</h3><ul>
<li><p>什么是Transparent HugePages（透明大页）?</p>
<p>  简单的讲，对于内存占用较大的程序，可以通过开启HugePage来提升系统性能。但这里会有个要求，就是在编写程序时，代码里需要显示的对HugePage进行支持。</p>
<p>  而红帽企业版Linux为了减少程序开发的复杂性，并对HugePage进行支持，部署了Transparent HugePages。Transparent HugePages是一个使管理Huge Pages自动化的抽象层，实现方案为操作系统后台有一个叫做khugepaged的进程，它会一直扫描所有进程占用的内存，在可能的情况下会把4kPage交换为Huge Pages。</p>
<p>  为什么Transparent HugePages（透明大页）对系统的性能会产生影响？<br>  在khugepaged进行扫描进程占用内存，并将4kPage交换为Huge Pages的这个过程中，对于操作的内存的各种分配活动都需要各种内存锁，直接影响程序的内存访问性能。并且，这个过程对于应用是透明的，在应用层面不可控制,对于专门为4k page优化的程序来说，可能会造成随机的性能下降现象。</p>
</li>
<li><p>怎么设置Transparent HugePages（透明大页）?</p>
<ol>
<li><p>查看是否启用透明大页</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@venus153 ~]# cat /sys/kernel/mm/transparent_hugepage/enabled </span><br><span class="line">[always] madvise never</span><br><span class="line">使用命令查看时，如果输出结果为[always]表示透明大页启用了，[never]表示透明大页禁用。</span><br></pre></td></tr></table></figure>
</li>
<li><p>关闭透明大页</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled </span><br><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br></pre></td></tr></table></figure>
<ol start="3">
<li><p>启用透明大页</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo always &gt;  /sys/kernel/mm/transparent_hugepage/enabled </span><br><span class="line">echo always &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>设置开机关闭</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># vim /etc/rc.local</span><br><span class="line">if test -f /sys/kernel/mm/redhat_transparent_hugepage/enabled; then     </span><br><span class="line">     echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled     </span><br><span class="line">    echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag </span><br><span class="line">fi</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
</ul>
<h3 id="进程间通信"><a href="#进程间通信" class="headerlink" title="进程间通信"></a>进程间通信</h3><p>管理命令<br>ipcs<br>ipcrm</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">shm:</span><br><span class="line">	shmmni: 系统级别，所允许使用的共享内存段上限；</span><br><span class="line">	shmall: 系统级别，能够为共享内存分配使用的最大页面数；</span><br><span class="line">	shmmax: 单个共享内存段的上限；</span><br><span class="line">messages:</span><br><span class="line">	msgmnb: 单个消息队列的上限，单位为字节；</span><br><span class="line">	msgmni: 系统级别，消息队列个数上限；</span><br><span class="line">	msgmax: 单个消息大小的上限，单位为字节；</span><br><span class="line"></span><br><span class="line">[root@localhost ~]# sysctl -a | grep shm</span><br><span class="line">kernel.shmmax = 68719476736</span><br><span class="line">kernel.shmall = 4294967296        #单位page，等于4294967296*4096 byte</span><br><span class="line">kernel.shmmni = 4096</span><br><span class="line">kernel.shm_rmid_forced = 0</span><br><span class="line">vm.hugetlb_shm_group = 0</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@nginx1 ~]# ipcs -l</span><br><span class="line"></span><br><span class="line">------ Shared Memory Limits --------</span><br><span class="line">max number of segments = 4096                #段最大数量</span><br><span class="line">max seg size (kbytes) = 67108864               #段大小</span><br><span class="line">max total shared memory (kbytes) = 17179869184</span><br><span class="line">min seg size (bytes) = 1</span><br><span class="line"></span><br><span class="line">------ Semaphore Limits --------</span><br><span class="line">max number of arrays = 128</span><br><span class="line">max semaphores per array = 250</span><br><span class="line">max semaphores system wide = 32000</span><br><span class="line">max ops per semop call = 32</span><br><span class="line">semaphore max value = 32767</span><br><span class="line"></span><br><span class="line">------ Messages: Limits --------</span><br><span class="line">max queues system wide = 32768</span><br><span class="line">max size of message (bytes) = 65536</span><br><span class="line">default max size of queue (bytes) = 65536</span><br></pre></td></tr></table></figure>
<h2 id="I-O-Tuning"><a href="#I-O-Tuning" class="headerlink" title="I/O Tuning"></a>I/O Tuning</h2><h3 id="File-System"><a href="#File-System" class="headerlink" title="File System"></a>File System</h3><ul>
<li><p>Ext4</p>
<ul>
<li><p>内节点表初始化</p>
<p>对于超大文件系统，mkfs.ext4  进程要花很长时间初始化文件系统中到所有内节点表。可使用 -Elazy_itable_init=1 选项延迟这个进程。如果使用这个选项，内核进程将在挂载文件系统后继续初始化<br>该文件它。可使用 mount 命令的 -o init_itable=n 选项控制发生初始化到比例，其中执行这个后台初始化的时间约为 1/n。n 的默认值为 10。</p>
</li>
<li><p>Auto-fsync 行为</p>
<p>因为在重命名、截取或者重新写入某个现有文件后，有些应用程序不总是可以正确执行 fsync()，在重命名和截取操作后，ext4  默认自动同步文件。这个行为与原有到 ext3 文件系统行为大致相同。但 fsync()<br>操作可能会很耗时，因此如果不需要这个自动行为，请在 m ount 命令后使用 -o noauto_da_alloc 选项禁用它。这意味着该程序必须明确使用 fsync() 以保证数据一致。</p>
</li>
<li><p>日志  I/O  优先权</p>
<p>默认情况下，日志注释 I/O 比普通 I/O 的优先权稍高。这个优先权可使用 mount 命令的journal_ioprio=n 选项控制。默认值为 3。有效值范围为 0 -7，其中 0  时最高优先权 I /O。</p>
</li>
</ul>
</li>
<li><p>XFS</p>
</li>
</ul>
<h2 id="Network-Tuning"><a href="#Network-Tuning" class="headerlink" title="Network Tuning"></a>Network Tuning</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tcp_max_tw_buckets: 只允许调大</span><br><span class="line">	tw：保存timewait的连接个数</span><br><span class="line">		established --&gt; tw</span><br></pre></td></tr></table></figure>
<h1 id="结尾"><a href="#结尾" class="headerlink" title="结尾"></a>结尾</h1><ul>
<li>性能观测工具</li>
</ul>
<p><img src="/images/linux/system-tuning/linux_observability_tools.png" alt="linux_observability_tools"></p>
<ul>
<li>性能测评工具</li>
</ul>
<p><img src="/images/linux/system-tuning/linux_benchmarking_tools.png" alt="linux_benchmarking_tools"></p>
<ul>
<li>性能调优工具</li>
</ul>
<p><img src="/images/linux/system-tuning/linux_tuning_tools.png" alt="linux_tuning_tools"></p>
<ul>
<li>sar命令</li>
</ul>
<p><img src="/images/linux/system-tuning/linux_observability_sar.png" alt="linux_observability_sar"></p>
<ul>
<li><a href="http://www.brendangregg.com/linuxperf.html" target="_blank" rel="noopener">Linux Performance大全</a></li>
</ul>
<p><img src="/images/linux/system-tuning/linux_perf_tools_full.png" alt="linux_perf_tools_full"></p>

          
        
      
    </div>

    

    
    
    

    

    
       
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/11/07/linux/performance-tuning/Linux-Performance-Tuning/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/11/07/linux/performance-tuning/Linux-Performance-Tuning/" class="post-title-link" itemprop="http://yoursite.com/index.html">Linux System Tuning</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-11-07 08:55:29" itemprop="dateCreated datePublished" datetime="2018-11-07T08:55:29+08:00">2018-11-07</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2018-12-26 21:10:09" itemprop="dateModified" datetime="2018-12-26T21:10:09+08:00">2018-12-26</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/Linux/" itemprop="url" rel="index"><span itemprop="name">Linux</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><h2 id="Purpose-of-Performance-Tuning"><a href="#Purpose-of-Performance-Tuning" class="headerlink" title="Purpose of Performance Tuning"></a>Purpose of Performance Tuning</h2><ul>
<li>将系统调节成扮演某个角色。譬如：数据库服务器、WEB服务器、文件服务器、邮件服务器等等。</li>
<li>找到并缓解系统瓶颈</li>
<li>调优指标：响应速度，吞吐量<ul>
<li>CPU、内存等硬件在最优情况下等达到最高的性能，你必须清楚。</li>
</ul>
</li>
</ul>
<h2 id="Required-Skills"><a href="#Required-Skills" class="headerlink" title="Required Skills"></a>Required Skills</h2><ul>
<li>Understand both hardware and software</li>
<li>Collecting and analysis of measurable relevant data about a performance problem</li>
<li>Set proper expectations</li>
<li>5 years full time system management experience</li>
</ul>
<h2 id="Tuning-Efficiency（调优效率）"><a href="#Tuning-Efficiency（调优效率）" class="headerlink" title="Tuning Efficiency（调优效率）"></a>Tuning Efficiency（调优效率）</h2><ul>
<li><p>Business Level Tuning</p>
<ul>
<li>Ask right question: “Reduce CPU utilization” or “Business goal”</li>
<li>Adjust workflow（调整业务的流程，减少对系统的不必要请求）</li>
<li>Removing unused services<ul>
<li>PC Smart Card Daemon</li>
<li>Buletooth and hidd</li>
</ul>
</li>
<li>Do i really need the default cron jobs?<ul>
<li>/etc/cron.daily/makewhatis.cron</li>
<li>/etc/cron.daily/mlocate.cron</li>
</ul>
</li>
</ul>
</li>
<li><p>Application Level Tuning</p>
<ul>
<li>Disable or defer expensive operations until analysis?（禁用或延迟对系统而言“很贵的”操作）<ul>
<li>Disable reverse name lookups</li>
<li>Set loglevel to warn for most production daemons</li>
</ul>
</li>
<li><p>Is syslogd a bottleneck?</p>
<ul>
<li>Daemon uses fsync() to flush every file write（系统为保证日志文件不丢失，会立刻调用fsync方法将数据写入磁盘）</li>
<li>Disable by prepending hyphen to name of log file in /etc/rsyslog.conf（在日志文件加“-”号，该日志会滞后写入）</li>
</ul>
<p>Tips：应用层调优，调的是应用程序本身。譬如你使用nginx、nfs，它们本身有大量的参数可用来调优。在应用层调优远远优于内核调优。</p>
</li>
</ul>
</li>
<li><p>Kernel Level Tuning(RH442)</p>
</li>
</ul>
<p>从上往下优化空间越来越小，效果越来越不明显。譬如目前你的WEB Server是apache，调优前首先考虑是否非得使用apache，我们的业务是否是高并发，能不能换成nginx。能够在顶层解决问题，尽量不要希望在底层去解决。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>一个命令敲下去，性能提高10%、20%，这是不切合实际的。红帽操作系统已经调优过了，我们是为了某个特定的角色再进行调优。</li>
<li>不同角色的系统有不同调优的参数，不能指望一个参数搞定所有事情。</li>
<li>物理级别的缺陷，比如硬盘、网卡等由于寿命原因性能大幅下降，则系统层面调优见效甚微。</li>
</ul>
<h1 id="Understand-Hardware"><a href="#Understand-Hardware" class="headerlink" title="Understand Hardware"></a>Understand Hardware</h1><h2 id="Computer-System-Architecture"><a href="#Computer-System-Architecture" class="headerlink" title="Computer System Architecture"></a>Computer System Architecture</h2><p><img src="/images/linux/performance-tuning/computer_system_architecture.jpg" alt="computer_architecture"></p>
<ul>
<li>CPU<ul>
<li>运算器</li>
<li>控制器（指令、数据存储过程）</li>
<li>寄存器（Register）</li>
</ul>
</li>
<li>North bridge（寻址）</li>
</ul>
<h3 id="疑问"><a href="#疑问" class="headerlink" title="疑问"></a>疑问</h3><ul>
<li><p>CPU如何读取RAM中的数据？</p>
<p>  答：使用缓存。CPU将数据从RAM拷贝到L3缓存，再一次拷贝到L2缓存、L1数据缓存，最后到寄存器中。</p>
</li>
<li><p>缓存置换算法</p>
<blockquote>
<p>置换算法影响因素：程序局部性</p>
<ul>
<li>空间局部性:</li>
<li>时间局部性:数据很可能会再短时间内被再次访问。</li>
</ul>
</blockquote>
<ul>
<li><p>LRU(Least recently used)</p>
<p>  最近最少使用算法，这个缓存算法将最近使用的条目存放到靠近缓存顶部的位置。当一个新条目被访问时，LRU将它放置到缓存的顶部。当缓存达到极限时，较早之前访问的条目将从缓存底部开始被移除。</p>
</li>
<li><p>MRU(Most recently used)</p>
<p>  最近最常使用算法，将使用时间距离现在最近的那条记录替换掉。</p>
</li>
</ul>
</li>
<li><p>CPU如何更新数据？</p>
<p>  答：CPU依次更新L1、L2、L3缓存对应的数据，最后更新RAM。</p>
</li>
<li><p>CPU Cache写机制</p>
<ul>
<li><p>write through</p>
<p>  cpu向cache写入数据时，同时向memory(后端存储)也写一份，使cache和memory的数据保持一致。优点是简单；缺点是每次都要访问memory，速度比较慢。</p>
</li>
<li><p>write back</p>
<p>cpu更新cache时，只是把更新的cache区标记一下，并不同步更新memory。当cache要被置换出去时，才去更新memory(后端存储)。优点是数据写入速度快，因为不需要写存储；缺点是一旦更新后的数据未被写入存储时出现系统掉电的情况，数据将无法找回。</p>
</li>
</ul>
</li>
</ul>
<h2 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h2><h3 id="X86架构机器三个特点（RH442）"><a href="#X86架构机器三个特点（RH442）" class="headerlink" title="X86架构机器三个特点（RH442）"></a>X86架构机器三个特点（RH442）</h3><ul>
<li><p>I/O Address</p>
<p>  X86架构CPU把外设寄存器看做是一个独立的地址空间，访问内存的指令不能用来访问这些外设寄存器，而需要用专用的指令（如IN、OUT指令），称为“I/O端口”。不同外设使用不同“I/O端口”，PCI总线就能知道是哪个外设了，当然“I/O端口”会被消耗完。</p>
<p>  Power PC、ARM的CPU把外设寄存器看做是内存的一部分、寄存器参与内存统一编址，通过一般的内存指令来访问这些外设寄存器，称为“I/O内存”。</p>
</li>
<li>IRQ（中断请求）</li>
<li><p>DMA（直接内存访问）</p>
<p>  DMA主要功能是传输数据，但是不需要占用CPU（不需要中断）。传输数据从外设到存储器或者从存储器到存储器。</p>
<p>  在实现DMA传输时，是由DMA控制器直接掌管总线，因此，存在着一个总线控制权转移问题。即DMA传输前，CPU要把总线控制权交给DMA控制器，而在结束DMA传输后，DMA控制器应立即把总线控制权再交回给CPU。</p>
<p>  完整的DMA传输过程必须经过下面的4个步骤 ：</p>
<ol>
<li><p>DMA请求</p>
<p> CPU对DMA控制器初始化，并向I/O接口发出操作命令，I/O接口提出DMA请求。</p>
</li>
<li><p>DMA响应</p>
<p> DMA控制器对DMA请求判别优先级及屏蔽，向总线裁决逻辑提出总线请求。当CPU执行完当前总线周期即可释放总线控制权。此时，总线裁决逻辑输出总线应答，表示DMA已经响应，通过DMA控制器通知I/O接口开始DMA传输。</p>
</li>
<li><p>DMA传输</p>
<p> DMA控制器获得总线控制权后，CPU即刻挂起或只执行内部操作，由DMA控制器输出读写命令，直接控制RAM与I/O接口进行DMA传输。在DMA控制器的控制下，在和外部设备之间直接进行数据传送，在传送过程中不需要的参与。开始时需提供要传送的数据的起始位置和数据长度。</p>
</li>
<li><p>DMA结束</p>
<p> 当完成规定的成批数据传送后，DMA控制器即释放总线控制权，并向I/O接口发出结束信号。当I/O接口收到结束信号后，一方面停止I/O设备的工作，另一方面向CPU提出中断请求，使CPU从不介入的状态解脱，并执行一段检查本次DMA传输操作正确性的代码。最后，带着本次操作结果及状态继续执行原来的程序。</p>
<p>由此可见，DMA传输方式无需CPU直接控制传输，也没有中断处理方式那样保留现场和恢复现场的过程，通过硬件为RAM与I/O设备开辟一条直接传送数据的通路，使CPU的效率大为提高。</p>
<p>参考：<a href="https://blog.csdn.net/lg1259156776/article/details/50802220" target="_blank" rel="noopener">DMA原理</a></p>
<p>扩展：内存中起始16M空间，是留给DMA使用的，用于把数据装载进内存空间低地址空间。内存中另有1M空间是给BIOS预留，计算机有自举功能，用于初始化足够的软件来查找并加载功能完整的操作系统。</p>
</li>
</ol>
</li>
</ul>
<h3 id="查看CPU"><a href="#查看CPU" class="headerlink" title="查看CPU"></a>查看CPU</h3><ul>
<li><p>总览</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# lscpu </span><br><span class="line">Architecture:          x86_64           #x86架构</span><br><span class="line">CPU op-mode(s):        32-bit, 64-bit</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                40               #逻辑CPU个数</span><br><span class="line">On-line CPU(s) list:   0-39</span><br><span class="line">Thread(s) per core:    2                #每核的线程数</span><br><span class="line">Core(s) per socket:    10               #每颗CPU的核数</span><br><span class="line">Socket(s):             2                #物理CPU个数</span><br><span class="line">NUMA node(s):          2                # NUMA节点数</span><br><span class="line">Vendor ID:             GenuineIntel</span><br><span class="line">CPU family:            6</span><br><span class="line">Model:                 79</span><br><span class="line">Model name:            Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz</span><br><span class="line">Stepping:              1</span><br><span class="line">CPU MHz:               1200.093</span><br><span class="line">BogoMIPS:              4805.86</span><br><span class="line">Virtualization:        VT-x</span><br><span class="line">L1d cache:             32K             #一级数据缓存</span><br><span class="line">L1i cache:             32K             #一级指令缓存</span><br><span class="line">L2 cache:              256K            #二级缓存</span><br><span class="line">L3 cache:              25600K          #三级缓存</span><br><span class="line">NUMA node0 CPU(s):     0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38</span><br><span class="line">NUMA node1 CPU(s):     1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39</span><br><span class="line"></span><br><span class="line">[root@yadoom ~]# lscpu </span><br><span class="line">Architecture:          x86_64           #x86架构</span><br><span class="line">CPU op-mode(s):        32-bit, 64-bit</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                8                #逻辑CPU个数</span><br><span class="line">On-line CPU(s) list:   0-7</span><br><span class="line">Thread(s) per core:    1                #每核的线程数</span><br><span class="line">Core(s) per socket:    4                #每颗CPU的核数</span><br><span class="line">Socket(s):             2                #物理CPU个数</span><br><span class="line">NUMA node(s):          2                # NUMA节点数</span><br><span class="line">Vendor ID:             GenuineIntel</span><br><span class="line">CPU family:            6</span><br><span class="line">Model:                 62</span><br><span class="line">Stepping:              4</span><br><span class="line">CPU MHz:               2499.904</span><br><span class="line">BogoMIPS:              4999.28</span><br><span class="line">Virtualization:        VT-x</span><br><span class="line">L1d cache:             32K             #一级数据缓存</span><br><span class="line">L1i cache:             32K             #一级指令缓存</span><br><span class="line">L2 cache:              256K            #二级缓存</span><br><span class="line">L3 cache:              10240K          #三级缓存</span><br><span class="line">NUMA node0 CPU(s):     0,2,4,6</span><br><span class="line">NUMA node1 CPU(s):     1,3,5,7</span><br></pre></td></tr></table></figure>
</li>
<li><p>核数</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# grep CPU /proc/cpuinfo</span><br><span class="line">model name      : Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz</span><br><span class="line">model name      : Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz</span><br><span class="line">......</span><br><span class="line">model name      : Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz</span><br><span class="line">model name      : Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz</span><br><span class="line">[root@yadoom ~]# grep CPU /proc/cpuinfo | wc -l</span><br><span class="line">40</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="CPU架构"><a href="#CPU架构" class="headerlink" title="CPU架构"></a>CPU架构</h3><ul>
<li>SMP架构：Symmetric Multi-Processor</li>
</ul>
<p>大多数早期的单处理机系统的设计为让每个 CPU 到每个内存位置都使用同一逻辑路径（一般是平行总线）。这样每次 CPU 访问任意位置的内存时与其他系统中的 CPU 对内存的访问消耗的时间是相同的。此类架构就是我们所说的对称多处理器（SMP）系统。SMP 适合 CPU 数较少的系统，但一旦 CPU 计数超过某一点（8 或者 16），要满足对内存的平等访问所需的平行 trace 数就会使用过多的板载资源，留给外设的空间就太少。</p>
<p><img src="/images/linux/performance-tuning/uma.png" alt="uma"></p>
<ul>
<li>UNMA架构：Non-Uniform Memory Access</li>
</ul>
<p>不是为每个处理器包提供对等的内存访问，而是让每个包/插槽组合有一个或者多个专用内存区以便提供高速访问。每个插槽还有到另一个插槽的互联以便提供对其他插槽内存的低速访问。</p>
<p>下图中 CPU0 访问左边的内存条大约需要三个时钟周期：一个周期是将地址发给内存控制器，一个周期是设置对该内存位置的访问，一个周期是读取或者写入到该位置。但 CPU1 可能需要 6 个时钟周期方可访问内存的同一位置，因为它位于不同的插槽，必须经过两个内存控制器：插槽 1 中的本地内存控制器和插槽 0  中的远程内存控制器。如果在那个位置出现竞争（即如果有一个以上 CPU 同时尝试访问同一位置），内存控制器需要对该内存进行随机且连续的访问，所以内存访问所需时间会较长。添加缓存一致性（保证本地 CPU 缓存包含同一内存位置的相同数据）会让此过程更为复杂。</p>
<p><img src="/images/linux/performance-tuning/numa.png" alt="numa"></p>
<p><img src="/images/linux/performance-tuning/numa_mac.png" alt="numa"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Tips: 当CPU有多颗时，物理CPU常见通讯方法有三种：</span><br><span class="line">- FSB（传统的前端总线，常用于PC机）</span><br><span class="line">- QPI(intel)</span><br><span class="line">- HyperTransport(AMD)</span><br></pre></td></tr></table></figure>
<ul>
<li>MPP架构：Massive Parallel Processing</li>
</ul>
<p>和 NUMA 不同， MPP 提供了另外一种进行系统扩展的方式，它由多个 SMP 服务器通过一定的节点互联网络进行连接，协同工作，完成相同的任务，从用户的角度来看是一个服务器系统。其基本特征是由多个 SMP 服务器 ( 每个 SMP 服务器称节点 ) 通过节点互联网络连接而成，每个节点只访问自己的本地资源 ( 内存、存储等 ) ，是一种完全无共享 (Share Nothing) 结构，因而扩展能力最好，理论上其扩展无限制，目前的技术可实现 512 个节点互联，数千个 CPU 。目前业界对节点互联网络暂无标准，如 NCR 的 Bynet ， IBM 的 SPSwitch ，它们都采用了不同的内部实现机制。但节点互联网仅供 MPP 服务器内部使用，对用户而言是透明的。</p>
<p>在 MPP 系统中，每个 SMP 节点也可以运行自己的操作系统、数据库等。但和 NUMA 不同的是，它不存在异地内存访问的问题。换言之，每个节点内的 CPU 不能访问另一个节点的内存。节点之间的信息交互是通过节点互联网络实现的，这个过程一般称为数据重分配 (Data Redistribution) 。</p>
<p>但是 MPP 服务器需要一种复杂的机制来调度和平衡各个节点的负载和并行处理过程。目前一些基于 MPP 技术的服务器往往通过系统级软件 ( 如数据库 ) 来屏蔽这种复杂性。举例来说， NCR 的 Teradata 就是基于 MPP 技术的一个关系数据库软件，基于此数据库来开发应用时，不管后台服务器由多少个节点组成，开发人员所面对的都是同一个数据库系统，而不需要考虑如何调度其中某几个节点的负载。</p>
<p>MPP (Massively Parallel Processing)，大规模并行处理系统，这样的系统是由许多松耦合的处理单元组成的，要注意的是这里指的是处理单元而不是处理器。每个单元内的CPU都有自己私有的资源，如总线，内存，硬盘等。在每个单元内都有操作系统和管理数据库的实例复本。这种结构最大的特点在于不共享资源。</p>
<h2 id="Numa-Architecture"><a href="#Numa-Architecture" class="headerlink" title="Numa Architecture"></a>Numa Architecture</h2><h3 id="查看Numa-node"><a href="#查看Numa-node" class="headerlink" title="查看Numa node"></a>查看Numa node</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# numactl --hardware</span><br><span class="line">available: 2 nodes (0-1)     #当前机器有2个NUMA node，编号0、1</span><br><span class="line">node 0 cpus: 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30</span><br><span class="line">node 0 size: 32722 MB     #物理内存大小</span><br><span class="line">node 0 free: 2352 MB      #当前free内存大小</span><br><span class="line">node 1 cpus: 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31</span><br><span class="line">node 1 size: 32768 MB</span><br><span class="line">node 1 free: 12314 MB</span><br><span class="line">node distances:           #node距离，可以简单认为Node内部访问及跨Node访问的成本。</span><br><span class="line">node   0   1 </span><br><span class="line">  0:  10  20              #由此可知跨node访问内存的成本是 内部访问的2倍。</span><br><span class="line">  1:  20  10</span><br></pre></td></tr></table></figure>
<h3 id="查看node"><a href="#查看node" class="headerlink" title="查看node"></a>查看node</h3><ul>
<li><p>node0包含的CPU</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# ls -l /sys/devices/system/node/node0/</span><br><span class="line">cpu0/    cpu12/   cpu16/   cpu2/    cpu22/   cpu26/   cpu30/   cpu6/    cpulist </span><br><span class="line">cpu10/   cpu14/   cpu18/   cpu20/   cpu24/   cpu28/   cpu4/    cpu8/    cpumap</span><br></pre></td></tr></table></figure>
</li>
<li><p>/sys/devices/system/cpu</p>
<p>  系统的 CPU 是如何互相连接的信息。</p>
</li>
<li><p>/sys/devices/system/node</p>
<p>  系统中 NUMA 节点以及那些节点间相对距离的信息。</p>
</li>
</ul>
<h3 id="查看cpu-cache"><a href="#查看cpu-cache" class="headerlink" title="查看cpu cache"></a>查看cpu cache</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# ls -l /sys/devices/system/cpu/cpu0/cache/</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x 2 root root 0 Nov 10  2017 index0   # 1级数据cache</span><br><span class="line">drwxr-xr-x 2 root root 0 Nov  3  2017 index1   # 1级指令cache</span><br><span class="line">drwxr-xr-x 2 root root 0 Nov  3  2017 index2   # 2级cache</span><br><span class="line">drwxr-xr-x 2 root root 0 Nov 10  2017 index3   # 3级cache, 对应cpuinfo里的cache</span><br></pre></td></tr></table></figure>
<p>目录里的文件是cache信息描述，以本机的cpu0/index0为例简单解释一下：</p>
<table><br>    <tr><br>        <td>文件</td><br>        <td>内容</td><br>        <td>说明</td><br>   </tr><br>   <tr><br>        <td>type</td><br>        <td>Data</td><br>        <td>数据cache，如果查看index1就是Instruction</td><br>   </tr><br>   <tr><br>        <td>Level</td><br>        <td>1</td><br>        <td>L1</td><br>    </tr><br>    <tr><br>        <td>Size</td><br>        <td>32K</td><br>        <td>大小为32K</td><br>    </tr><br>    <tr><br>        <td>coherency_line_size</td><br>        <td>64</td><br>        <td rowspan="4">64 <em> 4 </em> 128=32K</td><br>    </tr><br>    <tr><br>        <td>physical_line_partition</td><br>        <td>1</td><br>    </tr><br>    <tr><br>        <td>ways_of_associativity</td><br>        <td>4</td><br>    </tr><br>    <tr><br>        <td>number_of_sets</td><br>        <td>128</td><br>    </tr><br>    <tr><br>        <td>shared_cpu_map</td><br>        <td>00000101</td><br>        <td>表示这个cache被CPU0和CPU8 share</td><br>    </tr><br></table>

<p>解释一下shared_cpu_map内容的格式，表面上看是2进制，其实是16进制表示，每个bit表示一个cpu，1个数字可以表示4个cpu<br>截取00000101的后4位，转换为2进制表示。</p>
<table>
<thead>
<tr>
<th>CPU id</th>
<th>15</th>
<th>14</th>
<th>13</th>
<th>12</th>
<th>11</th>
<th>10</th>
<th>9</th>
<th>8</th>
<th>7</th>
<th>6</th>
<th>5</th>
<th>4</th>
<th>3</th>
<th>2</th>
<th>1</th>
<th>0</th>
</tr>
</thead>
<tbody>
<tr>
<td>0×0101的2进制表示</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>0101表示cpu8和cpu0，即cpu0的L1 data cache是和cpu8共享的。</p>
<p>再看一下index3 shared_cpu_map的例子</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cat /sys/devices/system/cpu/cpu0/cache/index3/shared_cpu_map</span><br><span class="line">00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000f0f</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>CPU id</th>
<th>15</th>
<th>14</th>
<th>13</th>
<th>12</th>
<th>11</th>
<th>10</th>
<th>9</th>
<th>8</th>
<th>7</th>
<th>6</th>
<th>5</th>
<th>4</th>
<th>3</th>
<th>2</th>
<th>1</th>
<th>0</th>
</tr>
</thead>
<tbody>
<tr>
<td>0x0f0f的2进制表示</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>cpu0,1,2,3和cpu8,9,10,11共享L3 cache。</p>
<h3 id="查看numa状态"><a href="#查看numa状态" class="headerlink" title="查看numa状态"></a>查看numa状态</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# numastat</span><br><span class="line">                           node0           node1</span><br><span class="line">numa_hit              1011487518       716368222</span><br><span class="line">numa_miss                      0       145365467</span><br><span class="line">numa_foreign           145365467               0</span><br><span class="line">interleave_hit             20673           20631</span><br><span class="line">local_node            1011487341       716343592</span><br><span class="line">other_node                   177       145390097</span><br></pre></td></tr></table></figure>
<p>上述可知node 0的unma_miss过高，可考虑用numactl将进程和CPU绑定。详见CPU调优章节。</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>numa_hit</td>
<td>为这个节点成功的分配尝试数。</td>
</tr>
<tr>
<td>numa_miss</td>
<td>由于在目的节点中内存较低而尝试为这个节点分配到另一个节点的数目。每个 numa_miss 事件都在另一个节点中有对应的 numa_foreign 事件。</td>
</tr>
<tr>
<td>numa_foreign</td>
<td>最初要为这个节点但最后分配个另一个节点的分配数。每个 numa_foreign 事件都在另一个节点中有对应的 numa_miss 事件。</td>
</tr>
<tr>
<td>interleave_hit</td>
<td>成功分配给这个节点的尝试交错策略数。</td>
</tr>
<tr>
<td>local_node</td>
<td>这个节点中的进程成功在这个节点中分配内存的次数。</td>
</tr>
<tr>
<td>other_node</td>
<td>这个节点中的进程成功在另一个节点中分配内存的次数。</td>
</tr>
</tbody>
</table>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# numactl --show</span><br><span class="line">policy: default</span><br><span class="line">preferred node: current</span><br><span class="line">physcpubind: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 </span><br><span class="line">cpubind: 0 1</span><br><span class="line">nodebind: 0 1</span><br><span class="line">membind: 0 1</span><br></pre></td></tr></table></figure>
<h3 id="查看内存numa-node分布"><a href="#查看内存numa-node分布" class="headerlink" title="查看内存numa node分布"></a>查看内存numa node分布</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/&lt;pid&gt;/numa_maps</span><br><span class="line">cat /proc/$(pidof pname|cut -d &quot;&quot; -f1)/numa_maps</span><br></pre></td></tr></table></figure>
<h3 id="查看线程run在哪个processor"><a href="#查看线程run在哪个processor" class="headerlink" title="查看线程run在哪个processor"></a>查看线程run在哪个processor</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">top -p $(pidof pname|sed -e &apos;s/ /,/g&apos;)</span><br><span class="line"></span><br><span class="line">在默认配置下不显示线程信息，需要进入Top后按“shift+H”，打开线程显示。</span><br><span class="line">另外，如果没有P列，还需要按“f”，按“j”，添加，这一列显示的数字就是这个线程上次run的processor id。</span><br></pre></td></tr></table></figure>
<p><a href="https://blog.jcole.us/2010/09/28/mysql-swap-insanity-and-the-numa-architecture/" target="_blank" rel="noopener">https://blog.jcole.us/2010/09/28/mysql-swap-insanity-and-the-numa-architecture/</a><br><a href="https://software.intel.com/en-us/articles/intel-64-architecture-processor-topology-enumeration/" target="_blank" rel="noopener">https://software.intel.com/en-us/articles/intel-64-architecture-processor-topology-enumeration/</a></p>
<h3 id="numad"><a href="#numad" class="headerlink" title="numad"></a>numad</h3><pre><code>numad 是一个自动 NUMA 亲和性管理守护进程，它监控系统中的 NUMA 拓扑以及资源使用以便动态提高 NUMA 资源分配和管理（以及系统性能）。

numad 不会在进程只运行几分钟或者不会消耗很多资源时改进性能。

有连续不可预测内存访问的系统，比如大型内存中的数据库也不大可能从 numad 使用中受益。
</code></pre><h2 id="CPU-Cache"><a href="#CPU-Cache" class="headerlink" title="CPU Cache"></a>CPU Cache</h2><h3 id="Levels-of-CPU-Caches"><a href="#Levels-of-CPU-Caches" class="headerlink" title="Levels of CPU Caches"></a>Levels of CPU Caches</h3><ul>
<li>Level 1 private cache(on cpu chip)<ul>
<li>SRAM(static memory)：集成度低（存储相同的数据，静态RAM的体积是动态RAM的6倍），价格高（同容量的静态RAM是动态RAM的四倍）</li>
</ul>
</li>
<li>Level 2 private or shared cache(on cpu chip)<ul>
<li>DRAM(high speed dynamic memory)</li>
</ul>
</li>
<li>Level 3 private or shared cache(usually on mainboard)<ul>
<li>shared between cores inside a CPU socket</li>
</ul>
</li>
<li>Level 4 fully shared cache<ul>
<li>shared between CPU sockets</li>
</ul>
</li>
</ul>
<h3 id="Cache-Memory-Types"><a href="#Cache-Memory-Types" class="headerlink" title="Cache Memory Types"></a>Cache Memory Types</h3><p>内存和缓存的关联地址。</p>
<ul>
<li>Direct Mapped Cache<br>  a memory location can be cached into one cache line.（一个内存地址，只能被映射到缓存的一个区域。）</li>
<li>Full Associative Cache<br>  a memory location can be cached into any cache line.（一个内存地址，可以被映射到缓存所有区域。）</li>
<li>n-Way Associative Cache<br>  Fair,most used, a memory location can be cached into any one of n cache lines.</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[root@kvm-2 ~]# x86info -c</span><br><span class="line">x86info v1.25.  Dave Jones 2001-2009</span><br><span class="line">Feedback to &lt;davej@redhat.com&gt;.</span><br><span class="line"></span><br><span class="line">Found 8 CPUs</span><br><span class="line">--------------------------------------------------------------------------</span><br><span class="line">CPU #1</span><br><span class="line">EFamily: 0 EModel: 3 Family: 6 Model: 62 Stepping: 4</span><br><span class="line">CPU Model: Unknown model. </span><br><span class="line">Processor name string: Intel(R) Xeon(R) CPU E5-2609 v2 @ 2.50GHz</span><br><span class="line">Type: 0 (Original OEM)  Brand: 0 (Unsupported)</span><br><span class="line">Number of cores per physical package=16</span><br><span class="line">Number of logical processors per socket=32</span><br><span class="line">Number of logical processors per core=2</span><br><span class="line">APIC ID: 0x0    Package: 0  Core: 0   SMT ID 0</span><br><span class="line">Cache info</span><br><span class="line">TLB info</span><br><span class="line"> Instruction TLB: 4K pages, 4-way associative, 128 entries.   # 4路关联</span><br><span class="line"> Data TLB: 4KB pages, 4-way associative, 64 entries</span><br><span class="line"> 64 byte prefetching.</span><br><span class="line">Found unknown cache descriptors: 63 76 ca ff </span><br><span class="line">--------------------------------------------------------------------------</span><br></pre></td></tr></table></figure>
<h3 id="查看CPU-Cache"><a href="#查看CPU-Cache" class="headerlink" title="查看CPU Cache"></a>查看CPU Cache</h3><pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# lscpu -p</span><br><span class="line"># The following is the parsable format, which can be fed to other</span><br><span class="line"># programs. Each different item in every column has an unique ID</span><br><span class="line"># starting from zero.</span><br><span class="line"># CPU,Core,Socket,Node,,L1d,L1i,L2,L3    # “Node”表示NUMA nodes</span><br><span class="line">0,0,0,0,,0,0,0,0</span><br><span class="line">1,1,1,1,,1,1,1,1</span><br><span class="line">2,2,0,0,,2,2,2,0</span><br><span class="line">3,3,1,1,,3,3,3,1</span><br><span class="line">4,4,0,0,,4,4,4,0</span><br><span class="line">5,5,1,1,,5,5,5,1</span><br><span class="line">6,6,0,0,,6,6,6,0</span><br><span class="line">7,7,1,1,,7,7,7,1</span><br><span class="line"></span><br><span class="line">[root@yadoom ~]# dmidecode -t 7</span><br><span class="line"># dmidecode 2.11</span><br><span class="line">SMBIOS 2.7 present.</span><br><span class="line"></span><br><span class="line">Handle 0x0700, DMI type 7, 19 bytes</span><br><span class="line">Cache Information</span><br><span class="line">        Socket Designation: Not Specified</span><br><span class="line">        Configuration: Enabled, Not Socketed, Level 1</span><br><span class="line">        Operational Mode: Write Through</span><br><span class="line">        Location: Internal</span><br><span class="line">        Installed Size: 128 kB         #一级缓存（32k * 4）</span><br><span class="line">        Maximum Size: 128 kB</span><br><span class="line">        Supported SRAM Types:</span><br><span class="line">                Unknown</span><br><span class="line">        Installed SRAM Type: Unknown</span><br><span class="line">        Speed: Unknown</span><br><span class="line">        Error Correction Type: Single-bit ECC</span><br><span class="line">        System Type: Data</span><br><span class="line">        Associativity: 8-way Set-associative</span><br><span class="line"></span><br><span class="line">Handle 0x0701, DMI type 7, 19 bytes</span><br><span class="line">Cache Information</span><br><span class="line">        Socket Designation: Not Specified</span><br><span class="line">        Configuration: Enabled, Not Socketed, Level 2</span><br><span class="line">        Operational Mode: Write Through</span><br><span class="line">        Location: Internal</span><br><span class="line">        Installed Size: 1024 kB       #二级缓存（256k * 4）</span><br><span class="line">        Maximum Size: 1024 kB</span><br><span class="line">        Supported SRAM Types:</span><br><span class="line">                Unknown</span><br><span class="line">        Installed SRAM Type: Unknown</span><br><span class="line">        Speed: Unknown</span><br><span class="line">        Error Correction Type: Single-bit ECC</span><br><span class="line">        System Type: Unified</span><br><span class="line">        Associativity: 8-way Set-associative</span><br><span class="line"></span><br><span class="line">Handle 0x0702, DMI type 7, 19 bytes</span><br><span class="line">Cache Information</span><br><span class="line">        Socket Designation: Not Specified</span><br><span class="line">        Configuration: Enabled, Not Socketed, Level 3</span><br><span class="line">        Operational Mode: Write Back</span><br><span class="line">        Location: Internal</span><br><span class="line">        Installed Size: 10240 kB    #与lscpu数值一样，说明4核共享三级缓存</span><br><span class="line">        Maximum Size: 10240 kB</span><br><span class="line">        Supported SRAM Types:</span><br><span class="line">                Unknown</span><br><span class="line">        Installed SRAM Type: Unknown</span><br><span class="line">        Speed: Unknown</span><br><span class="line">        Error Correction Type: Single-bit ECC</span><br><span class="line">        System Type: Unified</span><br><span class="line">        Associativity: 20-way Set-associative</span><br></pre></td></tr></table></figure>
</code></pre><p>由上输出可得知该服务器的L1，L2是每个核心独享的，L3是共享的。</p>
<h2 id="Memory"><a href="#Memory" class="headerlink" title="Memory"></a>Memory</h2><ul>
<li><p>Memory size,max allowed</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# free</span><br><span class="line">             total       used       free     shared    buffers     cached</span><br><span class="line">Mem:      90743596   23288228   67455368          0     684452    2938648</span><br><span class="line">-/+ buffers/cache:   19665128   71078468</span><br><span class="line">Swap:     26214392          0   26214392</span><br><span class="line"></span><br><span class="line">[root@yadoom ~]# more /proc/meminfo </span><br><span class="line">MemTotal:       263860344 kB      # 已经减去了显卡占用的内存</span><br><span class="line">MemFree:        206924460 kB</span><br><span class="line">MemAvailable:   248609828 kB</span><br><span class="line">Buffers:          145604 kB</span><br><span class="line">Cached:         51754488 kB</span><br><span class="line">SwapCached:            0 kB</span><br><span class="line">Active:         45933660 kB</span><br><span class="line">Inactive:        7115240 kB</span><br><span class="line">Active(anon):    8130140 kB</span><br><span class="line">Inactive(anon):  4851368 kB</span><br><span class="line">Active(file):   37803520 kB</span><br><span class="line">Inactive(file):  2263872 kB</span><br><span class="line"></span><br><span class="line">[root@yadoom ~]# dmidecode -t 17</span><br><span class="line"># dmidecode 2.11</span><br><span class="line">SMBIOS 2.7 present.</span><br><span class="line"></span><br><span class="line">Handle 0x1100, DMI type 17, 34 bytes</span><br><span class="line">Memory Device</span><br><span class="line">        Array Handle: 0x1000</span><br><span class="line">        Error Information Handle: Not Provided</span><br><span class="line">        Total Width: 72 bits</span><br><span class="line">        Data Width: 64 bits</span><br><span class="line">        Size: 8192 MB                 # 内存大小</span><br><span class="line">        Form Factor: DIMM</span><br><span class="line">        Set: 1</span><br><span class="line">        Locator: DIMM_A1 </span><br><span class="line">        Bank Locator: Not Specified</span><br><span class="line">        Type: DDR3                   # DDR3代</span><br><span class="line">        Type Detail: Synchronous Registered (Buffered)</span><br><span class="line">        Speed: 1333 MHz              # 时钟频率</span><br><span class="line">        Manufacturer: 00CE04B300CE</span><br><span class="line">        Serial Number: 4400B1EA</span><br><span class="line">        Asset Tag: 01104611</span><br><span class="line">        Part Number: M393B1K70CH0-YH9  </span><br><span class="line">        Rank: 2</span><br><span class="line">        Configured Clock Speed: 1333 MHz</span><br><span class="line">......</span><br></pre></td></tr></table></figure>
</li>
<li><p>bandwidth and letency</p>
<ul>
<li>DDR2(2 bits),DDR3(4 bits),DDR4(8 bits)</li>
<li>Bandwidth = Clock rate(时钟频率) <em> 4(DDR3) </em> 2(Double) * bits / 8（Double就是D，脉冲升频降会各取一次数据。带宽无需手动计算，内存卡会标识PC xxxxMB）</li>
<li>Letency(wait time before read again,in ns)读取内存时，要等待的时间。动态内存需要电门不停的刷，所以读取数据需要时间。</li>
<li>ECC(slower,safer)<ul>
<li>Corrects single-bit errors</li>
<li>Detects multiple-bit errors</li>
</ul>
</li>
</ul>
</li>
<li>Method of memory accessing<ul>
<li>UMA,NUMA</li>
</ul>
</li>
</ul>
<h2 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h2><ul>
<li>Type of storage used<ul>
<li>mechanical magnetic platters（机械磁盘）</li>
<li>SSD devices（固态磁盘）</li>
</ul>
</li>
<li>Hardware RAID Level<ul>
<li>stripe depth（条带深度）</li>
<li>stripe width（条带宽度）</li>
<li>stripe size = stripe depth × stripe width</li>
</ul>
</li>
<li>Direct-attached Storage（直连存储）<ul>
<li>SATA,SAS,IDE</li>
</ul>
</li>
<li>SCSI,Fibre Channel,ISCSI<ul>
<li>Bandwidth,latency,multipath</li>
</ul>
</li>
</ul>
<h3 id="磁盘架构"><a href="#磁盘架构" class="headerlink" title="磁盘架构"></a>磁盘架构</h3><ul>
<li><p>Capacity</p>
<ul>
<li>CLV(Constant Linear Velocity)</li>
<li>CAV(Constant Angular Velocity)</li>
<li><p>Zoned CAV</p>
<p><img src="/images/linux/performance-tuning/storage_discs_capacity.jpg" alt="storage-discs-capacity"></p>
</li>
</ul>
</li>
<li>整体结构<br>  硬盘中包含多个硬盘盘片，硬盘盘片为圆形，每个硬盘盘片都有一个可以读写的磁头(Head)，将这个磁头固定，使硬盘盘片旋转一周，所走轨迹就是磁道(Track)。硬盘内所有盘片的相同磁道号的集合成为磁柱(Cylinder)。每一磁道被划分成许多区域，每个区域叫一个扇区(Sector)。扇区是硬盘的最小存储物理量，一个扇区的存储容量大约是512字节(约0.5K)。</li>
<li><p>概念</p>
<ul>
<li><p>geometry</p>
<p>  geometry应该翻译为几何数据，其实就是指的CHS(Cylinder、Head、Sector/Track) 。</p>
<ul>
<li>Cylinder（每面盘片上有几条磁道）</li>
<li>Head（磁头数量，也就是几面盘片。）</li>
<li>Sector/Track（每条磁道上有几个扇区）</li>
</ul>
</li>
<li>sector</li>
<li>block</li>
</ul>
</li>
</ul>
<p>Linux里文件的文件名、文件属性、文件内容是分别存储的：文件名存放在目录项（即dentry）中，文件属性存放在inode中，一般情况下，每个inode占用128Bity磁盘空间，文件内容存放在block中。每个block最多存放一个文件，而当一个block存放不下的情况下，会占用下一个block。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 总容量：heads * cylinders * sectors * Sector size</span><br><span class="line">[root@localhost ~]# fdisk -l /dev/sda</span><br><span class="line"></span><br><span class="line">Disk /dev/sdd: 299.4 GB, 299439751168 bytes</span><br><span class="line">255 heads, 63 sectors/track, 36404 cylinders</span><br><span class="line">Units = cylinders of 16065 * 512 = 8225280 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br><span class="line">Disk identifier: 0x00056494</span><br><span class="line">......</span><br><span class="line">[root@localhost ~]# tune2fs -l /dev/sda1 | grep Block</span><br><span class="line">Block count:              204800</span><br><span class="line">Block size:               1024</span><br><span class="line">Blocks per group:         8192</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# vmstat 1 3</span><br><span class="line">procs    -----------memory----------   ---swap-- ----io---- --system-- -----cpu-----</span><br><span class="line"> r  b     swpd   free   buff  cache     si   so    bi    bo   in   cs us sy  id wa st</span><br><span class="line"> 1  0      0 260352448  34480 2212952    0    0     0     1    2    3  0  0 100  0  0</span><br><span class="line"> 0  0      0 260352192  34480 2212956    0    0     0     0  124  177  0  0 100  0  0</span><br><span class="line"> 0  0      0 260352192  34480 2212956    0    0     0     0   27   44  0  0 100  0  0</span><br></pre></td></tr></table></figure>
<p>bi bo 的单位是块，但是它不是操作系统里的那个块，它表示物理磁盘的块（sector，扇区），每个扇区 512 字节。</p>
<p>磁盘外圈速度比里圈速度快得多，所以操作系统默认将swap分区分配在里圈。</p>
<h3 id="磁盘速度"><a href="#磁盘速度" class="headerlink" title="磁盘速度"></a>磁盘速度</h3><p>一般都指Burst speed（顺序读写）速率。</p>
<p><img src="/images/linux/performance-tuning/storage_bandwidth.png" alt="storage-bandwidth"></p>
<h3 id="磁盘类型"><a href="#磁盘类型" class="headerlink" title="磁盘类型"></a>磁盘类型</h3><ul>
<li><p>IDE（并口）</p>
<p>  IDE（Integrated Drive Electronics电子集成驱动器）的缩写，它的本意是指把控制器与盘体集成在一起的硬盘驱动器，是一种硬盘的传输接口，它有另一个名称叫做ATA（Advanced Technology Attachment），这两个名词都有厂商在用，指的是相同的东西。</p>
<p>  IDE的规格后来有所进步，而推出了EIDE（Enhanced IDE）的规格名称，而这个规格同时又被称为Fast ATA。所不同的是Fast ATA是专指硬盘接口，而EIDE还制定了连接光盘等非硬盘产品的标准。而这个连接非硬盘类的IDE标准，又称为ATAPI接口。而之后再推出更快的接口，名称都只剩下ATA的字样，像是Ultra ATA、ATA/66、ATA/100等。</p>
</li>
<li><p>SATA（串口）</p>
<p>  SATA（Serial ATA）口的硬盘又叫串口硬盘。2001年，由Intel、APT、Dell、IBM、希捷、迈拓这几大厂商组成的Serial ATA委员会正式确立了Serial ATA 1.0规范。</p>
<p>  SATA接口需要硬件芯片的支持，例如Intel ICH5(R)、VIA VT8237、nVIDIA的MCP RAID和SiS964，如果主板南桥芯片不能直接支持的话，就需要选择第三方的芯片，例如Silicon Image 3112A芯片等，不过这样也就会产生一些硬件性能的差异，并且驱动程序也比较繁杂。</p>
</li>
</ul>
<p>　　SATA的优势：支持热插拔，传输速度快，执行效率高。</p>
<ul>
<li><p>SCSI（小型计算机系统专用接口）</p>
<p>  SCSI的英文全称为“Small Computer System Interface”（小型计算机系统接口），是同IDE（ATA）完全不同的接口，IDE接口是普通PC的标准接口，而SCSI并不是专门为硬盘设计的接口，是一种广泛应用于小型机上的高速数据传输技术。SCSI接口具有应用范围广、多任务、带宽大、CPU占用率低，以及热插拔等优点，但较高的价格使得它很难如IDE硬盘般普及，因此SCSI硬盘主要应用于中、高端服务器和高档工作站中。</p>
</li>
<li><p>SAS（就是串口的SCSI接口）</p>
<p>  SAS(Serial Attached SCSI)即串行连接SCSI，是新一代的SCSI技术。和现在流行的Serial ATA(SATA)硬盘相同，都是采用串行技术以获得更高的传输速度，并通过缩短连结线改善内部空间等。SAS是并行SCSI接口之后开发出的全新接口。此接口的设计是为了改善存储系统的效能、可用性和扩充性，并且提供与SATA硬盘的兼容性。</p>
</li>
<li><p>FC（光纤通道）</p>
<p>  光纤通道的英文拼写是Fiber Channel，和SCIS接口一样光纤通道最初也不是为硬盘设计开发的接口技术，是专门为网络系统设计的，但随着存储系统对速度的需求，才逐渐应用到硬盘系统中。光纤通道硬盘是为提高多硬盘存储系统的速度和灵活性才开发的，它的出现大大提高了多硬盘系统的通信速度。光纤通道的主要特性有：热插拔性、高速带宽、远程连接、连接设备数量大等。</p>
</li>
<li><p>SSD（固态硬盘）</p>
<p>  固态硬盘（Solid State Disk或Solid State Drive），也称作电子硬盘或者固态电子盘，是由控制单元和固态存储单元（DRAM或FLASH芯片）组成的硬盘。固态硬盘的接口规范和定义、功能及使用方法上与普通硬盘的相同，在产品外形和尺寸上也与普通硬盘一致。由于固态硬盘没有普通硬盘的旋转介质，因而抗震性极佳。其芯片的工作温度范围很宽（-40~85℃）。</p>
<p>  由于固态硬盘技术与传统硬盘技术不同，所以产生了不少新兴的存储器厂商。厂商只需购买NAND存储器，再配合适当的控制芯片，就可以制造固态硬盘了。新一代的固态硬盘普遍采用SATA-2接口。</p>
</li>
</ul>
<h3 id="查看磁盘"><a href="#查看磁盘" class="headerlink" title="查看磁盘"></a>查看磁盘</h3><pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# fdisk -l /dev/sda </span><br><span class="line"></span><br><span class="line">Disk /dev/sda: 4000.8 GB, 4000787030016 bytes</span><br><span class="line">255 heads, 63 sectors/track, 486401 cylinders</span><br><span class="line">Units = cylinders of 16065 * 512 = 8225280 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br><span class="line">Disk identifier: 0x00000000</span><br><span class="line"></span><br><span class="line">[root@yadoom ~]# lspci</span><br><span class="line">......</span><br><span class="line">00:1f.0 ISA bridge: Intel Corporation C600/X79 series chipset LPC Controller (rev 05)</span><br><span class="line">00:1f.2 SATA controller: Intel Corporation C600/X79 series chipset 6-Port SATA AHCI Controller (rev 05)</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">[root@yadoom ~]# hdparm -I /dev/sda</span><br><span class="line"></span><br><span class="line">/dev/sda:</span><br><span class="line"></span><br><span class="line">ATA device, with non-removable media</span><br><span class="line">	Model Number:       ST1000NM0033-9ZM173       [root@yadoom ~]# 希捷1T</span><br><span class="line">	Serial Number:      Z1W3VQXC</span><br><span class="line">	Firmware Revision:  GA0A</span><br><span class="line">	Transport:          Serial, SATA Rev 3.0</span><br><span class="line">Standards:</span><br><span class="line">	Supported: 9 8 7 6 5 </span><br><span class="line">	Likely used: 9</span><br><span class="line">......</span><br><span class="line">[root@yadoom ~]# dmesg |grep -C5 SATA</span><br></pre></td></tr></table></figure>
</code></pre><h2 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h2><ul>
<li>Electronic disk, no moving mechanical components</li>
<li>No startup time</li>
<li>Very low latency</li>
<li>Potentially longer life time（尽可能延长寿命）<ul>
<li>Wear leveling（磨损平衡）:闪存寿命是以P/E（完全擦写）次数来计算的，而WL就是确保闪存内每个块被写入的次数相等的一种机制。</li>
</ul>
</li>
<li>Parameter<ul>
<li>TBW<ul>
<li>在 SSD 使用寿命结束之前指定工作量可以写入 SSD 的总数据量。</li>
</ul>
</li>
<li>DWPD<ul>
<li>在保固期内（或不同的数年时段内）每天可以写入硬盘用户存储容量的次数。</li>
<li>DWPD = (固态硬盘的 TBW (TB) <em> P/E) / (365 天 </em> 年数 * 固态硬盘用户容量 (GB))</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="SSD-Types"><a href="#SSD-Types" class="headerlink" title="SSD Types"></a>SSD Types</h3><p>固态硬盘就是靠NAND Flash闪存芯片存储数据的，这点类似于我们常见的U盘。NAND Flash根据存储原理分为三种，SLC、MLC、TLC。</p>
<ul>
<li><p>SLC</p>
<p>  Single-Level Cell ，即1bit/cell（1个存储器储存单元可存放1 bit的数据），速度快寿命长，价格超贵（约MLC 3倍以上的价格），约10万次擦写寿命</p>
</li>
<li><p>MLC</p>
<p>  Multi-Level Cell，即2bit/cell，速度一般寿命一般，价格一般，约1000–3000次擦写寿命</p>
</li>
<li><p>TLC</p>
<p>  Trinary-Level Cell，即3bit/cell，也有Flash厂家叫8LC，速度慢寿命短，价格便宜，约1000次擦写寿命。<br>  单位容量的存储器，可以存储更多的数据，所以TLC每百万字节生产成本是最低的。</p>
</li>
</ul>
<p>实战：计算256G的TLC固态硬盘的使用寿命。</p>
<p>假设该硬盘每天读取100G数据，256G*1000/356/100G=7.19（年） </p>
<h3 id="SSD-Garbage-Collection"><a href="#SSD-Garbage-Collection" class="headerlink" title="SSD Garbage Collection"></a>SSD Garbage Collection</h3><p><img src="/images/linux/performance-tuning/ssd_garbage_collection.png" alt="ssd-garbage-collection"></p>
<ol>
<li>上图SSD中有两个空的（erased）的Block X和Block Y, 每个Block有12个Pages;</li>
<li>首先在Block X中写入4个Pages(A, B, C, D);</li>
<li>接着再向Block X中写入新的4个pages(E, F, G, H), 同时写入PageA-D的更新数据（A’, B’, C’, D’), 这时PageA-D变为失效数据（invalid）;</li>
<li>为了向PageA-D的位置写入数据，需要将E, F, G, H, A’, B’, C’, D’ 8个pages先搬到Block Y中, 之后再把Block X erase掉，这个过程就为GC。</li>
</ol>
<p>Nand flash 以Page为单位读写数据，而以Block为单位擦除数据。</p>
<p>不过，由于GC的过程增加了数据的读写过程，势必会对SSD的performance的产生一定的影响，所以GC发生的条件与触发点很关键。</p>
<p>GC触发条件大致有3点：</p>
<ol>
<li>Spare Block（）备用块太少</li>
<li>Wear leveling</li>
<li>处理ECC错误Block</li>
</ol>
<h3 id="SSD-Trim"><a href="#SSD-Trim" class="headerlink" title="SSD Trim"></a>SSD Trim</h3><p>操作系统删除数据时，Windows只会做个标记，说明这里已经没东西了，等到真正要写入数据时再来真正删除，并且做标记这个动作会保留在磁盘缓存中，等到磁盘空闲时再执行；Linux只会把inode table回收。</p>
<p>所以对于非空的page，SSD在写入前必须先进行一次Erase，则写入过程为read-erase-modify-write:将整个block的内容读取到cache中，整个block从SSD中Erase,要覆写的page写入到cache的block中，将cache中更新的block写入闪存介质，这个现象称之为写入放大(write amplification)。</p>
<p>为了解决这个问题，SSD开始支持TRIM，TRIM功能使操作系统得以通知SSD哪些页不再包含有效的数据。</p>
<p>当Windows识别到SSD并确认SSD支持Trim后，在删除数据时，会不向硬盘通知删除指令，只使用Volume Bitmap来记住这里的数据已经删除。Volume Bitmap只是一个磁盘快照，其建立速度比直接读写硬盘去标记删除区域要快得多。这一步就已经省下一大笔时间了。然后再是写入数据的时候，由于NAND闪存保存数据是纯粹的数字形式，因此可以直接根据Volume Bitmap的情况，向快照中已删除的区块写入新的数据，而不用花时间去擦除原本的数据。</p>
<ul>
<li><p>Linux启用Trim</p>
<ol>
<li><p>确认 SSD 、操作系统、文件系统都支持 TRIM</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># discard_granularity 非 0 表示支持</span><br><span class="line">[root@yadoom ~]# cat /sys/block/sda/queue/discard_granularity</span><br><span class="line">0</span><br><span class="line">[root@yadoom ~]# cat /sys/block/nvme0n1/queue/discard_granularity</span><br><span class="line">512</span><br><span class="line"></span><br><span class="line"># DISC-GRAN (discard granularity) 和 DISC-MAX (discard max bytes) 列非 0 表示该 SSD 支持 TRIM 功能。</span><br><span class="line">[root@yadoom ~]# lsblk --discard</span><br><span class="line">NAME    DISC-ALN DISC-GRAN DISC-MAX DISC-ZERO</span><br><span class="line">sda            0        0B       0B         0</span><br><span class="line">├─sda1         0        0B       0B         0</span><br><span class="line">├─sda2         0        0B       0B         0</span><br><span class="line">└─sda3         0        0B       0B         0</span><br><span class="line">sr0            0        0B       0B         0</span><br><span class="line">nvme0n1      512      512B       2T         1</span><br><span class="line">nvme1n1      512      512B       2T         1</span><br></pre></td></tr></table></figure>
</li>
<li><p>开启</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">对于 ext4 文件系统，可以在/etc/fstab里添加 discard 参数来启用 TRIM，添加前请确认你的 SSD 支持 TRIM。</span><br><span class="line">[root@yadoom ~]# vim /etc/fstab</span><br><span class="line">/dev/sdb1  /data1       ext4   defaults,noatime,discard   0  0</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>Windows启用Trim</p>
</li>
</ul>
<p>注意：如果SSD组RAID0后，将失去Trim功能。</p>
<h2 id="RAID"><a href="#RAID" class="headerlink" title="RAID"></a>RAID</h2><ul>
<li><p>striping（条带化）</p>
<p>  条带（strip）是把连续的数据分割成相同大小的数据块，把每段数据分别写入到阵列中的不同磁盘上的方法。简单的说，条带是一种将多个磁盘驱动器合并为一个卷的方法。 许多情况下，这是通过硬件控制器来完成的。</p>
</li>
<li><p>why striping?</p>
<p>  首先介绍什么是磁盘冲突。当多个进程同时访问一个磁盘时，磁盘的访问次数（每秒的 I/O 操作，IOPS）和数据传输率（每秒传输的数据量，TPS）达到极限后，后面的进程就需要等待，这时就是所谓的磁盘冲突。</p>
<p>  避免磁盘冲突是优化 I/O 性能的一个重要目标，而 I/O 性能的优化与其他资源（如CPU和内存）的优化有着很大的区别,I/O 优化最有效的手段是将 I/O 最大限度的进行平衡。</p>
<p>  条带化技术就是一种自动的将 I/O 的负载均衡到多个物理磁盘上的技术，条带化技术就是将一块连续的数据分成很多小部分并把他们分别存储到不同磁盘上去。这就能使多个进程同时访问数据的多个不同部分而不会造成磁盘冲突，而且在需要对这种数据进行顺序访问的时候可以获得最大程度上的 I/O 并行能力，从而获得非常好的性能。</p>
</li>
<li><p>stripe depth(stripe unit)</p>
<p>  条带深度：指的是条带的大小。这个参数指的是写在每块磁盘上的条带数据块的大小。RAID的数据块大小一般在2KB到512KB之间(或者更大)，其数值是 2 的次方，即2KB,4KB,8KB,16KB这样。</p>
<p>  条带大小对性能的影响比条带宽度难以量化的多：</p>
<ul>
<li>减小条带大小: 由于条带大小减小了，则文件被分成了更多个，更小的数据块。这些数据块会被分散到更多的硬盘上存储，因此提高了传输的性能，但是由于要多次寻找不同的数据块，磁盘定位的性能就下降了。</li>
<li><p>增加条带大小: 与减小条带大小相反，会降低传输性能，提高定位性能。</p>
<p>根据上边的论述，我们会发现根据不同的应用类型，不同的性能需求，不同驱动器的不同特点(如SSD硬盘)，不存在一个普遍适用的”最佳条带大小”。所以这也是存储厂家，文件系统编写者允许我们自己定义条带大小的原因。</p>
</li>
</ul>
</li>
<li><p>stripe width</p>
<p>  条带宽度：是指同时可以并发读或写的条带数量。这个数量等于RAID中的物理硬盘数量。例如一个经过条带化的，具有4块物理硬盘的阵列的条带宽度就是 4。增加条带宽度，可以增加阵列的读写性能。道理很明显，增加更多的硬盘，也就增加了可以同时并发读或写的条带数量。</p>
</li>
<li><p>stripe size</p>
<p>  有时也称block size块大小、chunk size簇大小、stripe length条带长度、granularity粒度，是单块磁盘上的每次I/O的最小单位。</p>
</li>
</ul>
<p>实战：<a href="http://www.mysqlab.net/blog/2011/12/raid10-stripe-size-for-mysql-innodb/" target="_blank" rel="noopener">Raid1+0 stripe size for MySQL InnoDB</a></p>
<h3 id="RAID卡"><a href="#RAID卡" class="headerlink" title="RAID卡"></a>RAID卡</h3><ul>
<li><p>写策略 write-through和write-back</p>
<ul>
<li><p>write-through</p>
<p>  数据在写入存储的同时，要写入缓存，这种方式安全但是会牺牲写性能，因为只有等数据完全落入硬盘后，才算是一次io完成，这个过程会造成cpu的iowait。</p>
</li>
<li><p>write-back</p>
<p>  数据直接写入缓存，写缓存的速度是远远大于写磁盘的，所以这种方式可以提高服务器的写性能。也许你会想当断电了怎么办？不用担心，raid卡是有电池的，完全可以支持缓存中的数据再写入磁盘。除非点背，raid卡电池也没电了。</p>
<p>  我们生产环境用的就是write-back，就是并且还设置了force write-back  (即使电池没电了，也要写缓存)，这样有了点冒险，但是大幅度的提高了写性能，我觉得利大于弊吧。</p>
</li>
</ul>
</li>
<li><p>读策略read policy</p>
<ul>
<li>No-Read-Ahead（非预读）</li>
<li>Read-ahead（预读）<br>  提前加载数据到缓存，加速顺序读请求。</li>
<li>Adaptive（自适应）<br>  如果最近两次的磁盘访问都落在了连续的扇区内那就采用Read-ahead,否则就采用No-Read-ahead。</li>
</ul>
</li>
<li><p>Disk cache Policy<br>  磁盘上的缓存，区别于raid卡的缓存，该项SAS和SATA一般都是关的,因为掉电了有可能会丢数据，ssd可以打开。</p>
</li>
</ul>
<h2 id="Networking-Profile"><a href="#Networking-Profile" class="headerlink" title="Networking Profile"></a>Networking Profile</h2><h3 id="查看网卡"><a href="#查看网卡" class="headerlink" title="查看网卡"></a>查看网卡</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# lspci | grep Ethernet</span><br><span class="line">01:00.0 Ethernet controller: Broadcom Corporation NetXtreme BCM5720 Gigabit Ethernet PCIe</span><br><span class="line">01:00.1 Ethernet controller: Broadcom Corporation NetXtreme BCM5720 Gigabit Ethernet PCIe</span><br><span class="line">02:00.0 Ethernet controller: Broadcom Corporation NetXtreme BCM5720 Gigabit Ethernet PCIe</span><br><span class="line">02:00.1 Ethernet controller: Broadcom Corporation NetXtreme BCM5720 Gigabit Ethernet PCIe</span><br><span class="line"># 示：四块博通千兆网卡</span><br><span class="line"></span><br><span class="line">[root@yadoom ~]# ethtool bond0</span><br><span class="line">Settings for bond0:</span><br><span class="line">        Supported ports: [ ]</span><br><span class="line">        Supported link modes:   Not reported</span><br><span class="line">        Supported pause frame use: No</span><br><span class="line">        Supports auto-negotiation: No</span><br><span class="line">        Advertised link modes:  Not reported</span><br><span class="line">        Advertised pause frame use: No</span><br><span class="line">        Advertised auto-negotiation: No</span><br><span class="line">        Speed: 1000Mb/s</span><br><span class="line">        Duplex: Full         # 当前工作在全双工模式</span><br><span class="line">        Port: Other</span><br><span class="line">        PHYAD: 0</span><br><span class="line">        Transceiver: internal</span><br><span class="line">        Auto-negotiation: off</span><br><span class="line">        Link detected: yes</span><br><span class="line"></span><br><span class="line">[root@yadoom ~]# ip a s bond0</span><br><span class="line">6: bond0: &lt;BROADCAST,MULTICAST,MASTER,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP </span><br><span class="line">    link/ether f8:bc:12:48:91:64 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet6 fe80::fabc:12ff:fe48:9164/64 scope link </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>
<h2 id="主板"><a href="#主板" class="headerlink" title="主板"></a>主板</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# dmidecode -t baseboard</span><br></pre></td></tr></table></figure>
<h2 id="PCI设备"><a href="#PCI设备" class="headerlink" title="PCI设备"></a>PCI设备</h2><p>PCI是CPU和外围设备通信的高速传输总线。</p>
<p><img src="/images/linux/performance-tuning/pci_bandwidth.png" alt="pci-bandwidth"></p>
<h3 id="查看pci设备"><a href="#查看pci设备" class="headerlink" title="查看pci设备"></a>查看pci设备</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# lspci            # pciutils软件包</span><br><span class="line">7f:08.0 System peripheral: Intel Corporation Xeon E5 v2/Core i7 QPI Link 0 (rev 04)</span><br><span class="line">......</span><br><span class="line">7f:0c.1 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Unicast Registers (rev 04)</span><br><span class="line">......</span><br><span class="line">7f:16.2 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Broadcast Registers (rev 04)</span><br><span class="line"></span><br><span class="line">[root@yadoom ~]# lspci -vt</span><br><span class="line">-+-[0000:7f]-+-08.0  Intel Corporation Xeon E5 v2/Core i7 QPI Link 0</span><br><span class="line"> |           +-09.0  Intel Corporation Xeon E5 v2/Core i7 QPI Link 1</span><br><span class="line">......</span><br><span class="line"> |           +-0f.0  Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 0 Target Address/Thermal Registers</span><br><span class="line">......</span><br><span class="line"> |           +-10.7  Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 1 Channel 0-3 ERROR Registers 3</span><br><span class="line"> |           +-13.0  Intel Corporation Xeon E5 v2/Core i7 R2PCIe</span><br><span class="line"> |           +-13.1  Intel Corporation Xeon E5 v2/Core i7 R2PCIe</span><br><span class="line"> |           +-13.4  Intel Corporation Xeon E5 v2/Core i7 QPI Ring Registers</span><br><span class="line"> |           +-13.5  Intel Corporation Xeon E5 v2/Core i7 QPI Ring Performance Ring Monitoring</span><br><span class="line"> |           +-16.0  Intel Corporation Xeon E5 v2/Core i7 System Address Decoder</span><br><span class="line"> |           +-16.1  Intel Corporation Xeon E5 v2/Core i7 Broadcast Registers</span><br><span class="line"> |           \-16.2  Intel Corporation Xeon E5 v2/Core i7 Broadcast Registers</span><br><span class="line"> \-[0000:00]-+-00.0  Intel Corporation Xeon E5 v2/Core i7 DMI2</span><br><span class="line">             +-01.0-[02]--+-00.0  Broadcom Corporation NetXtreme BCM5720 Gigabit Ethernet PCIe</span><br><span class="line">             |            \-00.1  Broadcom Corporation NetXtreme BCM5720 Gigabit Ethernet PCIe</span><br><span class="line">             +-01.1-[01]--+-00.0  Broadcom Corporation NetXtreme BCM5720 Gigabit Ethernet PCIe</span><br><span class="line">             |            \-00.1  Broadcom Corporation NetXtreme BCM5720 Gigabit Ethernet PCIe</span><br><span class="line">             +-02.0-[04]--</span><br><span class="line">             +-02.2-[03]----00.0  LSI Logic / Symbios Logic MegaRAID SAS 2008 [Falcon]</span><br><span class="line">             +-03.0-[05]--</span><br><span class="line">             +-03.2-[06]--</span><br><span class="line">             +-05.0  Intel Corporation Xeon E5 v2/Core i7 VTd/Memory Map/Misc</span><br><span class="line">             +-05.2  Intel Corporation Xeon E5 v2/Core i7 IIO RAS</span><br><span class="line">             +-11.0-[07]--</span><br><span class="line">             +-16.0  Intel Corporation C600/X79 series chipset MEI Controller #1</span><br><span class="line">             +-16.1  Intel Corporation C600/X79 series chipset MEI Controller #2</span><br><span class="line">             +-1a.0  Intel Corporation C600/X79 series chipset USB2 Enhanced Host Controller #2</span><br><span class="line">             +-1c.0-[08]--</span><br><span class="line">             +-1c.7-[09-0d]----00.0-[0a-0d]--+-00.0-[0b-0c]----00.0-[0c]----00.0  Matrox Electronics Systems Ltd. G200eR2</span><br><span class="line">             |                               \-01.0-[0d]--</span><br><span class="line">             +-1d.0  Intel Corporation C600/X79 series chipset USB2 Enhanced Host Controller #1</span><br><span class="line">             +-1e.0-[0e]--</span><br><span class="line">             +-1f.0  Intel Corporation C600/X79 series chipset LPC Controller</span><br><span class="line">             \-1f.2  Intel Corporation C600/X79 series chipset 6-Port SATA AHCI Controller</span><br><span class="line"></span><br><span class="line">[root@yadoom ~]# lspci -xxx -s 7f:13.5        # x越多，列出的信息越详细</span><br><span class="line">7f:13.5 Performance counters: Intel Corporation Xeon E5 v2/Core i7 QPI Ring Performance Ring Monitoring (rev 04)</span><br><span class="line">00: 86 80 36 0e 00 00 00 00 04 00 01 11 10 00 80 00</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">[root@yadoom ~]# lspci -vv -s 7f:13.5         # v越多，列出的信息越详细</span><br><span class="line">7f:13.5 Performance counters: Intel Corporation Xeon E5 v2/Core i7 QPI Ring Performance Ring Monitoring (rev 04)</span><br><span class="line">        Subsystem: Dell Device 048c</span><br><span class="line">        Control: I/O- Mem- BusMaster- SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-</span><br><span class="line">        Status: Cap- 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast &gt;TAbort- &lt;TAbort- &lt;MAbort- &gt;SERR- &lt;PERR- INTx-</span><br></pre></td></tr></table></figure>
<h3 id="USB设备"><a href="#USB设备" class="headerlink" title="USB设备"></a>USB设备</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# lsusb           # usbutils包</span><br><span class="line">Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub</span><br><span class="line">Bus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub</span><br><span class="line">Bus 001 Device 002: ID 8087:0024 Intel Corp. Integrated Rate Matching Hub</span><br><span class="line">Bus 002 Device 002: ID 8087:0024 Intel Corp. Integrated Rate Matching Hub</span><br><span class="line">Bus 001 Device 003: ID 0624:0248 Avocent Corp. Virtual Hub</span><br><span class="line">Bus 001 Device 004: ID 0624:0249 Avocent Corp. Virtual Keyboard/Mouse</span><br><span class="line"></span><br><span class="line">[root@yadoom ~]# lsusb -vt</span><br><span class="line">Bus#  2</span><br><span class="line">`-Dev#   1 Vendor 0x1d6b Product 0x0002</span><br><span class="line">  `-Dev#   2 Vendor 0x8087 Product 0x0024</span><br><span class="line">Bus#  1</span><br><span class="line">`-Dev#   1 Vendor 0x1d6b Product 0x0002</span><br><span class="line">  `-Dev#   2 Vendor 0x8087 Product 0x0024</span><br><span class="line">    `-Dev#   3 Vendor 0x0624 Product 0x0248</span><br><span class="line">      `-Dev#   4 Vendor 0x0624 Product 0x0249</span><br></pre></td></tr></table></figure>
<h3 id="查看内核产生的硬件日志"><a href="#查看内核产生的硬件日志" class="headerlink" title="查看内核产生的硬件日志"></a>查看内核产生的硬件日志</h3><ul>
<li>/var/log/dmesg<br> 系统启动，一次性将启动时关于硬件的kernel日志写入该文件。</li>
<li>dmesg命名<br>  实时记录kernel日志，譬如插入USB设备，可使用dmesg命令查看。</li>
</ul>
<h3 id="获取硬件命令汇总"><a href="#获取硬件命令汇总" class="headerlink" title="获取硬件命令汇总"></a>获取硬件命令汇总</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">1. CPU型号</span><br><span class="line">dmidecode -t 4| awk &apos;/Version/ &amp;&amp; /CPU/&apos;|uniq</span><br><span class="line">2. CPU核数</span><br><span class="line">grep -c &apos;processor&apos; /proc/cpuinfo</span><br><span class="line">sysctl kern.smp.cpus|awk &apos;&#123;print $2&#125;&apos;</span><br><span class="line">3.内存大小</span><br><span class="line">free -m|awk &apos;/Mem/&#123;printf(&quot;%d\n&quot;, $2/1024+0.5)&#125;&apos;</span><br><span class="line">sysctl hw.physmem|awk &apos;&#123;printf (&quot;%d\n&quot;, $2/1073741824+0.5)&#125;&apos;</span><br><span class="line">4.硬盘总大小</span><br><span class="line">fdisk -l|awk &apos;/Disk \/dev\//&#123;sum +=$3&#125;END&#123;printf(&quot;%d\n&quot;, sum+0.5)&#125;&apos;</span><br><span class="line">sysctl kern.geom.conftxt|grep -Eo &apos;DISK \w+ \w+&apos;|awk &apos;&#123;sum +=$3&#125;END&#123;printf(&quot;%d\n&quot;, sum/1000000000+0.5)&#125;&apos; </span><br><span class="line">df -Pm | awk &apos;/^\/dev\//&#123;sum +=$2&#125;END&#123;printf(&quot;%d\n&quot;, sum/1024+0.5)&#125;&apos;</span><br><span class="line">5.主板型号</span><br><span class="line">dmidecode -t 1| awk &apos;/Product Name/&apos;</span><br><span class="line">6.取主板Serial</span><br><span class="line">dmidecode -t 1|awk &apos;/Serial/&apos;</span><br><span class="line"></span><br><span class="line">一句话脚本：</span><br><span class="line">dmidecode -t 4| awk &apos;/Version:/&apos;|tail -n1</span><br><span class="line"></span><br><span class="line">if [ `uname` == Linux ];then grep -c &apos;processor&apos; /proc/cpuinfo;else sysctl kern.smp.cpus|awk &apos;&#123;print $2&#125;&apos;;fi</span><br><span class="line"></span><br><span class="line">if [ `uname` == Linux ];then free -m|awk &apos;/Mem/&#123;printf(&quot;%d\n&quot;, $2/1000+0.5)&#125;&apos;;else sysctl hw.physmem|awk &apos;&#123;printf (&quot;%d\n&quot;, $2/1073741824+0.5)&#125;&apos;;fi</span><br><span class="line"></span><br><span class="line">if [ `uname` == Linux ];then fdisk -l|awk &apos;/Disk \/dev\//&#123;sum +=$3&#125;END&#123;printf(&quot;%d\n&quot;, sum+0.5)&#125;&apos;;else sysctl kern.geom.conftxt|egrep -o &apos;DISK \w+ \w+&apos;|awk &apos;&#123;sum +=$3&#125;END&#123;printf(&quot;%d\n&quot;, sum/1000000000+0.5)&#125;&apos;;fi</span><br><span class="line"></span><br><span class="line">dmidecode -t 1| awk &apos;/Product Name/&apos;</span><br></pre></td></tr></table></figure>
<h1 id="System-Tuning"><a href="#System-Tuning" class="headerlink" title="System Tuning"></a>System Tuning</h1><h2 id="Stoarge"><a href="#Stoarge" class="headerlink" title="Stoarge"></a>Stoarge</h2><p>Storage is very slow compare to memory. Linux has two way to compensate the issue.</p>
<ul>
<li>Caching<ul>
<li>Read from memory, write to memory</li>
<li>Read can be cached, write can be deferred</li>
</ul>
</li>
<li>I/O Schedulers<ul>
<li>Kernel attempt to recorder, coalesce I/O requests（临近扇区，合并读请求）</li>
<li>Minimize relocating magnetic head（最小化磁头的动作，重新定位）</li>
</ul>
</li>
</ul>
<h2 id="I-O-Tuning"><a href="#I-O-Tuning" class="headerlink" title="I/O Tuning"></a>I/O Tuning</h2><p>参考：<a href="http://www.cnblogs.com/cobbliu/p/5389556.html" target="_blank" rel="noopener">http://www.cnblogs.com/cobbliu/p/5389556.html</a></p>
<ul>
<li><p>cfq(Complete Fair Queuing)</p>
<ul>
<li><p>default schduler after kernel 2.6.18</p>
<p>它试图为竞争块设备使用权的所有进程分配一个请求队列和一个时间片，在调度器分配给进程的时间片内，进程可以将其读写请求发送给底层块设备，当进程的时间片消耗完，进程的请求队列将被挂起，等待调度。 </p>
<p>每个进程的时间片和每个进程的队列长度取决于进程的IO优先级，每个进程都会有一个IO优先级，CFQ调度器将会将其作为考虑的因素之一，来确定该进程的请求队列何时可以获取块设备的使用权。IO优先级从高到低可以分为三大类:RT(real time),BE(best try),IDLE(idle),其中RT和BE又可以再划分为8个子优先级。</p>
<p>实际上，我们已经知道CFQ调度器的公平是针对于进程而言的，而只有同步请求(read或syn write)才是针对进程而存在的，他们会放入进程自身的请求队列，而所有同优先级的异步请求，无论来自于哪个进程，都会被放入公共的队列，异步请求的队列总共有8(RT)+8(BE)+1(IDLE)=17个。</p>
</li>
<li><p>IO Priority</p>
<ul>
<li>Class 1(real time): first-access to disk, can starve（饿死） other classes<ul>
<li>0-7: The scheduling class data</li>
</ul>
</li>
<li>Class 2(best-effort): round-robin access, the default<ul>
<li>0-7: The scheduling class data</li>
</ul>
</li>
<li>Class 3(idle): receives disk I/O only if no other requests in queue</li>
</ul>
<p><strong>ionice命令可调节进程的IO优先级</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ionice -n0 -c1 -p pid</span><br><span class="line">ionice -n7 -c2 -p pid</span><br><span class="line">ionice -c3 -p pid        # 我不入地狱，谁入地狱</span><br><span class="line">ionice -c 2 -n 0 bash  # Runs ’bash’ as a best-effort program with highest priority.</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>deadline</p>
<ul>
<li>with predictable service time（可预见的服务时间）</li>
<li><p>for virtualization host</p>
<p>Deadline算法中引入了四个队列，这四个队列可以分为两类，每一类都由读和写两类队列组成，一类队列用来对请求按起始扇区序号进行排序，通过红黑树来组织，称为sort_list；另一类对请求按它们的生成时间进行排序，由链表来组织，称为fifo_list。每当确定了一个传输方向(读或写)，那么将会从相应的sort_list中将一批连续请求dispatch到requst_queue的请求队列里，具体的数目由fifo_batch来确定。只有下面三种情况才会导致一次批量传输的结束：</p>
</li>
</ul>
<ol>
<li>对应的sort_list中已经没有请求了</li>
<li>下一个请求的扇区不满足递增的要求</li>
<li><p>上一个请求已经是批量传输的最后一个请求了</p>
<p>所有的请求在生成时都会被赋上一个期限值(根据jiffies)，并按期限值排序在fifo_list中，读请求的期限时长默认为为500ms，写请求的期限时长默认为5s，可以看出内核对读请求是十分偏心的，其实不仅如此，在deadline调度器中，还定义了一个starved和writes_starved，writes_starved默认为2，可以理解为写请求的饥饿线，内核总是优先处理读请求，starved表明当前处理的读请求批数，只有starved超过了writes_starved后，才会去考虑写请求。因此，假如一个写请求的期限已经超过，该请求也不一定会被立刻响应，因为读请求的batch还没处理完，即使处理完，也必须等到starved超过writes_starved才有机会被响应。为什么内核会偏袒读请求？这是从整体性能上进行考虑的。读请求和应用程序的关系是同步的，因为应用程序要等待读取的内容完毕，才能进行下一步工作，因此读请求会阻塞进程，而写请求则不一样，应用程序发出写请求后，内存的内容何时写入块设备对程序的影响并不大，所以调度器会优先处理读请求。</p>
<p>默认情况下，读请求的超时时间是500ms，写请求的超时时间是5s。</p>
<p><a href="http://www.ibm.com/support/knowledgecenter/api/content/linuxonibm/liaat/liaatbestpractices_pdf.pdf" target="_blank" rel="noopener">这篇文章</a>说在一些多线程应用下，Deadline算法比CFQ算法好。<a href="https://www.percona.com/blog/2009/01/30/linux-schedulers-in-tpcc-like-benchmark/" target="_blank" rel="noopener">这篇文章</a>说在一些数据库应用下，Deadline算法比CFQ算法好。</p>
</li>
</ol>
</li>
<li><p>anticipatory(AS)</p>
<ul>
<li>wait for a while after read request</li>
<li><p>for sequential read workloads（大量顺序读的）</p>
<p>Anticipatory算法从Linux 2.6.33版本后，就被移除了，因为CFQ通过配置也能达到Anticipatory算法的效果。</p>
</li>
</ul>
</li>
<li><p>noop(No Operation)</p>
<ul>
<li>quick to response, low CPU overhead</li>
<li><p>for SSD,virtualization guests（宿主机使用了deadline，则虚拟机使用noop，因为真正写盘操作是主机完成）</p>
<p>Noop调度算法也叫作电梯调度算法，它将IO请求放入到一个FIFO队列中，然后逐个执行这些IO请求，当然对于一些在磁盘上连续的IO请求，Noop算法会适当做一些合并。这个调度算法特别适合那些不希望调度器重新组织IO请求顺序的应用。</p>
<p>这种调度算法在以下场景中优势比较明显：</p>
</li>
</ul>
<ol>
<li><p>在IO调度器下方有更加智能的IO调度设备。如果您的Block Device Drivers是Raid，或者SAN，NAS等存储设备，这些设备会更好地组织IO请求，不用IO调度器去做额外的调度工作；</p>
<ol start="2">
<li><p>上层的应用程序比IO调度器更懂底层设备。或者说上层应用程序到达IO调度器的IO请求已经是它经过精心优化的，那么IO调度器就不需要画蛇添足，只需要按序执行上层传达下来的IO请求即可。</p>
</li>
<li><p>对于一些非旋转磁头氏的存储设备，使用Noop的效果更好。因为对于旋转磁头式的磁盘来说，IO调度器的请求重组要花费一定的CPU时间，但是对于SSD磁盘来说，这些重组IO请求的CPU时间可以节省下来，因为SSD提供了更智能的请求调度算法，不需要内核去画蛇添足。</p>
</li>
</ol>
</li>
</ol>
</li>
</ul>
<h3 id="Queue"><a href="#Queue" class="headerlink" title="Queue"></a>Queue</h3><ul>
<li><p>/sys/block/\&lt;device>/queue/nr_requests</p>
<p>  磁盘请求队列长度（一次性交给磁盘的请求数量）。增大它会牺牲更多内存。 </p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# cat /sys/block/sda/queue/nr_requests </span><br><span class="line">128</span><br></pre></td></tr></table></figure>
</li>
<li><p>/sys/block/\&lt;device>/queue/read_ahead_kb</p>
<p>  预先读数据块大小，对于大量的连续读业务，可以增大它。</p>
</li>
</ul>
<h3 id="I-O-Scheduler-Manage"><a href="#I-O-Scheduler-Manage" class="headerlink" title="I/O Scheduler Manage"></a>I/O Scheduler Manage</h3><ul>
<li><p>/sys/block/\&lt;device>/queue/scheduler<br>  切换I/O调度算法。</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# cat /sys/block/sda/queue/scheduler </span><br><span class="line">noop anticipatory deadline [cfq]</span><br></pre></td></tr></table></figure>
</li>
<li><p>每种调度算法的可调参数</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 该目录会根据不同Schduler而变化</span><br><span class="line">[root@yadoom ~]# cd /sys/block/sda/queue/iosched/</span><br><span class="line">[root@yadoom iosched]# ls</span><br><span class="line">back_seek_max      fifo_expire_async  group_idle       low_latency  slice_async     slice_idle</span><br><span class="line">back_seek_penalty  fifo_expire_sync   group_isolation  quantum      slice_async_rq  slice_sync</span><br></pre></td></tr></table></figure>
</li>
<li><p>CFQ</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">/sys/block/&lt;device&gt;/queue/iosched/slice_idle </span><br><span class="line">当一个进程的队列被分配到时间片却没有 IO 请求时，调度器在轮询至下一个队列之前的等待时间，以提升 IO 的局部性，对于 SSD 设备，可以将这个值设为 0。</span><br><span class="line"></span><br><span class="line">/sys/block/&lt;device&gt;/queue/iosched/quantum </span><br><span class="line">一个进程的队列每次被处理 IO 请求的最大数量，默认为 4，RHEL6 为 8，增大这个值可以提升并行处理 IO 的性能，但可能会造成某些 IO 延迟问题。</span><br><span class="line"></span><br><span class="line">/sys/block/&lt;device&gt;/queue/iosched/slice_async_rq </span><br><span class="line">一次处理写请求的最大数</span><br><span class="line"></span><br><span class="line">/sys/block/&lt;device&gt;/queue/iosched/low_latency </span><br><span class="line">如果IO延迟的问题很严重，将这个值设为 1</span><br></pre></td></tr></table></figure>
</li>
<li><p>Deadline</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">/sys/block/&lt;device&gt;/queue/iosched/writes_starved </span><br><span class="line">进行一个写操作之前，允许进行多少次读操作</span><br><span class="line"></span><br><span class="line">/sys/block/&lt;device&gt;/queue/iosched/read_expire </span><br><span class="line">读请求的过期时间，默认为 5ms</span><br><span class="line"></span><br><span class="line">/sys/block/&lt;device&gt;/queue/iosched/write_expire </span><br><span class="line">写请求的过期时间，默认为 500ms</span><br><span class="line"></span><br><span class="line">/sys/block/sda/queue/iosched/front_merges </span><br><span class="line">是否进行前合并</span><br></pre></td></tr></table></figure>
</li>
<li><p>Anticipatory</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">/sys/block/&lt;device&gt;/queue/iosched/antic_expire </span><br><span class="line">预测等待时长，默认为 6ms</span><br><span class="line"></span><br><span class="line">/sys/block/&lt;device&gt;/queue/iosched/&#123;write_expire,read_expire&#125; </span><br><span class="line">读写请求的超时时长</span><br><span class="line"></span><br><span class="line">/sys/block/&lt;device&gt;/queue/iosched/&#123;write_batch_expire,read_batch_expire&#125; </span><br><span class="line">读写的批量处理时长</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Tuning-Theory"><a href="#Tuning-Theory" class="headerlink" title="Tuning Theory"></a>Tuning Theory</h3><ul>
<li>L: Queue length: average number of requests waiting in the system</li>
<li>A: Arrival rate: the rate at which requests enter a system</li>
<li>W: Wait time: average time to satisfy a request<ul>
<li>also known as wall clock,latency,response time, or residence time</li>
</ul>
</li>
</ul>
<p>L = A * W</p>
<h3 id="Queue-Length"><a href="#Queue-Length" class="headerlink" title="Queue Length"></a>Queue Length</h3><ul>
<li>Requests are buffered in memory</li>
<li>L may be read-write tunable or a read-only measurement</li>
</ul>
<h3 id="Wait-Time"><a href="#Wait-Time" class="headerlink" title="Wait Time"></a>Wait Time</h3><ul>
<li>Includes<ul>
<li>Queue time（排队时间）</li>
<li>Service time（服务时间）</li>
</ul>
</li>
<li>Tactics（策略）<ul>
<li>Reduce queue time</li>
<li>Reduce service time</li>
</ul>
</li>
</ul>
<p>L = A <em> W = A </em> (T<sub>q</sub> + T<sub>s</sub>)</p>
<h3 id="Service-Time"><a href="#Service-Time" class="headerlink" title="Service Time"></a>Service Time</h3><ul>
<li>Includes<ul>
<li>Sysem time: time in kernel mode</li>
<li>User time: time in user mode(doing useful work)</li>
</ul>
</li>
<li>Tactics<ul>
<li>Reduce system time(blocks user mode operations)</li>
<li>spend as much time as needed in user mode</li>
</ul>
</li>
</ul>
<p>L = A <em> W = A </em> (Q + S) = A * (T<sub>q</sub> + (T<sub>sys</sub> + T<sub>user</sub>))</p>
<h3 id="Summary-of-Queue-Theory"><a href="#Summary-of-Queue-Theory" class="headerlink" title="Summary of Queue Theory"></a>Summary of Queue Theory</h3><ul>
<li>L: Queue length</li>
<li>A: Arrival rate(requests/second)</li>
<li>W: Wait time(latency, time to satisfy a request)</li>
<li>Q: Queue time</li>
<li>S: Service time(includes system time, user time)</li>
<li><p>C: Complete rate(requests/second)</p>
<ul>
<li>Steady state: A = C</li>
<li>L = A <em> W = A </em> (Q + S) = A * (T<sub>q</sub> + (T<sub>sys</sub> + T<sub>user</sub>))</li>
</ul>
</li>
</ul>
<h3 id="Summary-of-strategies"><a href="#Summary-of-strategies" class="headerlink" title="Summary of strategies"></a>Summary of strategies</h3><ul>
<li>Tune L<ul>
<li>Constrain queue length</li>
<li>Sort the queue to prefer reads</li>
</ul>
</li>
<li>Tune A or C<ul>
<li>Reduce visit count by distributing across multiple resources(SMP,RAID)</li>
<li>Defer resource visits until think time(lazy write)</li>
<li>Improve throughput for resource(more efficient protocol, less overhead)</li>
</ul>
</li>
<li>Tune W<ul>
<li>Use expiration time for requests</li>
<li>use resources with smaller service time(in-memory cache vs disk)</li>
</ul>
</li>
</ul>
<h2 id="Kernel-Module"><a href="#Kernel-Module" class="headerlink" title="Kernel Module"></a>Kernel Module</h2><h3 id="Module-commands"><a href="#Module-commands" class="headerlink" title="Module commands"></a>Module commands</h3><ul>
<li><p>lsmod</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# lsmod </span><br><span class="line">Module                  Size  Used by</span><br><span class="line">tcp_diag                1041  0 </span><br><span class="line">inet_diag               8735  1 tcp_diag    # 表示该模块被tcp_diag依赖，使用次数为1</span><br><span class="line">ip6table_filter         2889  0 </span><br><span class="line">ip6_tables             18732  1 ip6table_filter</span><br><span class="line">ebtable_nat             2009  0 </span><br><span class="line">ebtables               18135  1 ebtable_nat</span><br><span class="line">ipt_MASQUERADE          2466  3 </span><br><span class="line">iptable_nat             6158  1 </span><br><span class="line">nf_nat                 22759  2 ipt_MASQUERADE,iptable_nat</span><br><span class="line">nf_conntrack_ipv4       9506  4 iptable_nat,nf_nat</span><br><span class="line">nf_defrag_ipv4          1483  1 nf_conntrack_ipv4</span><br><span class="line">xt_state                1492  1 </span><br><span class="line">nf_conntrack           79758  5 ipt_MASQUERADE,iptable_nat,nf_nat,nf_conntrack_ipv4,xt_state</span><br><span class="line">ipt_REJECT              2351  2</span><br></pre></td></tr></table></figure>
</li>
<li><p>/lib/modules/\&lt;kernel-release>/kernel/</p>
<p>  内核模块所在目录。</p>
</li>
<li><p>modinfo [ modulename… ]</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# modinfo sx8</span><br><span class="line">filename:       /lib/modules/2.6.32-431.el6.x86_64/kernel/drivers/block/sx8.ko</span><br><span class="line">version:        1.0</span><br><span class="line">description:    Promise SATA SX8 block driver</span><br><span class="line">license:        GPL</span><br><span class="line">author:         Jeff Garzik</span><br><span class="line">srcversion:     4772099AB984FE59198263E</span><br><span class="line">alias:          pci:v0000105Ad00008002sv*sd*bc*sc*i*</span><br><span class="line">alias:          pci:v0000105Ad00008000sv*sd*bc*sc*i*</span><br><span class="line">depends:        </span><br><span class="line">vermagic:       2.6.32-431.el6.x86_64 SMP mod_unload modversions </span><br><span class="line">parm:           max_queue:Maximum number of queued commands. (min==1, max==30, safe==1) (int)</span><br></pre></td></tr></table></figure>
</li>
<li><p>modprobe [ modulename… ]</p>
</li>
<li>rmmod [ modulename… ]</li>
</ul>
<h3 id="Modules-parameters"><a href="#Modules-parameters" class="headerlink" title="Modules parameters"></a>Modules parameters</h3><ul>
<li><p>查看某模块有哪些参数可调整</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# modinfo -p usb_storage</span><br><span class="line">quirks:supplemental list of device IDs and their quirks</span><br><span class="line">delay_use:seconds to delay before using a new device</span><br><span class="line">swi_tru_install:TRU-Install mode (1=Full Logic (def), 2=Force CD-Rom, 3=Force Modem)</span><br><span class="line">option_zero_cd:ZeroCD mode (1=Force Modem (default), 2=Allow CD-Rom</span><br><span class="line"></span><br><span class="line">[root@yadoom ~]# modinfo -p sx8</span><br><span class="line">max_queue:Maximum number of queued commands. (min==1, max==30, safe==1)</span><br></pre></td></tr></table></figure>
</li>
<li><p>自定义参数</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# cat /etc/modprobe.d/my.conf </span><br><span class="line">options usb_storage delay_use=3</span><br><span class="line">options st buffer_kbs=128</span><br><span class="line">options sx8 max_queue=10</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用modprobe命令重新加载这些模块，自定义的参数就会生效</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# modprobe usb_storage</span><br><span class="line">[root@yadoom ~]# modprobe st</span><br><span class="line">[root@yadoom ~]# modprobe sx8</span><br></pre></td></tr></table></figure>
</li>
<li><p>Check runtime module parameters</p>
<ul>
<li>/sys/module/<modulename>/parameters/<pname></pname></modulename></li>
</ul>
</li>
<li><p>Automatically loading modules</p>
<ul>
<li>/etc/sysconfig/modules/my.modules</li>
<li><p>Linux init脚本会执行以上目录下modules结尾的文件。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">modprobe usb_storage|st|sx8</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h2 id="Tuned"><a href="#Tuned" class="headerlink" title="Tuned"></a>Tuned</h2><ul>
<li>Tune a system on the fly as needed</li>
<li>Based on tuning profiles<ul>
<li>max power saving</li>
<li>max disk performance</li>
<li>self made profile allowed（允许自定义tune方案）</li>
<li>profile can even has monitoring program to run（还支持运行监控程序）</li>
</ul>
</li>
<li>SysV service<ul>
<li>tuned</li>
<li>ktune</li>
</ul>
</li>
<li>Can be used with crond to switch between profiles<ul>
<li>0 7 <em> </em> * 1-5 /usr/bin/tuned-adm profile throughput-performance</li>
<li>0 20 <em> </em> * 1-5 /usr/bin/tuned-adm profile server-powersave</li>
</ul>
</li>
</ul>
<h3 id="Use-Tuned"><a href="#Use-Tuned" class="headerlink" title="Use Tuned"></a>Use Tuned</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@cmdb-192-168-21-241 ~]# cat /etc/redhat-release </span><br><span class="line">CentOS Linux release 7.2.1511 (Core) </span><br><span class="line"></span><br><span class="line">[root@yadoom ~]# tuned-adm active</span><br><span class="line">Current active profile: virtual-guest</span><br><span class="line"></span><br><span class="line">[root@yadoom ~]# tuned-adm list</span><br><span class="line">Available profiles:</span><br><span class="line">- balanced                    - General non-specialized tuned profile</span><br><span class="line">- desktop                     - Optimize for the desktop use-case</span><br><span class="line">- latency-performance         - Optimize for deterministic performance at the cost of increased power consumption</span><br><span class="line">- network-latency             - Optimize for deterministic performance at the cost of increased power consumption, focused on low latency network performance</span><br><span class="line">- network-throughput          - Optimize for streaming network throughput, generally only necessary on older CPUs or 40G+ networks</span><br><span class="line">- powersave                   - Optimize for low power consumption</span><br><span class="line">- throughput-performance      - Broadly applicable tuning that provides excellent performance across a variety of common server workloads</span><br><span class="line">- virtual-guest               - Optimize for running inside a virtual guest</span><br><span class="line">- virtual-host                - Optimize for running KVM guests</span><br><span class="line">Current active profile: virtual-guest</span><br><span class="line"></span><br><span class="line">[root@yadoom ~]# tuned-adm profile powersave</span><br></pre></td></tr></table></figure>
<ul>
<li><p>latency-performance</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# grep -vE &apos;^#|^$&apos; /usr/lib/tuned/latency-performance/tuned.conf </span><br><span class="line">[main]</span><br><span class="line">summary=Optimize for deterministic performance at the cost of increased power consumption</span><br><span class="line">[cpu]</span><br><span class="line">force_latency=1</span><br><span class="line">governor=performance</span><br><span class="line">energy_perf_bias=performance</span><br><span class="line">min_perf_pct=100</span><br><span class="line">[sysctl]</span><br><span class="line">kernel.sched_min_granularity_ns=10000000</span><br><span class="line">vm.dirty_ratio=10</span><br><span class="line">vm.dirty_background_ratio=3</span><br><span class="line">vm.swappiness=10</span><br><span class="line">kernel.sched_migration_cost_ns=5000000</span><br></pre></td></tr></table></figure>
</li>
<li><p>throughput-performance</p>
</li>
</ul>
<h3 id="Custom-Tuning-Profiles"><a href="#Custom-Tuning-Profiles" class="headerlink" title="Custom Tuning Profiles"></a>Custom Tuning Profiles</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# cd /usr/lib/tuned/</span><br><span class="line">[root@yadoom ~]# mkdir test-performance</span><br><span class="line"></span><br><span class="line">[root@yadoom ~]# vim test-performance/tuned.conf</span><br><span class="line">[main]</span><br><span class="line">include=latency-performance</span><br><span class="line">summary=Test profile that uses settings for latency-performance tuning profile</span><br><span class="line"></span><br><span class="line">[root@yadoom ~]# tuned-adm list</span><br><span class="line">......</span><br><span class="line">- test-performance            - Test profile that uses settings for latency-performance tuning profile</span><br><span class="line">......</span><br></pre></td></tr></table></figure>
<h2 id="CPU-Tuning"><a href="#CPU-Tuning" class="headerlink" title="CPU Tuning"></a>CPU Tuning</h2><h3 id="CPU模式"><a href="#CPU模式" class="headerlink" title="CPU模式"></a>CPU模式</h3><ul>
<li><p>内核模式</p>
<p>  Ring0，Core程序运行。</p>
</li>
<li><p>用户模式</p>
<p>  Ring3，用户进程运行。该模式的进程无法直接操作硬件，通过系统调用，由内核程序翻译后调用。</p>
</li>
</ul>
<blockquote>
<p>Tips: 完全虚拟化如何让虚拟机认为自己直接运行在主机上？</p>
<p>答：引入了-1环（硬件虚拟化），使虚拟机管理程序运行在-1环，虚拟机操作系统就可以运行在Ring0。</p>
</blockquote>
<h3 id="CPU-Scheduler"><a href="#CPU-Scheduler" class="headerlink" title="CPU Scheduler"></a>CPU Scheduler</h3><ul>
<li>FF SCHED_FIFO: 先进先出的实时进程</li>
<li>RR SCHED_RR: 循环的实时进程</li>
<li>TS SCHED_OTHER (SCHED_NORMAL): 调度算法为CFQ。普通的共享时间进程，用户态进程。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@kvm-2 ~]# ps -e -o class,cmd</span><br><span class="line">CLS CMD</span><br><span class="line">TS  /sbin/init</span><br><span class="line">TS  [kthreadd]</span><br><span class="line">FF  [migration/0]</span><br><span class="line">TS  [ksoftirqd/0]</span><br><span class="line">FF  [migration/0]</span><br></pre></td></tr></table></figure>
<h3 id="Scheduler-Priority"><a href="#Scheduler-Priority" class="headerlink" title="Scheduler Priority"></a>Scheduler Priority</h3><ul>
<li>Real-time process (SCHED_FIFO/SCHED_RR) real-time priority<ul>
<li>1 (lowest priority) to 99 (higest priority)。</li>
</ul>
</li>
<li>Conventional process static priority(SCHED_NORMAL)<ul>
<li>100 (highest priority) to 139 (lowest priority)。nice值可动态调整该值。</li>
<li>Conventional process’s static priority = (120 + Nice value)</li>
<li>By default, conventional process starts with nice value of 0 which equals static priority 120</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@kvm-2 ~]# ps -e -o class,rtprio,pri,nice,cmd</span><br><span class="line">CLS RTPRIO PRI  NI CMD</span><br><span class="line">TS       -  19   0 /sbin/init       # RTPRIO 为&quot;-&quot;，说明该进程没有实时优先级</span><br><span class="line">TS       -  19   0 [kthreadd]       # PRI 为19，说明它静态优先级为 100+19+0=119</span><br><span class="line">FF      99 139   - [migration/0]    # CLS是FF，说明它是实时优先级 99</span><br><span class="line">TS       -  19   0 [ksoftirqd/0]</span><br><span class="line">......</span><br><span class="line">TS       -  19   0 /sbin/rsyslogd -i /var/run/syslogd.pid -c 5</span><br><span class="line">TS       -  19   0 irqbalance --pid=/var/run/irqbalance.pid</span><br><span class="line">TS       -  19   0 rpcbind</span><br><span class="line">TS       -  19   0 dbus-daemon --system</span><br><span class="line">TS       -  19   0 NetworkManager --pid-file=/var/run/NetworkManager/NetworkManager.pid</span><br><span class="line">TS       -  19   0 /usr/sbin/modem-manager</span><br><span class="line">TS       -  19   0 rpc.statd</span><br><span class="line">TS       -  19   0 /usr/sbin/acpid   # 大多数用户进程，优先级都为 119</span><br></pre></td></tr></table></figure>
<ul>
<li><p>dynamic priority</p>
<p>  指内核监控长时间未运行的（TS类别）进程，临时调高它的优先级（nice值），避免进程饥饿。相反地，也会惩罚长时间占用cpu的进程。</p>
<p>  dynamic priority = max (100, min (  static priority - bonus + 5, 139)) </p>
<p>  bonus is ranging from 0 to 10,  which is set by scheduler depends on the past history of the process; more precisely, it is related to the average sleep time of the process.</p>
</li>
</ul>
<h3 id="Scheduler-Priority-Changing"><a href="#Scheduler-Priority-Changing" class="headerlink" title="Scheduler Priority Changing"></a>Scheduler Priority Changing</h3><ul>
<li>Changing Real-time/Conventional process priority.</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#Real-time process </span><br><span class="line">$chrt 80  ps -e -o class,rtprio,pri,nice,cmd</span><br><span class="line">..</span><br><span class="line">FF      80 120   - ps -e -o class,rtprio,pri,nice,cmd</span><br><span class="line"># Conventional  process</span><br><span class="line">$nice -n 10  ps -e -o class,rtprio,pri,nice,cmd</span><br><span class="line">...</span><br><span class="line">TS       -  12  10 ps -e -o class,rtprio,pri,nice,cmd</span><br></pre></td></tr></table></figure>
<h3 id="Interrupt-and-IRQ"><a href="#Interrupt-and-IRQ" class="headerlink" title="Interrupt and IRQ"></a>Interrupt and IRQ</h3><p>中断请求（IRQ）是用于服务的请求，在硬件层发出。可使用专用硬件线路或者跨硬件总线的信息数据包（消息信号中断，MSI ）发出中断。启用中断后，接收 IRQ 后会提示切换到中断上下文。</p>
<p>CPU绑定后，它仍然要服务于中断。应该将中断绑定至那些非隔离的CPU上，从而避免那些隔离的CPU处理中断程序；</p>
<p>/proc/interrupts文件列出每个I/O 设备中每个 CPU 的中断数，每个 CPU 核处理的中断数，中断类型，以及用逗号分开的注册为接收中断的驱动程序列表。（详情请参考 proc(5) man page：man 5 proc）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# cat /proc/interrupts </span><br><span class="line">           CPU0       CPU1       </span><br><span class="line">  0:      14678          0   IO-APIC-edge      timer</span><br><span class="line">  1:          2          0   IO-APIC-edge      i8042</span><br><span class="line">  4:          2          0   IO-APIC-edge    </span><br><span class="line">  7:          0          0   IO-APIC-edge      parport0</span><br><span class="line">  8:          1          0   IO-APIC-edge      rtc0</span><br><span class="line">  9:          0          0   IO-APIC-fasteoi   acpi</span><br><span class="line"> 12:          4          0   IO-APIC-edge      i8042</span><br><span class="line"> 14:   45394223          0   IO-APIC-edge      ata_piix</span><br><span class="line"> 15:          0          0   IO-APIC-edge      ata_piix</span><br><span class="line"> 16:         56   16232636   IO-APIC-fasteoi   i915, p2p1</span><br><span class="line"> 18:    5333843   11365439   IO-APIC-fasteoi   uhci_hcd:usb4</span><br><span class="line"> 20:    2277759          0   IO-APIC-fasteoi   ata_piix</span><br><span class="line"> 21:          3          0   IO-APIC-fasteoi   ehci_hcd:usb1, uhci_hcd:usb2</span><br><span class="line"> 22:          0          0   IO-APIC-fasteoi   uhci_hcd:usb3</span><br><span class="line"> 23:       3813       6412   IO-APIC-fasteoi   uhci_hcd:usb5, Intel ICH7</span><br><span class="line">......</span><br><span class="line"># APIC表示高级可编程中断控制器（Advanced Programmable Interrupt Controlle）</span><br><span class="line"># APIC是SMP体系的核心，通过APIC可以将中断分发到不同的CPU 来处理。</span><br><span class="line"># i915：Intel i915 集成显卡驱动</span><br></pre></td></tr></table></figure>
<h3 id="Soft-Interrupt-and-Context-Switch"><a href="#Soft-Interrupt-and-Context-Switch" class="headerlink" title="Soft Interrupt and Context Switch"></a>Soft Interrupt and Context Switch</h3><p>上下文切换（也称做进程切换或任务切换）是指 CPU 从一个进程或线程切换到另一个进程或线程。</p>
<p>CPU通过时间片分配算法来循环执行任务，当前任务执行一个时间片后会切换到下一个任务。但是，在切换前会保存上一个任务的状态，以便下次切换回这个任务时，可以再次加载这个任务的状态，从任务保存到再加载的过程就是一次上下文切换。</p>
<h3 id="查看中断"><a href="#查看中断" class="headerlink" title="查看中断"></a>查看中断</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mpstat</span><br><span class="line">dstat -c</span><br></pre></td></tr></table></figure>
<h3 id="将IRQ绑定CPU"><a href="#将IRQ绑定CPU" class="headerlink" title="将IRQ绑定CPU"></a>将IRQ绑定CPU</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">echo CPU_MASK &gt; /proc/irq/&lt;irq number&gt;/smp_affinity</span><br><span class="line"></span><br><span class="line"># 案例：将CPU中断绑定到CPU #0,#1上。</span><br><span class="line">echo 3 &gt; /proc/irq/&lt;irq number&gt;/smp_affinity</span><br></pre></td></tr></table></figure>
<p>将IRQ绑定到某个CPU，那么最好在系统启动时，将那个CPU隔离起来，不被scheduler通常的调度。<br>可以通过在Linux kernel中加入启动参数：isolcpus=cpu-list将CPU隔离起来。</p>
<h3 id="IRQ-Irqbalance"><a href="#IRQ-Irqbalance" class="headerlink" title="IRQ Irqbalance"></a>IRQ Irqbalance</h3><p>irqbalance用于优化中断分配，它会自动收集系统数据以分析使用模式，并依据系统负载状况将工作状态置于 Performance mode 或 Power-save mode。处于Performance mode 时，irqbalance 会将中断尽可能均匀地分发给各个 CPU core，以充分利用 CPU 多核，提升性能。</p>
<p>处于Power-save mode 时，irqbalance 会将中断集中分配给第一个 CPU，以保证其它空闲 CPU 的睡眠时间，降低能耗</p>
<p>但是在实时系统中会导致中断自动漂移，对性能造成不稳定因素，在高性能的场合建议关闭。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/etc/init.d/irqbalance stop</span><br></pre></td></tr></table></figure>
<h3 id="查看上下文切换"><a href="#查看上下文切换" class="headerlink" title="查看上下文切换"></a>查看上下文切换</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sar -w           # 查看上下文切换的平均次数，以及进程创建的平均值</span><br><span class="line">vmstat 1 3       # 每秒上下文切换次数</span><br></pre></td></tr></table></figure>
<h3 id="如何减少上下文切换"><a href="#如何减少上下文切换" class="headerlink" title="如何减少上下文切换"></a>如何减少上下文切换</h3><pre><code>既然上下文切换会导致额外的开销，因此减少上下文切换次数便可以提高多线程程序的运行效率。减少上下文切换的方法有无锁并发编程、CAS算法、使用最少线程和使用协程。

- 无锁并发编程。多线程竞争时，会引起上下文切换，所以多线程处理数据时，可以用一些办法来避免使用锁，如将数据的ID按照Hash取模分段，不同的线程处理不同段的数据
- CAS算法。Java的Atomic包使用CAS算法来更新数据，而不需要加锁
- 使用最少线程。避免创建不需要的线程，比如任务很少，但是创建了很多线程来处理，这样会造成大量线程都处于等待状态
- 协程。在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换
</code></pre><h2 id="CPU-Iffinity（姻亲关系）"><a href="#CPU-Iffinity（姻亲关系）" class="headerlink" title="CPU Iffinity（姻亲关系）"></a>CPU Iffinity（姻亲关系）</h2><p>当软中断和上下文切换过大时。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">taskset</span><br><span class="line">mask       二进程  #CPU</span><br><span class="line">0x0000 0001  0001：node 0</span><br><span class="line">0x0000 0003  0011：node 0和1</span><br><span class="line">0x0000 0005  0101：node 0和2</span><br><span class="line">0x0000 0007  0111：node 0-2</span><br><span class="line"></span><br><span class="line">[root@yadoom ~]# taskset -p mask pid</span><br><span class="line">101, 3# CPU</span><br><span class="line">[root@yadoom ~]# taskset -p 0x00000005 101</span><br><span class="line"></span><br><span class="line">绑定进程101，CPU 0-2#、7#</span><br><span class="line">taskset -p -c 0-2,7 101</span><br><span class="line"></span><br><span class="line"># 指定CPU启动进程</span><br><span class="line">taskset mask -- program</span><br><span class="line">taskset -c 0,5,7-9 – myprogram</span><br></pre></td></tr></table></figure>
<h3 id="CPU-子系统"><a href="#CPU-子系统" class="headerlink" title="CPU 子系统"></a>CPU 子系统</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# mkdir /cpusets</span><br><span class="line">[root@yadoom ~]# vim /etc/fstab</span><br><span class="line">cpuset                  /cpusets                cpuset  defaults        0 0</span><br><span class="line">[root@yadoom ~]# mount -a</span><br><span class="line">[root@yadoom ~]# ls /cpusets/</span><br><span class="line">cgroup.clone_children  cgroup.sane_behavior  cpuset.mem_exclusive   cpuset.memory_pressure          cpuset.memory_spread_slab  cpuset.sched_relax_domain_level  tasks</span><br><span class="line">cgroup.event_control   cpuset.cpu_exclusive  cpuset.mem_hardwall    cpuset.memory_pressure_enabled  cpuset.mems                notify_on_release</span><br><span class="line">cgroup.procs           cpuset.cpus           cpuset.memory_migrate  cpuset.memory_spread_page       cpuset.sched_load_balance  release_agent</span><br><span class="line">#</span><br><span class="line">[root@yadoom ~]# cat /cpusets/cpuset.cpus </span><br><span class="line">0-3</span><br><span class="line"></span><br><span class="line"># 创建子域</span><br><span class="line">[root@yadoom ~]# mkdir /cpusets/domain1</span><br><span class="line">[root@yadoom ~]# ls /cpusets/domain1/</span><br><span class="line">......</span><br><span class="line">[root@yadoom ~]# echo 0-1 &gt;/cpusets/domain1/cpuset.cpus   #将CPU #0,#1绑定进来</span><br><span class="line">[root@yadoom ~]# echo 0 &gt;/cpusets/domain1/cpuset.mems     #将内存绑定进来</span><br><span class="line">[root@yadoom ~]# echo #pid /cpusets/domain1/tasks         #将某个进程绑定进来，该进程只能在CPU 0-1上运行</span><br><span class="line"></span><br><span class="line">[root@yadoom ~]# ps -e -o psr,pid,cmd</span><br></pre></td></tr></table></figure>
<h2 id="Memory-Tuning"><a href="#Memory-Tuning" class="headerlink" title="Memory Tuning"></a>Memory Tuning</h2><h3 id="Memory-Management"><a href="#Memory-Management" class="headerlink" title="Memory Management"></a>Memory Management</h3><ul>
<li>An important thing for kernel to do</li>
<li>Paping（分页） is the way to manage system memory<ul>
<li>Memory is organized into pages</li>
<li>For x86, normal page size is 4KiB(4096 Bytes)</li>
</ul>
</li>
<li>Processes do not address physical memory directly, each process has a virtual address space<ul>
<li>32-bit: 4GiB maximum memory a process can have</li>
<li>64-bit: 16EiB maximum memory a process can have</li>
</ul>
</li>
<li>Process can only see physical pages that have been mapped into its virtual address, so security enforced（进程间是隔离的）</li>
<li>Virtual page must translate to physical page in order to access data.</li>
</ul>
<h3 id="VSZ-vs-RSS"><a href="#VSZ-vs-RSS" class="headerlink" title="VSZ vs. RSS"></a>VSZ vs. RSS</h3><ol>
<li><p>when process requests memory, only reserve virtual memory address, but does not actually map them to physical pages UNTIL they are used.</p>
<ul>
<li>Virtual memory used: VIRT, VSIZE, VSZ</li>
<li>Physical memory used: RES, RSS</li>
</ul>
</li>
<li><p>Processes can share memory by mapping same physical pages to their private virtual address space, kernel takes control（进程可以共享同一片物理内存）</p>
</li>
</ol>
<hr>
<ul>
<li>VIRT  –  Virtual Memory Size (KiB)</li>
</ul>
<p>The total amount of virtual memory used by the task.  It includes all code, data and shared libraries plus pages that have been swapped out and pages that have been mapped but not used.（一个任务使用的虚拟内存的总量，包括 代码、数据、共享库加上已换出的页和已经被映射出去了但是还没被使用的页。 简单理解就是所有的虚拟内存中含有共享库、共享内存、堆、栈和所有已经申请的内存空间。）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 利用命令查看：vsz=data + code + shared lib</span><br><span class="line">$ ps -a -o pid,pmem,cmd,vsz,drs,trs</span><br><span class="line">  PID %MEM CMD                            VSZ   DRS  TRS</span><br><span class="line"> 3870  0.0 ps -a -o pid,pmem,cmd,vsz,d 148912 148822  89</span><br><span class="line">10906  0.0 screen -dr                  129744 129744   0</span><br><span class="line">16116  0.0 sudo -i                     195524 195524   0</span><br><span class="line">16117  0.3 -zsh                        156876 156876   0</span><br></pre></td></tr></table></figure>
<ul>
<li>RES  –  Resident Memory Size (KiB)</li>
</ul>
<p>The non-swapped physical memory a task is using.（一个任务使用的不可交换的物理内存大小。是一个进程正在使用的内存空间（堆、栈）。）</p>
<ul>
<li><p>SHR  –  Shared Memory Size (KiB)</p>
<p> The amount of shared memory available to a task, not all of which is typically resident. It simply reflects memory that could be  potentially shared with other processes.（一个任务正在使用的共享内存大小，这个大小对该进程不是固定的，它只是简单的反应了可以被其他进程共享的内存大小。）</p>
</li>
</ul>
<h3 id="Page-Walk"><a href="#Page-Walk" class="headerlink" title="Page Walk"></a>Page Walk</h3><ul>
<li>Virtual to Physical address translations are stored in page tables</li>
<li>These page table are stored in memory</li>
<li>Looking up these page tables is a processes called a Page Walk</li>
<li>Every memory access requires a Page Walk</li>
<li>Hardware assisted but it is still expensive</li>
<li>Typically, takes 10-100 CPU clock cycles</li>
<li>The obvious optimization is to cache the results</li>
</ul>
<p><img src="/images/linux/performance-tuning/virtual_to_physical.gif" alt="virtual_to_physical"></p>
<p>一次虚拟内存地址到物理内存地址查找（page walk）太慢，所以引入TLB来缓存。</p>
<h3 id="TLB-Translation-look-aside-buffer"><a href="#TLB-Translation-look-aside-buffer" class="headerlink" title="TLB - Translation look-aside buffer"></a>TLB - Translation look-aside buffer</h3><ul>
<li>The Translation Lookaside Buffer(TLB) is a small CPU cache of recently used virtual to physical address mappings.</li>
<li>TLB is to speed up virtual to physical address translations</li>
<li>CPU checks TLB first<ul>
<li>if TLB hit, return address</li>
<li>if TLB miss, do a Page Walk and store translation in TLB</li>
</ul>
</li>
<li>Using TLB, increases performance by as much as 15%</li>
<li>Look up takes 0.5-1 clock cycle</li>
<li>TLB is typically implemented as CAM<ul>
<li>CAM(Content addressable memory，内容寻址存储器)</li>
</ul>
</li>
<li>TLB cache has to be flushed on every context switch</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# x86info -c</span><br><span class="line">x86info v1.25.  Dave Jones 2001-2009</span><br><span class="line">Feedback to &lt;davej@redhat.com&gt;.</span><br><span class="line"></span><br><span class="line">Found 40 CPUsMP Configuration Table Header MISSING!</span><br><span class="line"></span><br><span class="line">--------------------------------------------------------------------------</span><br><span class="line">CPU #1</span><br><span class="line">EFamily: 0 EModel: 4 Family: 6 Model: 79 Stepping: 1</span><br><span class="line">CPU Model: Unknown model. </span><br><span class="line">Processor name string: Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz</span><br><span class="line">Type: 0 (Original OEM)  Brand: 0 (Unsupported)</span><br><span class="line">Number of cores per physical package=16</span><br><span class="line">Number of logical processors per socket=32</span><br><span class="line">Number of logical processors per core=2</span><br><span class="line">APIC ID: 0x0    Package: 0  Core: 0   SMT ID 0</span><br><span class="line">Cache info</span><br><span class="line">TLB info</span><br><span class="line"> Data TLB: 4KB pages, 4-way associative, 64 entries    # TLB 很小</span><br><span class="line"> 64 byte prefetching.</span><br><span class="line">Found unknown cache descriptors: 63 76 b5 c3 ff</span><br></pre></td></tr></table></figure>
<h3 id="Huge-Page"><a href="#Huge-Page" class="headerlink" title="Huge Page"></a>Huge Page</h3><p>操作系统默认的内存是以4KB分页的，而虚拟地址和内存地址需要转换，而这个转换要查表，CPU为了加速这个查表过程会内建TLB(Translation Lookaside Buffer)。显然，如果虚拟页越小，表里的条目数也就越多，而TLB大小是有限的，条目数越多TLB的Cache Miss也就会越高，所以如果我们能启用大内存页就能间接降低TLB Cache Miss。</p>
<p>Huge Page在内存中必须是连续的地址，所以通常在系统开机时指定内核参数直接分配。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# sysctl -w vm.nr_hugepages=128</span><br><span class="line">vm.nr_hugepages = 128</span><br><span class="line">[root@yadoom ~]# cat /proc/meminfo | grep Huge</span><br><span class="line">AnonHugePages:     77824 kB</span><br><span class="line">HugePages_Total:     128</span><br><span class="line">HugePages_Free:      128</span><br><span class="line">HugePages_Rsvd:        0</span><br><span class="line">HugePages_Surp:        0</span><br><span class="line">Hugepagesize:       2048 kB</span><br></pre></td></tr></table></figure>
<p><img src="/images/linux/performance-tuning/huge_pages_tlb.png" alt="huge_pages_tlb"></p>
<h3 id="Transparent-HugePages（透明大页）"><a href="#Transparent-HugePages（透明大页）" class="headerlink" title="Transparent HugePages（透明大页）"></a>Transparent HugePages（透明大页）</h3><ul>
<li><p>什么是Transparent HugePages（透明大页）?</p>
<p>  简单的讲，对于内存占用较大的程序，可以通过开启HugePage来提升系统性能。但这里会有个要求，就是在编写程序时，代码里需要显示的对HugePage进行支持。</p>
<p>  而红帽企业版Linux为了减少程序开发的复杂性，并对HugePage进行支持，部署了Transparent HugePages。Transparent HugePages是一个使管理Huge Pages自动化的抽象层，实现方案为操作系统后台有一个叫做khugepaged的进程，它会一直扫描所有进程占用的内存，在可能的情况下会把4kPage交换为Huge Pages。</p>
<p>  为什么Transparent HugePages（透明大页）对系统的性能会产生影响？<br>  在khugepaged进行扫描进程占用内存，并将4kPage交换为Huge Pages的这个过程中，对于操作的内存的各种分配活动都需要各种内存锁，直接影响程序的内存访问性能。并且，这个过程对于应用是透明的，在应用层面不可控制,对于专门为4k page优化的程序来说，可能会造成随机的性能下降现象。</p>
</li>
<li><p>怎么设置Transparent HugePages（透明大页）?</p>
<ol>
<li><p>查看是否启用透明大页</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# cat /sys/kernel/mm/transparent_hugepage/enabled </span><br><span class="line">[always] madvise never</span><br><span class="line">使用命令查看时，如果输出结果为[always]表示透明大页启用了，[never]表示透明大页禁用。</span><br></pre></td></tr></table></figure>
</li>
<li><p>关闭透明大页</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled </span><br><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br></pre></td></tr></table></figure>
<ol start="3">
<li><p>启用透明大页</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo always &gt;  /sys/kernel/mm/transparent_hugepage/enabled </span><br><span class="line">echo always &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>设置开机关闭</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# vim /etc/rc.local</span><br><span class="line">if test -f /sys/kernel/mm/redhat_transparent_hugepage/enabled; then     </span><br><span class="line">     echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled     </span><br><span class="line">    echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag </span><br><span class="line">fi</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
</ul>
<p>问题：<a href="https://my.oschina.net/llzx373/blog/226446" target="_blank" rel="noopener">Transparent Huge Pages相关概念及对mysql的影响</a></p>
<h3 id="Swap"><a href="#Swap" class="headerlink" title="Swap"></a>Swap</h3><p>内存回收和swap的关系，我们可以提出以下几个问题：</p>
<ol>
<li>什么时候会进行内存回收呢？</li>
<li>哪些内存会可能被回收呢？</li>
<li>回收的过程中什么时候会进行交换呢？</li>
<li>具体怎么交换？</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">vm.swappiness=&#123;0..100&#125;：使用交换分区的倾向性, 默认60</span><br><span class="line">	overcommit_memory=2： 过量使用（0：启发式过量；1：总是过量；2：允许下述超出百分比）</span><br><span class="line">	overcommit_ratio=50：</span><br><span class="line">		可用虚存：swap+RAM*ratio</span><br><span class="line">			swap: 2G</span><br><span class="line">			RAM: 8G</span><br><span class="line">		        可用虚存：memory=2G+8G*50%=6G</span><br><span class="line"></span><br><span class="line">	充分使用物理内存：</span><br><span class="line">		1、swap跟RAM一样大；</span><br><span class="line">		2、overcommit_memory=2, overcommit_ratio=100：swappiness=0；</span><br><span class="line">			memory: swap+ram</span><br><span class="line">	参考设置：</span><br><span class="line">		1、Batch compute（批处理计算）：&lt;= 4 * RAM</span><br><span class="line">		2、Database server：&lt;= 1G</span><br><span class="line">		3、Application server：&gt;= 0.5 * RAM</span><br><span class="line"></span><br><span class="line">默认值：</span><br><span class="line">[root@yadoom ~]# cat /proc/sys/vm/overcommit_memory </span><br><span class="line">0</span><br><span class="line">[root@yadoom ~]# cat /proc/sys/vm/swappiness </span><br><span class="line">60</span><br><span class="line">[root@yadoom ~]# cat /proc/sys/vm/overcommit_ratio </span><br><span class="line">50</span><br></pre></td></tr></table></figure>
<h3 id="Swap-Tuning"><a href="#Swap-Tuning" class="headerlink" title="Swap Tuning"></a>Swap Tuning</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# vmstat 1 3</span><br><span class="line">procs    -----------memory----------   ---swap-- ----io---- --system-- -----cpu-----</span><br><span class="line"> r  b     swpd   free   buff  cache     si   so    bi    bo   in   cs us sy  id wa st</span><br><span class="line"> 1  0      0 260352448  34480 2212952    0    0     0     1    2    3  0  0 100  0  0</span><br><span class="line"> 0  0      0 260352192  34480 2212956    0    0     0     0  124  177  0  0 100  0  0</span><br><span class="line"> 0  0      0 260352192  34480 2212956    0    0     0     0   27   44  0  0 100  0  0</span><br></pre></td></tr></table></figure>
<ul>
<li>Memory<ul>
<li>swpd: the amount of virtual memory used.</li>
</ul>
</li>
<li><p>Swap</p>
<ul>
<li><p>si: Amount of memory swapped in from disk (/s).</p>
<ul>
<li>so: Amount of memory swapped to disk (/s).</li>
</ul>
<p>paging in and paging out frequently hurts performance. so si and so are very important.</p>
</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 在每个磁盘上建立swap分区，并给与相同优先级</span><br><span class="line">[root@localhost ~]# vim /etc/fstab</span><br><span class="line">.....</span><br><span class="line">/dev/sda1   swap swap pri=5 0 0</span><br><span class="line">/dev/sdb1   swap swap pri=5 0 0</span><br><span class="line">/dev/sdc1   swap swap pri=1 0 0    # 最慢的磁盘，swap最小的优先级</span><br></pre></td></tr></table></figure>
<h3 id="Buffer-Cache-and-Page-Cache"><a href="#Buffer-Cache-and-Page-Cache" class="headerlink" title="Buffer Cache and Page Cache"></a>Buffer Cache and Page Cache</h3><ul>
<li>Buffer cache(Slab cache)<br>  used to cache file system meta data(dentries and inodes).跟文件无关的东西的缓存。</li>
<li>Page cache<br>  used to cache file system block data(file content).跟文件相关的东西的缓存。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# sysctl -a |grep vfs_cache_pressure   # Buffer cache</span><br><span class="line">vm.vfs_cache_pressure = 100</span><br><span class="line"># 0: 不回收dentries和inodes;</span><br><span class="line"># 1-99: 倾向于不回收；</span><br><span class="line"># 100: 倾向性等于 page cache和swap cache；</span><br><span class="line"># 100+: 倾向于回收；</span><br></pre></td></tr></table></figure>
<h3 id="Slab"><a href="#Slab" class="headerlink" title="Slab"></a>Slab</h3><blockquote>
<p>在linux内核中会有许多小对象，这些对象构造销毁十分频繁，比如inode，dentry。这么这些对象如果每次构建的时候就向内存要一个页，而其实际大小可能只有几个字节，这样就非常浪费，为了解决这个问题就引入了一种新的机制来处理在同一页框中如何分配小存储器区。这就是我们要讨论的slab层。在讲述slab前，我想先铺垫一下有关内存页的概念，我们都知道在linux中内存都是以页为单位来进行管理的(通常为4KB)，当内核需要内存就调用如：kmem_getpages这样的接口(底层调用__alloc_pages())。那么内核是如何管理页的分配的，这里linux使用了伙伴算法。slab也是向内核申请一个个页，然后再对这些页框做管理来达到分配小存储区的目的的。</p>
</blockquote>
<ul>
<li><p>What is slab</p>
<p>  A slab is a set of one or more contiguous pages of memory set aside by the slab allocator for an individual cache. This memory is further divided into equal segments the size of the object type that the cache is managing.(Slab是一种内存分配器，通过将内存划分不同大小的空间分配给对象使用来进行缓存管理，应用于内核对象的缓存。)</p>
</li>
<li><p>作用（提升cpu访问内存小对象的效率）</p>
<ul>
<li>Slab对小对象进行分配，不用为每个小对象分配一个页，节省了空间。</li>
<li>内核中一些小对象创建析构很频繁，Slab对这些小对象做缓存，可以重复利用一些相同的对象，减少内存分配次数。</li>
</ul>
</li>
</ul>
<h3 id="Anonymous-pages"><a href="#Anonymous-pages" class="headerlink" title="Anonymous pages"></a>Anonymous pages</h3><ul>
<li>Anonymous pages can be another large consumer of data</li>
<li>Are not accociated with a file, but instead contain<ul>
<li>Program data - arrays, heap allocations, etc</li>
<li>Anonymous memory regions</li>
<li>Dirty memory mapped process private pages</li>
<li>IPC shared memory region pages</li>
</ul>
</li>
<li>Anonymous pages are eligible for swap(Swap分区可以将不活跃的页交换到硬盘中，交换之后页将被释放。)</li>
<li>Anonymous pages = RSS - Shared</li>
</ul>
<h3 id="Memory-Page-Status"><a href="#Memory-Page-Status" class="headerlink" title="Memory Page Status"></a>Memory Page Status</h3><ul>
<li>Free<br>  Page is available for immediate allocation.</li>
<li>Inactive Clean<br>  Page content has been written to disk, or has not changed since being read from disk. Page is available for allocation.</li>
<li>Inactive Dirty<br>  Page not in use, and page content has been modified but not written back to disk.</li>
<li>Active<br>  Page is in use by a process</li>
</ul>
<h3 id="Dirty-Page"><a href="#Dirty-Page" class="headerlink" title="Dirty Page"></a>Dirty Page</h3><p>因为硬盘的读写速度远赶不上内存的速度，系统就把读写比较频繁的数据事先放到内存中，以提高读写速度，这就叫高速缓存，linux是以页作为高速缓存的单位，当进程修改了高速缓存里的数据时，该页就被内核标记为脏页，内核将会在合适的时间把脏页的数据写到磁盘中去，以保持高速缓存中的数据和磁盘中的数据是一致的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# sysctl -a | grep dirty</span><br><span class="line">vm.dirty_background_ratio = 10</span><br><span class="line">vm.dirty_background_bytes = 0</span><br><span class="line">vm.dirty_ratio = 20</span><br><span class="line">vm.dirty_bytes = 0</span><br><span class="line">vm.dirty_writeback_centisecs = 500</span><br><span class="line">vm.dirty_expire_centisecs = 3000</span><br><span class="line"></span><br><span class="line">[root@yadoom ~]# sysctl -a | grep pdflush</span><br><span class="line">vm.nr_pdflush_threads = 0</span><br></pre></td></tr></table></figure>
<ul>
<li><p>vm.dirty_expire_centisecs</p>
<p>  默认是3000（单位是1/100秒）。这个值表示page cache中的数据多久之后被标记为脏数据。只有标记为脏的数据在下一个周期到来时pdflush才会刷入到磁盘，这样就意味着用户写的数据在30秒之后才有可能被刷入磁盘，在这期间断电都是会丢数据的。</p>
</li>
<li><p>vm.dirty_writeback_centisecs</p>
<p>  默认一般是500（单位是1/100秒）。这个参数表示5s的时间pdflush就会被唤起去刷新脏数据。建议用户使用默认值。</p>
</li>
<li><p>vm.dirty_background_ratio(default 10)</p>
<p>  这个参数指定了当文件系统缓存脏页数量达到系统内存百分之多少时（如5%）就会触发pdflush/flush/kdmflush等后台回写进程运行，将一定缓存的脏页<strong>异步</strong>地刷入外存；</p>
</li>
<li><p>vm.dirty_ratio(default 20)</p>
<p>  这个参数则指定了当文件系统缓存脏页数量达到系统内存百分之多少时（如10%），系统不得不开始处理缓存脏页（因为此时脏页数量已经比较多，为了避免数据丢失需要将一定脏页刷入外存）；在此过程中很多应用进程可能会因为系统转而处理文件IO而阻塞。</p>
</li>
</ul>
<blockquote>
<p>之前一直错误的以为dirty_ratio的触发条件不可能达到，因为每次肯定会先达到vm.dirty_background_ratio的条件，后来才知道自己理解错了。确实是先达到vm.dirty_background_ratio的条件然后触发flush进程进行异步的回写操作，但是这一过程中应用进程仍然可以进行写操作，如果多个应用进程写入的量大于flush进程刷出的量那自然会达到vm.dirty_ratio这个参数所设定的坎，此时操作系统会转入同步地处理脏页的过程，阻塞应用进程。</p>
</blockquote>
<ul>
<li><p>vm.nr_pdflush_threads<br>  pdflush是linux系统后台运行的一个线程，这个进程负责把page cahce中的dirty状态的数据定期的输入磁盘。</p>
<p>  /proc/sys/vm/nr_pdflush_threads查看当前系统运行pdflush数量。当一段时间（一般是1s）没有任何的pdflush处于工作状态，系统会remove一个pdflush线程。pdflush最大和最小的数量是有配置的，但这些配置一般很少修改。</p>
</li>
<li><p>drop_caches（干净页的回收，缓存清理）</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# cat /proc/sys/vm/drop_caches</span><br><span class="line">0</span><br><span class="line">[root@yadoom ~]# sync  #先将缓存写入磁盘</span><br><span class="line">[root@yadoom ~]# echo 1 /proc/sys/vm/drop_caches  #释放所有页缓冲内存</span><br><span class="line">[root@yadoom ~]# echo 2 /proc/sys/vm/drop_caches  #释放未使用的slab缓冲内存</span><br><span class="line">[root@yadoom ~]# echo 3 /proc/sys/vm/drop_caches  #释放所有页缓冲和slab缓冲内存</span><br><span class="line">备注：</span><br><span class="line">slab缓存详解（一）http://blog.chinaunix.net/uid-27102327-id-3268687.html</span><br><span class="line">slab缓存详解（二）http://blog.chinaunix.net/uid-27102327-id-3268711.html</span><br><span class="line">http://blog.csdn.net/hs794502825/article/details/7981524</span><br></pre></td></tr></table></figure>
<h3 id="OOM-Kill"><a href="#OOM-Kill" class="headerlink" title="OOM Kill"></a>OOM Kill</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# cat /proc/sys/vm/panic_on_oom     # 0:on, 1:off</span><br><span class="line">0</span><br><span class="line">[root@yadoom ~]# echo 1 /proc/sys/vm/panic_on_oom  # 关闭oom kill，不推荐</span><br><span class="line">注意：由调整的进程衍生的进程将继承该进程的 oom_score。例如：如果 sshd 进程不受 oom _killer 功能影响，所有由 SSH 会话产生的进程都将不受其影响。</span><br><span class="line"></span><br><span class="line">当内存耗尽时，系统使用oom kill杀死大oom_score（-16~15，2的平方）的进程。oom_score得分由oom_adj得来。</span><br><span class="line">减小oom-adj值，避免被系统杀死：</span><br><span class="line">[root@yadoom ~]# echo -17 &gt; /proc/$(pidof sshd)/oom_adj</span><br><span class="line"></span><br><span class="line">-17：避免oom_killer杀死自己</span><br><span class="line">-16~15：帮助计算oom_score</span><br><span class="line">16：预留的最低级别，一般对于缓存的进程才有可能设置成这个级别</span><br></pre></td></tr></table></figure>
<p>有时free查看还有充足的内存，但还是会触发OOM，是因为该进程可能占用了特殊的内存地址空间。(‘Lower’ memory zone like DMA, DMA32)</p>
<h3 id="min-free-kbytes"><a href="#min-free-kbytes" class="headerlink" title="min_free_kbytes"></a>min_free_kbytes</h3><ul>
<li>/proc/sys/vm/min_free_kbytes<br>强制Linux VM最低保留多少空闲内存（Kbytes）</li>
</ul>
<p>内存管理从三个层次管理内存，分别是node, zone ,page。64位的x86物理机内存从高地址到低地址分为: Normal DMA32 DMA</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# grep Node /proc/zoneinfo </span><br><span class="line">Node 0, zone      DMA</span><br><span class="line">Node 0, zone    DMA32</span><br><span class="line">Node 0, zone   Normal</span><br><span class="line">Node 1, zone   Normal</span><br></pre></td></tr></table></figure>
<p>每个zone都有自己的min low high,如下，但是单位是page。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# grep &quot;Node 0, zone&quot; -A10 /proc/zoneinfo </span><br><span class="line">Node 0, zone      DMA</span><br><span class="line">  pages free     3934</span><br><span class="line">        min      3</span><br><span class="line">        low      3</span><br><span class="line">        high     4</span><br><span class="line">        scanned  0</span><br><span class="line">        spanned  4095</span><br><span class="line">        present  3835</span><br><span class="line">    nr_free_pages 3934</span><br><span class="line">    nr_inactive_anon 0</span><br><span class="line">    nr_active_anon 0</span><br><span class="line">--</span><br><span class="line">Node 0, zone    DMA32</span><br><span class="line">  pages free     571977</span><br><span class="line">        min      749</span><br><span class="line">        low      936</span><br><span class="line">        high     1123</span><br><span class="line">        scanned  0</span><br><span class="line">        spanned  1044480</span><br><span class="line">        present  756520</span><br><span class="line">    nr_free_pages 571977</span><br><span class="line">    nr_inactive_anon 0</span><br><span class="line">    nr_active_anon 0</span><br><span class="line">--</span><br><span class="line">Node 0, zone   Normal</span><br><span class="line">  pages free     4737209</span><br><span class="line">        min      9478</span><br><span class="line">        low      11847</span><br><span class="line">        high     14217</span><br><span class="line">        scanned  0</span><br><span class="line">        spanned  9699328</span><br><span class="line">        present  9566720</span><br><span class="line">    nr_free_pages 4737209</span><br><span class="line">    nr_inactive_anon 166</span><br><span class="line">    nr_active_anon 3973945</span><br></pre></td></tr></table></figure>
<p>上面可知：low = 5/4 <em> min、high = 3/2 </em> min。</p>
<p>min 和 low的区别：</p>
<ol>
<li>min下的内存是保留给内核使用的；当到达min，会触发内存的direct reclaim</li>
<li>low水位比min高一些，当内存可用量小于low的时候，会触发 kswapd reclaim。当kswapd慢慢的将内存 回收到high水位，就开始继续睡眠</li>
</ol>
<ul>
<li>/proc/sys/vm/extra_free_kbytes</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# sysctl  -a | grep free</span><br><span class="line">vm.min_free_kbytes = 90112</span><br><span class="line">vm.extra_free_kbytes = 0</span><br></pre></td></tr></table></figure>
<p>意义：low = min_free_kbytes*5/4 + extra_free_kbytes</p>
<ul>
<li>总结<br>调整该内存的内核参数的时候！调大的风险远大于调小的风险，会导致频繁的触发内存回收！如果有人想将vm.min_free_kbytes 调大，千万要注意当前的Free，一旦超过Free内存，会立刻触发direct reclaim。</li>
</ul>
<h3 id="进程间通信（SysV-IPC）"><a href="#进程间通信（SysV-IPC）" class="headerlink" title="进程间通信（SysV IPC）"></a>进程间通信（SysV IPC）</h3><ul>
<li>Types of IPC<ul>
<li>Semaphores（信号量）</li>
<li>Message queues（消息队列）</li>
<li>Shared memory</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">[root@kvm-2 ~]# ipcs </span><br><span class="line"></span><br><span class="line">------ Shared Memory Segments --------</span><br><span class="line">key        shmid      owner      perms      bytes      nattch     status      </span><br><span class="line">0x6c0006e7 0          zabbix     600        657056     6                       </span><br><span class="line"></span><br><span class="line">------ Semaphore Arrays --------</span><br><span class="line">key        semid      owner      perms      nsems     </span><br><span class="line">0x00000000 0          root       600        1         </span><br><span class="line">0x00000000 65537      root       600        1         </span><br><span class="line">0x7a0006e7 98306      zabbix     600        13        </span><br><span class="line"></span><br><span class="line">------ Message Queues --------</span><br><span class="line">key        msqid      owner      perms      used-bytes   messages    </span><br><span class="line"></span><br><span class="line">[root@kvm-2 ~]# ipcs -l                        # IPC resource limits</span><br><span class="line"></span><br><span class="line">------ Shared Memory Limits --------</span><br><span class="line">max number of segments = 4096                   # SHMMNI</span><br><span class="line">max seg size (kbytes) = 49291832                # SHMMAX</span><br><span class="line">max total shared memory (kbytes) = 25165824     # SHMALL</span><br><span class="line">min seg size (bytes) = 1</span><br><span class="line"></span><br><span class="line">------ Semaphore Limits --------</span><br><span class="line">max number of arrays = 128                      # SEMMNI</span><br><span class="line">max semaphores per array = 250                  # SEMMSL</span><br><span class="line">max semaphores system wide = 32000              # SEMMNS</span><br><span class="line">max ops per semop call = 100                    # SEMOPM</span><br><span class="line">semaphore max value = 32767</span><br><span class="line"></span><br><span class="line">------ Messages: Limits --------</span><br><span class="line">max queues system wide = 32768                  # MSGMNI</span><br><span class="line">max size of message (bytes) = 65536             # MSGMAX</span><br><span class="line">default max size of queue (bytes) = 65536       # MSGMNB</span><br></pre></td></tr></table></figure>
<ul>
<li><p>Tune Semaphores</p>
<ul>
<li>1<sup>st</sup>, the maximum number of semaphores per semaphore array, default 250.</li>
<li>2<sup>st</sup>, the maximum number of semaphores allowed sysetm-wide, default 32000.</li>
<li>3<sup>st</sup>, the maximum number of semaphores of allowed operations per semaphore system call, default 32.</li>
<li>4<sup>st</sup>, the maximum number of semaphore arrays, default 128.</li>
<li><p>so, 250 * 128 &lt;= 32000</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@kvm-2 ~]# sysctl -a | grep kernel.sem</span><br><span class="line">kernel.sem = 250        32000   32     128</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>Tune Message Queues</p>
<ul>
<li>kernel.msgmnb<br>  The maximum number of bytes in a single message queue, default 65535.</li>
<li>kernel.msgmni<br>  The maximum number of message queue, default ?.</li>
<li><p>kernel.msgmax<br>  The maximum size of a message that can be passwd between processes, default 65535.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@kvm-2 ~]# sysctl -a | grep kernel.msg</span><br><span class="line">kernel.msgmax = 65536     # 限制进程可以发送消息的最大字节数</span><br><span class="line">kernel.msgmni = 32768     # 最大消息队列数</span><br><span class="line">kernel.msgmnb = 65536     # 一个消息队列中最大的字节数</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>Tune </p>
<ul>
<li>kernel.shmmni<br>  The maximum number of shared memory segments system-wide, default 4096.</li>
<li>kernel.shmmax<br>  The maximum size of a shared memory segment that can be created, default 68719476736(64G).</li>
<li><p>kernel.shmall<br>  The total amount of shared memory, in pages, that can be used at one time on the system. default </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# sysctl -a | grep kernel.shm</span><br><span class="line">kernel.shmmax = 68719476736</span><br><span class="line">kernel.shmall = 4294967296        # 单位page，等于 4294967296(4g)*4096 byte</span><br><span class="line">kernel.shmmni = 4096</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h3 id="延展阅读"><a href="#延展阅读" class="headerlink" title="延展阅读"></a>延展阅读</h3><ul>
<li><a href="https://blog.csdn.net/dazuiba008/article/details/51261152" target="_blank" rel="noopener">Linux内存中的Cache真的能被回收吗？</a></li>
<li><a href="https://blog.csdn.net/renwotao2009/article/details/51979343" target="_blank" rel="noopener">linux的内存回收和交换</a></li>
</ul>
<h2 id="File-System-Tuning"><a href="#File-System-Tuning" class="headerlink" title="File System Tuning"></a>File System Tuning</h2><ul>
<li><p>EXT4</p>
<ul>
<li><p>内节点表初始化</p>
<p>对于超大文件系统，mkfs.ext4  进程要花很长时间初始化文件系统中到所有内节点表。可使用 -Elazy_itable_init=1 选项延迟这个进程。如果使用这个选项，内核进程将在挂载文件系统后继续初始化<br>该文件它。可使用 mount 命令的 -o init_itable=n 选项控制发生初始化到比例，其中执行这个后台初始化的时间约为 1/n。n 的默认值为 10。</p>
</li>
<li><p>Auto-fsync 行为</p>
<p>因为在重命名、截取或者重新写入某个现有文件后，有些应用程序不总是可以正确执行 fsync()，在重命名和截取操作后，ext4  默认自动同步文件。这个行为与原有到 ext3 文件系统行为大致相同。但 fsync()<br>操作可能会很耗时，因此如果不需要这个自动行为，请在 m ount 命令后使用 -o noauto_da_alloc 选项禁用它。这意味着该程序必须明确使用 fsync() 以保证数据一致。</p>
</li>
<li><p>日志  I/O  优先权</p>
<p>默认情况下，日志注释 I/O 比普通 I/O 的优先权稍高。这个优先权可使用 mount 命令的journal_ioprio=n 选项控制。默认值为 3。有效值范围为 0 -7，其中 0  时最高优先权 I /O。</p>
</li>
</ul>
</li>
<li><p>XFS(large file system)</p>
</li>
</ul>
<h2 id="Network-Tuning"><a href="#Network-Tuning" class="headerlink" title="Network Tuning"></a>Network Tuning</h2><h3 id="Tune-Network-latency"><a href="#Tune-Network-latency" class="headerlink" title="Tune Network latency"></a>Tune Network latency</h3><ul>
<li>net.ipv4.tcp_low_latency<ul>
<li>optimizes for low latency</li>
<li>optimizes for throughput(default)<br>  默认的RHEL为高吞吐量设计。</li>
</ul>
</li>
</ul>
<h3 id="TCP-UDP-Total-Maximum-Buffers"><a href="#TCP-UDP-Total-Maximum-Buffers" class="headerlink" title="TCP/UDP Total Maximum Buffers"></a>TCP/UDP Total Maximum Buffers</h3><ul>
<li>When receiving and network packets, there are buffers(queue length)</li>
<li>Kernel adjusts the size of these buffers automatically</li>
<li>System wide maximum buffers for TCP/UDP connections, usually no need to tune, set by kernel during boot, near the size of system memory<ul>
<li>net.ipv4.tcp_mem(pages)</li>
<li>net.ipv4.udp_mem(pages)</li>
</ul>
</li>
<li>Each connection creates a buffer</li>
</ul>
<h3 id="Tune-UDP-Socket-Buffer"><a href="#Tune-UDP-Socket-Buffer" class="headerlink" title="Tune UDP Socket Buffer"></a>Tune UDP Socket Buffer</h3><ul>
<li>Tune the minimum socket buffers for UDP connection<ul>
<li>net.ipv4.udp_rmem_min(bytes)</li>
<li>net.ipv4.udp_wmem_min(bytes)</li>
</ul>
</li>
<li>Tune the default socket buffers for core networking, include udp connection<ul>
<li>net.core.rmem_default(bytes)</li>
<li>net.core.wmem_default(bytes)</li>
</ul>
</li>
<li>Tune the maximum socket buffers for core networking, include udp connection<ul>
<li>net.core.rmem_max(bytes)</li>
<li>net.core.wmem_max(bytes)</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# sysctl -a | grep net.ipv4.udp</span><br><span class="line">net.ipv4.udp_mem = 24786432     33048576        49572864</span><br><span class="line">net.ipv4.udp_rmem_min = 4096</span><br><span class="line">net.ipv4.udp_wmem_min = 4096</span><br><span class="line">[root@localhost ~]# sysctl -a | grep net.core | grep mem</span><br><span class="line">net.core.wmem_max = 124928</span><br><span class="line">net.core.rmem_max = 124928</span><br><span class="line">net.core.wmem_default = 124928</span><br><span class="line">net.core.rmem_default = 124928</span><br><span class="line"># 由上可知 mem_default 等于 mem_max，表示已调节为最大吞吐量，缺点是需要占用大量内存。</span><br></pre></td></tr></table></figure>
<h3 id="Tune-TCP-Socket-Buffer"><a href="#Tune-TCP-Socket-Buffer" class="headerlink" title="Tune TCP Socket Buffer"></a>Tune TCP Socket Buffer</h3><ul>
<li>First, tune buffers of total networking(like UDP)<ul>
<li>net.core.rmem_max(bytes)</li>
<li>net.core.wmem_max(bytes)</li>
</ul>
</li>
<li><p>Then tune buffers of TCP specific networking</p>
<ul>
<li>net.ipv4.tcp_rmem(bytes)</li>
<li><p>net.ipv4.tcp_wmem(bytes)</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.ipv4.tcp_wmem = 4096        16384   4194304</span><br><span class="line">net.ipv4.tcp_rmem = 4096        87380   4194304</span><br></pre></td></tr></table></figure>
<ul>
<li>min: minimum receive/send buffer for a tcp connection</li>
<li>default: default buffer size, usually half of max buffer size</li>
<li>max: maximum receive/send buffer for a tcp connection</li>
</ul>
</li>
</ul>
</li>
<li>Total networking buffers &gt;= TCP buffers</li>
</ul>
<h3 id="How-much-buffer-to-set"><a href="#How-much-buffer-to-set" class="headerlink" title="How much buffer to set?"></a>How much buffer to set?</h3><ul>
<li>Set buffer too large will hurt network speed and latency for connections of small amounts data(HTTP/SSH)</li>
<li>Set buffer too small will cause system dropping network packets</li>
<li>To calculate buffer size for maximum throughput of a BDP(Bandwidth Delay Product，时延带宽积) connection.（时延带宽积代表发送的第一个比特即将达到终点时、发送端就已经发出了多少个比特。因此时延带宽积又称为以比特为单位的链路长度。）<ul>
<li>BDP = Bandwidth * Delay</li>
<li>Example: 1 Mbits/s/8 * 2s = 262144 Bytes = 256 KiB</li>
<li>when BDP goes above 64 KiB, tcp connections will use window scalling(滑动窗口) by default to enable full use of BDP buffer, good for sending large files over slow networks</li>
</ul>
</li>
</ul>
<h3 id="Ethernet-Bonding"><a href="#Ethernet-Bonding" class="headerlink" title="Ethernet Bonding"></a>Ethernet Bonding</h3><ul>
<li>Bonding mode<ul>
<li>balance-rr,0: provides fault tolerance and load balancing(more like app level compared to mode 4)</li>
<li>active-backup,1: provides fault tolerance</li>
<li>802.3ad,4: Dynamic Link Aggregation, must configure switch also, default work on layer 2, can be set work on layer 2 and 3 with xmit_hash_policy=layer2+3</li>
<li><a href="http://www.linuxhorizon.ro/bonding.html" target="_blank" rel="noopener">Bonding (Port Trunking)</a></li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost network-scripts]# cat ifcfg-em1</span><br><span class="line">......</span><br><span class="line">DEVICE=em1</span><br><span class="line">MASTER=bond0</span><br><span class="line">SLAVE=yes</span><br><span class="line"></span><br><span class="line">[root@localhost network-scripts]# cat ifcfg-em2</span><br><span class="line">......</span><br><span class="line">DEVICE=em2</span><br><span class="line">MASTER=bond0</span><br><span class="line">SLAVE=yes</span><br><span class="line"></span><br><span class="line">[root@localhost network-scripts]# more ifcfg-bond0</span><br><span class="line">DEVICE=bond0</span><br><span class="line">TYPE=Bond</span><br><span class="line">NAME=bond0</span><br><span class="line">BONDING_MASTER=yes</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">USERCTL=no</span><br><span class="line">ONBOOT=yes</span><br><span class="line">IPADDR=172.20.0.2</span><br><span class="line">PREFIX=24</span><br><span class="line">GATEWAY=172.20.0.1</span><br><span class="line">BONDING_OPTS=&quot;mode=1 miimon=100&quot;</span><br><span class="line"># BONDING_OPTS=&quot;mode=802.3ad xmit_hash_policy=layer2+3&quot;</span><br><span class="line"></span><br><span class="line">[root@localhost network-scripts]# cat /proc/net/bonding/bond0</span><br><span class="line">Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011)</span><br><span class="line"></span><br><span class="line">Bonding Mode: fault-tolerance (active-backup)</span><br><span class="line">Primary Slave: None</span><br><span class="line">Currently Active Slave: em1</span><br><span class="line">MII Status: up</span><br><span class="line">MII Polling Interval (ms): 100</span><br><span class="line">Up Delay (ms): 0</span><br><span class="line">Down Delay (ms): 0</span><br><span class="line"></span><br><span class="line">Slave Interface: em1</span><br><span class="line">MII Status: up</span><br><span class="line">Speed: 1000 Mbps</span><br><span class="line">Duplex: full</span><br><span class="line">Link Failure Count: 0</span><br><span class="line">Permanent HW addr: d0:94:66:24:77:f4</span><br><span class="line">Slave queue ID: 0</span><br><span class="line"></span><br><span class="line">Slave Interface: em2</span><br><span class="line">MII Status: up</span><br><span class="line">Speed: 1000 Mbps</span><br><span class="line">Duplex: full</span><br><span class="line">Link Failure Count: 0</span><br><span class="line">Permanent HW addr: d0:94:66:24:77:f6</span><br><span class="line">Slave queue ID: 0</span><br></pre></td></tr></table></figure>
<h3 id="Protocal-Overheads"><a href="#Protocal-Overheads" class="headerlink" title="Protocal Overheads"></a>Protocal Overheads</h3><ul>
<li>When data is being sent over network, kernel forges packets, a typical TCP/IP packet head includes: Ethernet header, IP header and TCP header.<ul>
<li>TCP connection uses 52 bytes for protocol headers, with a default MTU of 1500 bytes, that’s (52/1500) = 3.5% overhead</li>
<li>UDP connection with 28 bytes for protocol headers, that’s (28/1500) = 1.9% overhead</li>
</ul>
</li>
</ul>
<h3 id="Jumbo-Frames"><a href="#Jumbo-Frames" class="headerlink" title="Jumbo Frames"></a>Jumbo Frames</h3><ul>
<li>To increase MTU can reduce these overhead</li>
<li>MTU bigger that 1500 is called Jumbo Frames</li>
<li>Every piece of networking equipment on the network must support Jumbo frames<br>  Nics, Switches, Routers, etc.</li>
<li><p>Official maximum size of a Jumbo frame is 9000 bytes<br>  (52/ 9000) = 0.58% overhead, compared to 3.5% before</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost network-scripts]# cat ifcfg-em1</span><br><span class="line">......</span><br><span class="line">MTU=9000</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tcp_max_tw_buckets: 只允许调大</span><br><span class="line">	tw：保存timewait的连接个数</span><br><span class="line">		established --&gt; tw</span><br></pre></td></tr></table></figure>
<ul>
<li><p>txqueuelen</p>
<p>  表示传送数据的缓冲区的最大长度。假设最大并发用户数为u，单次向用户推送数据占用的IP包为p个，则瞬间推送的IP包数为u<em>p，网卡发送队列长度需足够大以容纳这么多包数。考虑到一定的余量，建议txqueuelen设为u </em> p <em> 1.2，如果推送数据较小，则p = 1，txqueuelen = u </em> 1.2。</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@yadoom ~]# ifconfig em1 | grep txqueuelen</span><br><span class="line">          collisions:0 txqueuelen:1000</span><br></pre></td></tr></table></figure>
</li>
<li><p>网卡接受发送队列</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 看网卡的接受发送队列长度，intel网卡缺省256，可以设置到 4096</span><br><span class="line">[root@yadoom ~]# ethtool -g em1</span><br><span class="line">Ring parameters for em1:</span><br><span class="line">Pre-set maximums:</span><br><span class="line">RX:             2047</span><br><span class="line">RX Mini:        0</span><br><span class="line">RX Jumbo:       0</span><br><span class="line">TX:             511         # 队列长度</span><br><span class="line">Current hardware settings:</span><br><span class="line">RX:             200</span><br><span class="line">RX Mini:        0</span><br><span class="line">RX Jumbo:       0</span><br><span class="line">TX:             511</span><br><span class="line"></span><br><span class="line"># 设置网卡的接受发送队列</span><br><span class="line"># ethtool -G em1 rx N</span><br></pre></td></tr></table></figure>
<h3 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h3><ul>
<li><p>Too much ARP cache?</p>
<ul>
<li><p>ip neighbor list</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net.ipv4.neigh.default.gc_thresh1 = 128</span><br><span class="line">net.ipv4.neigh.default.gc_thresh2 = 512</span><br><span class="line">net.ipv4.neigh.default.gc_thresh3 = 1024</span><br></pre></td></tr></table></figure>
</li>
<li><p>ip neigh flush dev ethN</p>
</li>
</ul>
</li>
</ul>
<h3 id="内核参数"><a href="#内核参数" class="headerlink" title="内核参数"></a>内核参数</h3><ul>
<li>net.ipv4.tcp_tw_recycle</li>
</ul>
<p><a href="http://www.cnxct.com/coping-with-the-tcp-time_wait-state-on-busy-linux-servers-in-chinese-and-dont-enable-tcp_tw_recycle/" target="_blank" rel="noopener">不要在linux上启用net.ipv4.tcp_tw_recycle参数</a></p>
<h2 id="cgroup"><a href="#cgroup" class="headerlink" title="cgroup"></a>cgroup</h2><ul>
<li>Enable cgroup</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# yum install gcc libcap-devel libcgroup-devel -y</span><br><span class="line">[root@localhost ~]# ls /etc/cgconfig.conf      # cgroup主配置文件</span><br><span class="line">[root@localhost ~]# /etc/init.d/cgconfig start</span><br><span class="line">Starting cgconfig service:                                 [  OK  ]</span><br><span class="line">[root@localhost ~]# </span><br><span class="line">[root@localhost ~]# ls -l /cgroup/</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x 2 root root 0 Dec  6 01:06 blkio</span><br><span class="line">drwxr-xr-x 2 root root 0 Dec  6 01:06 cpu</span><br><span class="line">drwxr-xr-x 2 root root 0 Dec  6 01:06 cpuacct</span><br><span class="line">drwxr-xr-x 2 root root 0 Dec  6 01:06 cpuset</span><br><span class="line">drwxr-xr-x 2 root root 0 Dec  6 01:06 devices</span><br><span class="line">drwxr-xr-x 2 root root 0 Dec  6 01:06 freezer</span><br><span class="line">drwxr-xr-x 2 root root 0 Dec  6 01:06 memory</span><br><span class="line">drwxr-xr-x 2 root root 0 Dec  6 01:06 net_cls</span><br></pre></td></tr></table></figure>
<ul>
<li>Control cgroups Services<ul>
<li>cgconfig<ul>
<li>mount controllers</li>
<li>create cgroups</li>
<li>enables cgroup configure</li>
</ul>
</li>
<li>cgred<ul>
<li>assign process of specific users to cgroup</li>
<li>assign specific processes to cgroup</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# lssubsys -m</span><br><span class="line">cpuset /cgroup/cpuset</span><br><span class="line">cpu /cgroup/cpu</span><br><span class="line">cpuacct /cgroup/cpuacct</span><br><span class="line">memory /cgroup/memory</span><br><span class="line">devices /cgroup/devices</span><br><span class="line">freezer /cgroup/freezer</span><br><span class="line">net_cls /cgroup/net_cls</span><br><span class="line">blkio /cgroup/blkio</span><br></pre></td></tr></table></figure>
<h3 id="To-Control-CPU-Usage（案例，CPU使用控制）"><a href="#To-Control-CPU-Usage（案例，CPU使用控制）" class="headerlink" title="To Control CPU Usage（案例，CPU使用控制）"></a>To Control CPU Usage（案例，CPU使用控制）</h3><pre><code>1. Create cgroups under cpu controller

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# vim /etc/cgconfig.d/mycpu.conf</span><br><span class="line">group lesscpu &#123;</span><br><span class="line">        cpu &#123;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line">group morecpu &#123;</span><br><span class="line">        cpu &#123;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line">[root@localhost cgconfig.d]# /etc/init.d/cgconfig reload</span><br><span class="line">Stopping cgconfig service:                                 [  OK  ]</span><br><span class="line">Starting cgconfig service:                                 [  OK  ]</span><br><span class="line">[root@localhost cgconfig.d]# ls /cgroup/cpu/lesscpu/</span><br><span class="line">cgroup.event_control  cpu.cfs_period_us  cpu.rt_period_us   cpu.shares  notify_on_release</span><br><span class="line">cgroup.procs          cpu.cfs_quota_us   cpu.rt_runtime_us  cpu.stat    tasks</span><br><span class="line">[root@localhost cgconfig.d]# ls /cgroup/cpu/morecpu/</span><br><span class="line">cgroup.event_control  cpu.cfs_period_us  cpu.rt_period_us   cpu.shares  notify_on_release</span><br><span class="line">cgroup.procs          cpu.cfs_quota_us   cpu.rt_runtime_us  cpu.stat    tasks</span><br><span class="line"></span><br><span class="line"># 临时调整</span><br><span class="line">[root@localhost cgconfig.d]# cgcreate -g cpu:/lesscpu</span><br><span class="line">[root@localhost cgconfig.d]# cgcreate -g cpu:/morecpu</span><br><span class="line">或者</span><br><span class="line">[root@localhost cgconfig.d]# mkdir /cgroup/cpu/&#123;less,more&#125;</span><br></pre></td></tr></table></figure>


2. Configure priorities or limits

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# vim /etc/cgconfig.d/mycpu.conf</span><br><span class="line">group lesscpu &#123;</span><br><span class="line">        cpu &#123;</span><br><span class="line">                cpu.shares=100;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line">group morecpu &#123;</span><br><span class="line">        cpu &#123;</span><br><span class="line">                cpu.shares=200;</span><br><span class="line">        &#125;       </span><br><span class="line">[root@localhost cgconfig.d]# more /cgroup/cpu/lesscpu/cpu.shares </span><br><span class="line">100</span><br><span class="line">[root@localhost cgconfig.d]# more /cgroup/cpu/morecpu/cpu.shares     </span><br><span class="line">200</span><br><span class="line"></span><br><span class="line"># 临时调整，重载后失效</span><br><span class="line">[root@localhost cgconfig.d]# cgset -r cpu.shares=100 lesscpu</span><br><span class="line">[root@localhost cgconfig.d]# cgset -r cpu.shares=200 morecpu</span><br></pre></td></tr></table></figure>


3. Assign processes to cgroups

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 需要把其他CPU下线，只剩一核的情况下测试</span><br><span class="line">[root@localhost cgconfig.d]# cgexec -g cpu:lesscpu time dd if=/dev/zero of=/dev/null bs=1M count=200000</span><br><span class="line">200000+0 records in</span><br><span class="line">200000+0 records out</span><br><span class="line">209715200000 bytes (210 GB) copied, 11.4672 s, 18.3 GB/s</span><br><span class="line">0.01user 11.43system 0:11.46elapsed 99%CPU (0avgtext+0avgdata 1848maxresident)k</span><br><span class="line">0inputs+0outputs (0major+491minor)pagefaults 0swaps</span><br><span class="line">[root@localhost cgconfig.d]# cgexec -g cpu:morecpu time dd if=/dev/zero of=/dev/null bs=1M count=200000</span><br></pre></td></tr></table></figure>
</code></pre><h3 id="To-Control-Memory-Usage"><a href="#To-Control-Memory-Usage" class="headerlink" title="To Control Memory Usage"></a>To Control Memory Usage</h3><pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# vim /etc/cgconfig.d/mymem.conf</span><br><span class="line">group poormem &#123;</span><br><span class="line">        memory &#123;</span><br><span class="line">                # Physical memory limit, 256M, must be first set</span><br><span class="line">                memory.limit_in_bytes=268435456;</span><br><span class="line">                # Total memory limit, 512M, Physical Memory + Swap</span><br><span class="line">                memory.memsw.limit_in_bytes=536870912;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line">[root@localhost cgconfig.d]# /etc/init.d/cgconfig reload                                              </span><br><span class="line">Stopping cgconfig service:                                 [  OK  ]</span><br><span class="line">Starting cgconfig service:                                 [  OK  ]</span><br><span class="line">[root@localhost cgconfig.d]# cat /cgroup/memory/poormem/memory.limit_in_bytes   </span><br><span class="line">268435456</span><br><span class="line">[root@localhost cgconfig.d]# cat /cgroup/memory/poormem/memory.memsw.limit_in_bytes </span><br><span class="line">268435456</span><br><span class="line"></span><br><span class="line">[root@localhost cgconfig.d]# cgexec -g memory:poormem dd if=/dev/zero of=/dev/null bs=1M count=300</span><br><span class="line"># 该命令会占用256M物理内存，以及300-256=44M Swap内存。</span><br></pre></td></tr></table></figure>
</code></pre><h3 id="To-Control-Disk-I-O-Usage"><a href="#To-Control-Disk-I-O-Usage" class="headerlink" title="To Control Disk I/O Usage"></a>To Control Disk I/O Usage</h3><ul>
<li>Create cgroup under blkio controller</li>
<li>Configure blkio priorities and usage limit</li>
<li><p>Assign processes to cgroups</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# vim /etc/cgconfig.d/myio.conf</span><br><span class="line">group lowio &#123;</span><br><span class="line">        blkio &#123;</span><br><span class="line">                blkio.weight=100;      # blkio.weight works under cfq scheduler</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line">group highio &#123;</span><br><span class="line">        blkio &#123;</span><br><span class="line">                blkio.weight=200;      # blkio.weight works under cfq scheduler</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line">group ddio &#123;</span><br><span class="line">        blkio &#123;</span><br><span class="line">                blkio.throttle.read_bps_device=&quot;8:0 1000000&quot;</span><br><span class="line">                # 针对主号，从号“8:0”的磁盘设备限制I/O吞吐量</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"># 上面“8:0”（主号，从号）由下面的命令查看而来。</span><br><span class="line">[root@localhost ~]# ls -l /dev/sda</span><br><span class="line">brw-rw---- 1 root disk 8, 0 Dec  6 03:25 /dev/sda</span><br><span class="line"></span><br><span class="line">[root@localhost cgconfig.d]# /etc/init.d/cgconfig reload                                              </span><br><span class="line">Stopping cgconfig service:                                 [  OK  ]</span><br><span class="line">Starting cgconfig service:                                 [  OK  ]</span><br><span class="line"></span><br><span class="line">[root@localhost ~]# cat /sys/block/sda/queue/scheduler </span><br><span class="line">noop anticipatory deadline [cfq]</span><br><span class="line">[root@localhost ~]# echo 3 &gt; /proc/sys/vm/drop_caches </span><br><span class="line">[root@localhost ~]# </span><br><span class="line">[root@localhost ~]# ls -lh /tmp/bigfile*              </span><br><span class="line">-rw-r--r-- 1 root root 1000M Dec  6 03:52 /tmp/bigfile1</span><br><span class="line">-rw-r--r-- 1 root root 1000M Dec  6 03:52 /tmp/bigfile2</span><br><span class="line">[root@localhost tmp]# cgexec -g blkio:lowio time cat ./bigfile1 &gt; /dev/null</span><br><span class="line">[root@localhost tmp]# cgexec -g blkio:highio time cat ./bigfile2 &gt; /dev/null</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Using-cgred"><a href="#Using-cgred" class="headerlink" title="Using cgred"></a>Using cgred</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# vim /etc/cgrules.conf </span><br><span class="line">#&lt;user&gt;         &lt;controllers&gt;   &lt;destination&gt;</span><br><span class="line">john:*         blkio           ddio/</span><br><span class="line">*:dd         blkio           ddio/</span><br></pre></td></tr></table></figure>
<p>[root@localhost ~]# /etc/init.d/cgred reload</p>
<h3 id="To-Freeze-Certain-Processes"><a href="#To-Freeze-Certain-Processes" class="headerlink" title="To Freeze Certain Processes"></a>To Freeze Certain Processes</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# vim /etc/cgconfig.d/freeze.conf</span><br><span class="line">group stopit &#123;</span><br><span class="line">        freezer &#123;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line">[root@localhost ~]# /etc/init.d/cgconfig reload</span><br><span class="line">[root@localhost ~]# echo #pid &gt; /cgroup/freezer/stopit/tasks           # 绑定进程和freezer控制器</span><br><span class="line">[root@localhost ~]# echo FROZEN &gt; /cgroup/freezer/stopit/freezer.state    # 冻结该进程</span><br><span class="line">[root@localhost ~]# echo THAWED &gt; /cgroup/freezer/stopit/freezer.state    # 解冻</span><br></pre></td></tr></table></figure>
<h2 id="Command"><a href="#Command" class="headerlink" title="Command"></a>Command</h2><h3 id="strace-amp-ltrace"><a href="#strace-amp-ltrace" class="headerlink" title="strace &amp; ltrace"></a>strace &amp; ltrace</h3><ul>
<li><p>strace: kernerl space(system call)</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# strace -e trace=network ping -c 1 127.0.0.1 </span><br><span class="line">socket(PF_INET, SOCK_RAW, IPPROTO_ICMP) = 3</span><br><span class="line">socket(PF_INET, SOCK_DGRAM, IPPROTO_IP) = 4</span><br><span class="line">connect(4, &#123;sa_family=AF_INET, sin_port=htons(1025), sin_addr=inet_addr(&quot;127.0.0.1&quot;)&#125;, 16) = 0</span><br><span class="line">getsockname(4, &#123;sa_family=AF_INET, sin_port=htons(55365), sin_addr=inet_addr(&quot;127.0.0.1&quot;)&#125;, [16]) = 0</span><br><span class="line">setsockopt(3, SOL_RAW, ICMP_FILTER, ~(ICMP_ECHOREPLY|ICMP_DEST_UNREACH|ICMP_SOURCE_QUENCH|ICMP_REDIRECT|ICMP_TIME_EXCEEDED|ICMP_PARAMETERPROB), 4) = 0</span><br><span class="line">setsockopt(3, SOL_IP, IP_RECVERR, [1], 4) = 0</span><br><span class="line">setsockopt(3, SOL_SOCKET, SO_SNDBUF, [324], 4) = 0</span><br><span class="line">setsockopt(3, SOL_SOCKET, SO_RCVBUF, [65536], 4) = 0</span><br><span class="line">getsockopt(3, SOL_SOCKET, SO_RCVBUF, [131072], [4]) = 0</span><br><span class="line">PING 127.0.0.1 (127.0.0.1) 56(84) bytes of data.</span><br><span class="line">setsockopt(3, SOL_SOCKET, SO_TIMESTAMP, [1], 4) = 0</span><br><span class="line">setsockopt(3, SOL_SOCKET, SO_SNDTIMEO, &quot;\1\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;, 16) = 0</span><br><span class="line">setsockopt(3, SOL_SOCKET, SO_RCVTIMEO, &quot;\1\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0&quot;, 16) = 0</span><br><span class="line">sendmsg(3, &#123;msg_name(16)=&#123;sa_family=AF_INET, sin_port=htons(0), sin_addr=inet_addr(&quot;127.0.0.1&quot;)&#125;, msg_iov(1)=[&#123;&quot;\10\0\224B+\23\0\1\31&lt;\10\\\0\0\0\0V&gt;\2\0\0\0\0\0\20\21\22\23\24\25\26\27&quot;..., 64&#125;], msg_controllen=0, msg_flags=0&#125;, 0) = 64</span><br><span class="line">recvmsg(3, &#123;msg_name(16)=&#123;sa_family=AF_INET, sin_port=htons(0), sin_addr=inet_addr(&quot;127.0.0.1&quot;)&#125;, msg_iov(1)=[&#123;&quot;E\0\0TQ\253\0\0@\1*\374\177\0\0\1\177\0\0\1\0\0\234B+\23\0\1\31&lt;\10\\&quot;..., 192&#125;], msg_controllen=32, &#123;cmsg_len=32, cmsg_level=SOL_SOCKET, cmsg_type=0x1d /* SCM_??? */, ...&#125;, msg_flags=0&#125;, 0) = 84</span><br><span class="line">64 bytes from 127.0.0.1: icmp_seq=1 ttl=64 time=0.035 ms</span><br><span class="line"></span><br><span class="line">--- 127.0.0.1 ping statistics ---</span><br><span class="line">1 packets transmitted, 1 received, 0% packet loss, time 0ms</span><br><span class="line">rtt min/avg/max/mdev = 0.035/0.035/0.035/0.000 ms</span><br><span class="line">+++ exited with 0 +++</span><br><span class="line"></span><br><span class="line">[root@localhost ~]# strace -e trace=file ls /</span><br><span class="line">execve(&quot;/bin/ls&quot;, [&quot;ls&quot;, &quot;/&quot;], [/* 21 vars */]) = 0</span><br><span class="line">access(&quot;/etc/ld.so.preload&quot;, R_OK)      = -1 ENOENT (No such file or directory)</span><br><span class="line">open(&quot;/etc/ld.so.cache&quot;, O_RDONLY)      = 3</span><br><span class="line">open(&quot;/lib64/libselinux.so.1&quot;, O_RDONLY) = 3</span><br><span class="line">open(&quot;/lib64/librt.so.1&quot;, O_RDONLY)     = 3</span><br><span class="line">open(&quot;/lib64/libcap.so.2&quot;, O_RDONLY)    = 3</span><br><span class="line">open(&quot;/lib64/libacl.so.1&quot;, O_RDONLY)    = 3</span><br><span class="line">open(&quot;/lib64/libc.so.6&quot;, O_RDONLY)      = 3</span><br><span class="line">open(&quot;/lib64/libdl.so.2&quot;, O_RDONLY)     = 3</span><br><span class="line">open(&quot;/lib64/libpthread.so.0&quot;, O_RDONLY) = 3</span><br><span class="line">open(&quot;/lib64/libattr.so.1&quot;, O_RDONLY)   = 3</span><br><span class="line">statfs(&quot;/selinux&quot;, &#123;f_type=&quot;EXT2_SUPER_MAGIC&quot;, f_bsize=4096, f_blocks=1297385108, f_bfree=1296583382, f_bavail=1230678333, f_files=329531392, f_ffree=329482788, f_fsid=&#123;-525262460, 379377246&#125;, f_namelen=255, f_frsize=4096&#125;) = 0</span><br><span class="line">open(&quot;/proc/filesystems&quot;, O_RDONLY)     = 3</span><br><span class="line">open(&quot;/usr/lib/locale/locale-archive&quot;, O_RDONLY) = 3</span><br><span class="line">stat(&quot;/&quot;, &#123;st_mode=S_IFDIR|0555, st_size=4096, ...&#125;) = 0</span><br><span class="line">open(&quot;/&quot;, O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) = 3</span><br><span class="line">bin  boot  cgroup  dev  etc  home  lib  lib64  lost+found  media  mnt  opt  proc  root  sbin  selinux  srv  sys  tmp  usr  var</span><br><span class="line">+++ exited with 0 +++</span><br></pre></td></tr></table></figure>
</li>
<li><p>ltrace: user space(library function call)</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# ltrace date</span><br><span class="line">(0, 0, 0, 0x7f6770d11bc8, 88)                    = 0x30b3a22160</span><br><span class="line">__libc_start_main(0x401ec0, 1, 0x7ffff9ae0dc8, 0x408f30, 0x408f20 &lt;unfinished ...&gt;</span><br><span class="line">strrchr(&quot;date&quot;, &apos;/&apos;)                             = NULL</span><br><span class="line">setlocale(6, &quot;&quot;)                                 = &quot;en_US.UTF-8&quot;</span><br><span class="line">bindtextdomain(&quot;coreutils&quot;, &quot;/usr/share/locale&quot;) = &quot;/usr/share/locale&quot;</span><br><span class="line">textdomain(&quot;coreutils&quot;)                          = &quot;coreutils&quot;</span><br><span class="line">__cxa_atexit(0x4057a0, 0, 0, 0x736c6974756572, 0x30b3f8e188) = 0</span><br><span class="line">getopt_long(1, 0x7ffff9ae0dc8, &quot;d:f:I::r:Rs:u&quot;, 0x60c1a0, NULL) = -1</span><br><span class="line">nl_langinfo(131180, 0x7ffff9ae0dc8, 0x30b3f8e9ec, 0x30b3f8e9e4, 0) = 0x7f676affd425</span><br><span class="line">clock_gettime(0, 0x7ffff9ae0c90, 0x1a06440, 0x30b3f8e9e4, 0) = 0</span><br><span class="line">localtime(0x7ffff9ae0b80)                        = 0x30b3f93440</span><br><span class="line">strftime(&quot;&quot;, 140082153508067, NULL, 0x7ffff9ae0b23) = 4</span><br><span class="line">fwrite(&quot;Wed&quot;, 3, 1, 0x30b3f8f040)                = 1</span><br><span class="line">fputc(&apos; &apos;, 0x30b3f8f040)                         = 32</span><br><span class="line">strftime(&quot;&quot;, 140082153508184, NULL, 0x7ffff9ae0b23) = 4</span><br><span class="line">fwrite(&quot;Dec&quot;, 3, 1, 0x30b3f8f040)                = 1</span><br><span class="line">fputc(&apos; &apos;, 0x30b3f8f040)                         = 32</span><br><span class="line">fputc(&apos; &apos;, 0x30b3f8f040)                         = 32</span><br><span class="line">fwrite(&quot;5&quot;, 1, 1, 0x30b3f8f040)                  = 1</span><br><span class="line">fputc(&apos; &apos;, 0x30b3f8f040)                         = 32</span><br><span class="line">fwrite(&quot;21&quot;, 2, 1, 0x30b3f8f040)                 = 1</span><br><span class="line">fputc(&apos;:&apos;, 0x30b3f8f040)                         = 58</span><br><span class="line">fwrite(&quot;25&quot;, 2, 1, 0x30b3f8f040)                 = 1</span><br><span class="line">fputc(&apos;:&apos;, 0x30b3f8f040)                         = 58</span><br><span class="line">fwrite(&quot;49&quot;, 2, 1, 0x30b3f8f040)                 = 1</span><br><span class="line">fputc(&apos; &apos;, 0x30b3f8f040)                         = 32</span><br><span class="line">strlen(&quot;CST&quot;)                                    = 3</span><br><span class="line">fwrite(&quot;CST&quot;, 3, 1, 0x30b3f8f040)                = 1</span><br><span class="line">fputc(&apos; &apos;, 0x30b3f8f040)                         = 32</span><br><span class="line">fwrite(&quot;2018&quot;, 4, 1, 0x30b3f8f040)               = 1</span><br><span class="line">exit(0 &lt;unfinished ...&gt;</span><br><span class="line">__fpending(0x30b3f8f040, 0, 0x30b3f8fbd0, 0x30b3f8fbd0, 4) = 29</span><br><span class="line">fclose(0x30b3f8f040)                             = 0</span><br><span class="line">Wed Dec  5 21:25:49 CST 2018</span><br><span class="line">__fpending(0x30b3f8f120, 0, 0x30b3f906b0, 0, 0x7f6770d10700) = 0</span><br><span class="line">fclose(0x30b3f8f120)                             = 0</span><br><span class="line">+++ exited (status 0) +++</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="systemtap"><a href="#systemtap" class="headerlink" title="systemtap"></a>systemtap</h2><p>Powerful tracing system to profile kernel space and user space code.</p>
<h1 id="结尾"><a href="#结尾" class="headerlink" title="结尾"></a>结尾</h1><ul>
<li>性能观测工具</li>
</ul>
<p><img src="/images/linux/performance-tuning/linux_observability_tools.png" alt="linux_observability_tools"></p>
<ul>
<li>性能测评工具</li>
</ul>
<p><img src="/images/linux/performance-tuning/linux_benchmarking_tools.png" alt="linux_benchmarking_tools"></p>
<ul>
<li>性能调优工具</li>
</ul>
<p><img src="/images/linux/performance-tuning/linux_tuning_tools.png" alt="linux_tuning_tools"></p>
<ul>
<li>sar命令</li>
</ul>
<p><img src="/images/linux/performance-tuning/linux_observability_sar.png" alt="linux_observability_sar"></p>
<ul>
<li><a href="http://www.brendangregg.com/linuxperf.html" target="_blank" rel="noopener">Linux Performance大全</a></li>
</ul>
<p><img src="/images/linux/performance-tuning/linux_perf_tools_full.png" alt="linux_perf_tools_full"></p>
<h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><p><a href="http://honglus.blogspot.com/" target="_blank" rel="noopener">http://honglus.blogspot.com/</a></p>

          
        
      
    </div>

    

    
    
    

    

    
       
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/11/06/linux/performance-tuning/overview-of-performance-tuning/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/11/06/linux/performance-tuning/overview-of-performance-tuning/" class="post-title-link" itemprop="http://yoursite.com/index.html">Overview of Performance Tuning</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-11-06 08:55:29" itemprop="dateCreated datePublished" datetime="2018-11-06T08:55:29+08:00">2018-11-06</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2018-12-07 21:43:49" itemprop="dateModified" datetime="2018-12-07T21:43:49+08:00">2018-12-07</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/Linux/" itemprop="url" rel="index"><span itemprop="name">Linux</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>为了更能通俗易懂的理解我们即将要的性能调优的话题，我在这里简单的和大家说一下我写这篇文章的写作方法 “5w+1h”方法。</p>
<p>5w+1h 就是对所做工作进行科学的分析，对某一工作在调查研究的基础上，就其工作内容（What）、责任者（Who）、工作岗位（Where）、工作时间（When）、怎样操作（How）以及为何这样做（Why），即”5W”、”1H”进行书面描述，并按此描述进行操作，达到完成职务任务的目标。</p>
<h1 id="什么是性能调优？-what"><a href="#什么是性能调优？-what" class="headerlink" title="什么是性能调优？(what)"></a>什么是性能调优？(what)</h1><p><img src="/images/linux/performance-tuning/linux_architecture.jpg" alt="linux-architecture"></p>
<p>在说什么是性能调优之前我们先来说一下，计算机的体系结构。如上图，简单来说包括三块：硬件、操作系统、应用程序。其实，性能调优就是调节这些内容，包括硬件、操作系统、应用程序。</p>
<p>其中，这三大方面中又包含了若干的内容。硬件包括CPU、内存、磁盘、网卡等，操作系统包括进程、虚拟内存、文件系统、网络等，应用程序常见的有Apache、MySQL、Nginx、Memcahed等。</p>
<p>那什么是性能调优呢？性能调优就是对计算机硬件、操作系统和应用有相当深入的了解，调节三者之间的关系，实现整个系统（包括硬件、操作系统、应用）的性能最大化，并能不断的满足现有的业务需求。</p>
<h1 id="为什么需要性能调优？-why"><a href="#为什么需要性能调优？-why" class="headerlink" title="为什么需要性能调优？(why)"></a>为什么需要性能调优？(why)</h1><p>当一个发行版打包发送到客户手中的时候，它是为了完全兼容市场中大部分计算机而设计的。这是一个相当混杂的硬件集合（硬盘，显卡，网卡，等等）。所以Red Hat， SUSE，Mandriva，Ubuntu 和其他的一些发行版厂商选择了一些通用的设置来确保安装成功。</p>
<h1 id="什么时候需要性能调优？-when"><a href="#什么时候需要性能调优？-when" class="headerlink" title="什么时候需要性能调优？(when)"></a>什么时候需要性能调优？(when)</h1><ul>
<li><p>上线前（基本优化）</p>
<p>  包括操作系统优化和应用环境优化等，我称上线前的优化为基本优化也称为经验优化。根据你做过的项目和你工作中的经验对上线前的服务器或架构进行基本的性能优化来满足业务需求。</p>
</li>
<li><p>上线后（持续优化）</p>
<p>  对上线后的项目进行性能监控包括服务器性能监控和服务性能监控，其中服务器性能监控包括CPU使用率、CPU负载、内存使用率、磁盘I/O、磁盘空间使用率、网络流量、系统进程等，服务性能监控包括apache、nginx、mysql等架构中所有的服务都需要进行性能监控，一但发现有问题我们都得去进行性能优化，在这个过程中我称为持续优化也称为监控优化。</p>
</li>
</ul>
<h1 id="什么地方需要性能调优？-where"><a href="#什么地方需要性能调优？-where" class="headerlink" title="什么地方需要性能调优？(where)"></a>什么地方需要性能调优？(where)</h1><ul>
<li><p>硬件 （CPU、内存、磁盘、网卡）</p>
</li>
<li><p>操作系统（进程、文件系统、内核）</p>
</li>
<li><p>应用程序（Nginx、MySQL）</p>
</li>
</ul>
<h1 id="什么人来进行性能调优？-who"><a href="#什么人来进行性能调优？-who" class="headerlink" title="什么人来进行性能调优？(who)"></a>什么人来进行性能调优？(who)</h1><p>一说起性能优化我们第一个想到的是运维工程师，他们来进行优化。其实我想说，这么说是片面的性能优化不仅仅是运维工程师的事。其实呢，性能优化是一个团队的事。我为什么这么说呢？下面我们想一下公司要做一个项目，那项目的具体流程是什么呢？可能不是很详细，但大体过程是样的：</p>
<ul>
<li>运营提出需求</li>
<li>产品整理需求</li>
<li>开发开发具体的业务应用</li>
<li>运维搭建开发环境</li>
<li>QA 进行项目测试</li>
<li>运维进行项目上线</li>
<li>监控进行项目监控</li>
<li>……</li>
</ul>
<p>需要运营部、产品部、开发部、运维部、QA （测试）、监控等所有部门的参加，同样的一个项目（业务）存在性能问题，不会只是运维部门需要性能调优而是所以部门一起解决这个性能问题，这是缺一不可的。可能出现在产品，也可能出现在程序上，也可能是业务需要本身就有问题，也可能是运维的环境搭建有问题。但参加性能调优的更多的是开发、运维、测试和监控。</p>
<h1 id="怎么样进行性能调优？-How"><a href="#怎么样进行性能调优？-How" class="headerlink" title="怎么样进行性能调优？(How)"></a>怎么样进行性能调优？(How)</h1><p>说一说怎么进行性能调优，具体步骤如下：</p>
<ul>
<li>性能指标 –&gt; 确认衡量标准</li>
<li>性能测试 –&gt; 验证性能指标</li>
<li>性能分析 –&gt; 找出性能瓶颈</li>
<li>性能调优 –&gt; 解决性能问题</li>
<li>性能监控 –&gt; 检验调优效果</li>
</ul>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>在这篇“性能优化概述”的博文中我只是给大家讲解一下具体的优化思路，帮助大家理解性能优化，这样大家更容易理解一些，让大家知道性能优化并不是传说中的那么难，难到不可动手去做，只要我们掌握好方法，什么难题都可以解决。</p>

          
        
      
    </div>

    

    
    
    

    

    
       
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">John Doe</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">4</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                    
                      
                    
                    <span class="site-state-item-count">1</span>
                    <span class="site-state-item-name">categories</span>
                  
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">3</span>
                    <span class="site-state-item-name">tags</span>
                  
                </div>
              
            </nav>
          

          

          

          

          
          

          
            
          
          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>

  

  
</div>


  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v6.5.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.5.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.5.0"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.5.0"></script>



  



  










  





  

  

  

  

  

  
  

  

  

  

  

  

  

</body>
</html>
